````python
# -*- coding: utf-8 -*-
"""
**Python íŒ¨í‚¤ì§€ ì„¤ì¹˜**:

* ì•„ë˜ ëª…ë ¹ì–´ë¡œ ëª¨ë“  í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤.
```bash
pip install -U gradio ollama langchain langchain-community langchain-chroma \
langchain-text-splitters pypdf pymupdf sentence-transformers chromadb \
pandas openpyxl Pillow easyocr paddleocr duckduckgo-search beautifulsoup4 \
youtube-search pytz camelot-py[cv] tabula-py pdfplumber
```
* **Java ì„¤ì¹˜**: `tabula-py` ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” Javaê°€ í•„ìš”í•©ë‹ˆë‹¤. ì‹œìŠ¤í…œì— JDK 8 ì´ìƒì´ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.
[Ultra-Think í†µí•©ë³¸ - v1.1 ìˆ˜ì •] 38-in-1 AI-Toolkit on Gradio
- ëª©ì : ì œê³µëœ 38ê°œ ì½”ë“œë¥¼ DeepSeek-R1(Ollama) ë° Gradio ê¸°ë°˜ì˜ ë‹¨ì¼ í†µí•© ì•±ìœ¼ë¡œ ì¬êµ¬ì„±
- íŠ¹ì§•:
    1) API Key ë¶ˆí•„ìš” (100% ë¡œì»¬ Ollama êµ¬ë™)
    2) 7ê°œ íƒ­ìœ¼ë¡œ ê¸°ëŠ¥ ë¶„ë¦¬: ì±—ë´‡, PDF, RAG, Vision, ê³ ê¸‰ ìƒì„±ê¸°, ì›¹/ë„êµ¬, ìˆœì°¨ì  ì‚¬ê³ 
    3) ì‹¬ë¯¸ì„±ê³¼ ì‚¬ìš© í¸ì˜ì„±ì„ ê³ ë ¤í•œ UI/UX ë””ìì¸
    4) ì¤‘ë³µ ì½”ë“œ ì œê±° ë° ê¸°ëŠ¥ë³„ ëª¨ë“ˆí™”, ì „ì—­ ìƒíƒœ ê´€ë¦¬ë¡œ íš¨ìœ¨ì„± ì¦ëŒ€
- ì‹¤í–‰ ì „ì œ:
    - Ollama ì„¤ì¹˜ ë° ëª¨ë¸ pull (deepseek-r1, qwen2.5-vl, nomic-embed-text)
    - ìƒê¸° ëª…ì‹œëœ ëª¨ë“  Python íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ
"""
````python
# -*- coding: utf-8 -*-
"""
[Ultra-Think í†µí•©ë³¸ - v1.2 ìˆ˜ì •] 38-in-1 AI-Toolkit on Gradio
- ëª©ì : ì œê³µëœ 38ê°œ ì½”ë“œë¥¼ DeepSeek-R1(Ollama) ë° Gradio ê¸°ë°˜ì˜ ë‹¨ì¼ í†µí•© ì•±ìœ¼ë¡œ ì¬êµ¬ì„±
- v1.2 ë³€ê²½ ì‚¬í•­:
    1. [FIX] Chatbotì˜ `type="messages"` í˜•ì‹ì— ë§ì¶° history ì²˜ë¦¬ ë¡œì§ ìˆ˜ì • (ì±„íŒ… ì˜¤ë¥˜ í•´ê²°)
    2. [REFACTOR] Chatbotì˜ Gradio ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ë¥¼ ë” ì•ˆì •ì ì¸ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ê°œì„ 
- v1.1 ë³€ê²½ ì‚¬í•­:
    1. [FIX] gr.Codeì˜ language="mermaid" ì˜¤ë¥˜ë¥¼ "markdown"ìœ¼ë¡œ ìˆ˜ì •
    2. [FIX] gr.Chatbotì˜ type ëˆ„ë½ìœ¼ë¡œ ì¸í•œ DeprecationWarning í•´ê²° (type="messages" ì¶”ê°€)
    3. [REFACTOR] Mermaid ìƒì„±ê¸° ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ë¥¼ ëŒë‹¤ì—ì„œ ëª…ì‹œì  í•¨ìˆ˜ë¡œ ë³€ê²½í•˜ì—¬ íš¨ìœ¨ì„± ê°œì„ 
- íŠ¹ì§•:
    1) API Key ë¶ˆí•„ìš” (100% ë¡œì»¬ Ollama êµ¬ë™)
    2) 7ê°œ íƒ­ìœ¼ë¡œ ê¸°ëŠ¥ ë¶„ë¦¬: ì±—ë´‡, PDF, RAG, Vision, ê³ ê¸‰ ìƒì„±ê¸°, ì›¹/ë„êµ¬, ìˆœì°¨ì  ì‚¬ê³ 
    3) ì‹¬ë¯¸ì„±ê³¼ ì‚¬ìš© í¸ì˜ì„±ì„ ê³ ë ¤í•œ UI/UX ë””ìì¸
    4) ì¤‘ë³µ ì½”ë“œ ì œê±° ë° ê¸°ëŠ¥ë³„ ëª¨ë“ˆí™”, ì „ì—­ ìƒíƒœ ê´€ë¦¬ë¡œ íš¨ìœ¨ì„± ì¦ëŒ€
- ì‹¤í–‰ ì „ì œ:
    - Ollama ì„¤ì¹˜ ë° ëª¨ë¸ pull (deepseek-r1, qwen2.5-vl, nomic-embed-text)
    - ìƒê¸° ëª…ì‹œëœ ëª¨ë“  Python íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ
"""

# -*- coding: utf-8 -*-
"""
[Ultra-Think í†µí•©ë³¸ - v1.4 ìµœì¢… ìˆ˜ì •] 38-in-1 AI-Toolkit on Gradio
- ëª©ì : ì œê³µëœ 38ê°œ ì½”ë“œë¥¼ DeepSeek-R1(Ollama) ë° Gradio ê¸°ë°˜ì˜ ë‹¨ì¼ í†µí•© ì•±ìœ¼ë¡œ ì¬êµ¬ì„±
- v1.4 ë³€ê²½ ì‚¬í•­:
    1. [CRITICAL FIX] Mermaid ìƒì„±ê¸° í•¸ë“¤ëŸ¬ê°€ ì»´í¬ë„ŒíŠ¸ ê°ì²´ ëŒ€ì‹  í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë°˜í™˜í•˜ë„ë¡ ìˆ˜ì •í•˜ì—¬ ë Œë”ë§ ì˜¤ë¥˜ í•´ê²°.
    2. [CRITICAL FIX] ì±—ë´‡ ë° RAG ì±—ë´‡ì—ì„œ ì‚¬ìš©ì ë©”ì‹œì§€ê°€ historyì— ì¤‘ë³µ ì¶”ê°€ë˜ëŠ” ì ì¬ì  ì˜¤ë¥˜ ìˆ˜ì •. ì´ë²¤íŠ¸ íë¦„ì„ ë”ìš± ì•ˆì •í™”.
- v1.3 ë³€ê²½ ì‚¬í•­:
    1. [CRITICAL FIX] ì±—ë´‡ ìŠ¤íŠ¸ë¦¬ë° ë¡œì§ ì „ë©´ ì¬êµ¬ì„±í•˜ì—¬ ë°˜ë³µì ì¸ ì±„íŒ… ì˜¤ë¥˜ ë¬¸ì œ í•´ê²°.
... (ì´ì „ ë³€ê²½ ì´ë ¥ ìƒëµ)
"""



from __future__ import annotations
import os
import gc
import re
import json
import base64
import time
import tempfile
from io import BytesIO
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple, Generator
from pathlib import Path

# --- UI ---
import gradio as gr

# --- LLM & LangChain ---
import ollama
from langchain_community.chat_models import ChatOllama
from langchain_community.embeddings import OllamaEmbeddings
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage

# --- ë°ì´í„° ì²˜ë¦¬ ë° ë„êµ¬ ---
import fitz  # PyMuPDF
import pandas as pd
from PIL import Image, ImageDraw
import numpy as np

# --- RAG ê´€ë ¨ ---
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate

# --- OCR ê´€ë ¨ ---
try:
    import easyocr
except ImportError:
    print("Warning: easyocr not installed. Some OCR features will be disabled.")
    easyocr = None
try:
    from paddleocr import PPStructure
except ImportError:
    print("Warning: paddleocr not installed. Table OCR feature will be disabled.")
    PPStructure = None

# --- ì›¹ & ê¸°íƒ€ ë„êµ¬ ---
from langchain_community.tools import DuckDuckGoSearchResults
from langchain_community.document_loaders import WebBaseLoader
import pytz
from youtube_search import YoutubeSearch

# ==================================================================================
# âš™ï¸ ì „ì—­ ì„¤ì • ë° ìƒíƒœ ê´€ë¦¬ (Global Settings & State Management)
# ==================================================================================

class GlobalState:
    """ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ì „ì—­ ìƒíƒœì™€ ë¦¬ì†ŒìŠ¤ë¥¼ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""
    def __init__(self):
        self.llm: Optional[ChatOllama] = None
        self.rag_vectorstore: Optional[Chroma] = None
        self.rag_embeddings: Optional[OllamaEmbeddings] = None
        self.last_rag_sources: List[Dict] = []

    def get_llm(self, model_name="deepseek-r1", temperature=0.7) -> ChatOllama:
        if self.llm is None or self.llm.model != model_name or self.llm.temperature != temperature:
            print(f"Loading LLM: {model_name} with temp: {temperature}")
            self.llm = ChatOllama(model=model_name, temperature=temperature)
        return self.llm

STATE = GlobalState()
DEFAULT_PERSIST_DIR = "./chroma_db_store"
os.makedirs(DEFAULT_PERSIST_DIR, exist_ok=True)

# ==================================================================================
# í—¬í¼ í•¨ìˆ˜ (Helper Functions)
# ==================================================================================

def log(message: str) -> str:
    return f"[{datetime.now().strftime('%H:%M:%S')}] {message}\n"

def strip_think_tags(text: str) -> str:
    return re.sub(r"<think>.*?</think>", "", text, flags=re.DOTALL | re.IGNORECASE).strip()

def safe_json_extract(text: str) -> Optional[Dict | List]:
    match = re.search(r"```json\s*(\{.*\}|\[.*\])\s*```", text, re.DOTALL)
    if match:
        try: return json.loads(match.group(1))
        except json.JSONDecodeError: pass
    match = re.search(r'\{[\s\S]*\}|\[[\s\S]*\]', text)
    if match:
        try: return json.loads(match.group(0))
        except json.JSONDecodeError: pass
    return None

def call_vlm(image_paths: list, prompt: str, vlm_model: str) -> str:
    try:
        messages = [{"role": "user", "content": prompt, "images": image_paths}]
        response = ollama.chat(model=vlm_model, messages=messages)
        return response.get("message", {}).get("content", "VLM ëª¨ë¸ í˜¸ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        return f"VLM ëª¨ë¸ í˜¸ì¶œ ì˜¤ë¥˜: {e}\n'{vlm_model}' ëª¨ë¸ì´ ë¡œì»¬ì— ì„¤ì¹˜(pull)ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”."

# ==================================================================================
# íƒ­ 1: ğŸ’¬ ì±—ë´‡ (Chatbot)
# ==================================================================================

PERSONAS = {
    "ê¸°ë³¸ ìƒë‹´ì‚¬": "ë„ˆëŠ” ì‚¬ìš©ìë¥¼ ë„ì™€ì£¼ëŠ” ì¹œì ˆí•˜ê³  ìœ ëŠ¥í•œ ìƒë‹´ì‚¬ì•¼.",
    "ë°±ì„¤ê³µì£¼ ë§ˆë²•ê±°ìš¸": "ë„ˆëŠ” ë°±ì„¤ê³µì£¼ ì´ì•¼ê¸° ì†ì˜ ë§ˆë²• ê±°ìš¸ì´ì•¼. ê·¸ ì´ì•¼ê¸° ì†ì˜ ë§ˆë²• ê±°ìš¸ì˜ ìºë¦­í„°ì— ë¶€í•©í•˜ê²Œ, í’ˆìœ„ ìˆê³  ìš´ìœ¨ê° ìˆëŠ” ë§íˆ¬ë¡œ ë‹µë³€í•´ì¤˜.",
    "ìœ ì¹˜ì›ìƒ": "ë„ˆëŠ” 5ì‚´ ìœ ì¹˜ì›ìƒì´ì•¼. ëª¨ë“  ì§ˆë¬¸ì— ìœ ì¹˜ì›ìƒì²˜ëŸ¼ ìˆœìˆ˜í•˜ê³  ê·€ì—½ê²Œ ëŒ€ë‹µí•´ì¤˜. ì§§ê³  ì‰¬ìš´ ë‹¨ì–´ë¥¼ ì‚¬ìš©í•´.",
}

def handle_chatbot_stream(message: str, history: list, persona: str, temperature: float):
    if not message.strip():
        return

    llm = STATE.get_llm(temperature=temperature)
    system_prompt = PERSONAS.get(persona, PERSONAS["ê¸°ë³¸ ìƒë‹´ì‚¬"])
    
    messages_for_llm = [SystemMessage(content=system_prompt)]
    for turn in history:
        if turn["role"] == "user":
            messages_for_llm.append(HumanMessage(content=turn["content"]))
        elif turn["role"] == "assistant":
            messages_for_llm.append(AIMessage(content=turn["content"]))
            
    messages_for_llm.append(HumanMessage(content=message))

    response_stream = llm.stream(messages_for_llm)
    
    # First, yield the user message to the history
    history.append({"role": "user", "content": message})
    history.append({"role": "assistant", "content": ""})
    
    for chunk in response_stream:
        history[-1]["content"] += chunk.content
        yield history

# ==================================================================================
# íƒ­ 2: ğŸ“„ PDF & ë¬¸ì„œ (Documents)
# ... (ì´í•˜ ì½”ë“œëŠ” ë³€ê²½ ì—†ìŒ) ...
# ==================================================================================
def handle_pdf_summary(pdf_file, chunk_size: int, temperature: float, progress=gr.Progress()):
    if not pdf_file: return "PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.", ""
    llm = STATE.get_llm(temperature=temperature)
    progress(0, desc="PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...")
    doc = fitz.open(pdf_file.name)
    full_text = "\n".join([page.get_text() for page in doc])
    doc.close()
    if not full_text.strip(): return "PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", full_text
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=int(chunk_size*0.1))
    chunks = text_splitter.split_text(full_text)
    summaries = []
    system_prompt = "ë‹¤ìŒ í…ìŠ¤íŠ¸ëŠ” ë¬¸ì„œì˜ ì¼ë¶€ì…ë‹ˆë‹¤. ì´ ë¶€ë¶„ì˜ í•µì‹¬ ë‚´ìš©ì„ ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”."
    for i, chunk in enumerate(progress.tqdm(chunks, desc="ê° ì²­í¬ ìš”ì•½ ì¤‘...")):
        messages = [SystemMessage(content=system_prompt), HumanMessage(content=chunk)]
        summary = llm.invoke(messages).content
        summaries.append(summary)
    if len(summaries) == 1:
        final_summary = summaries[0]
    else:
        progress(0.8, desc="ë¶€ë¶„ ìš”ì•½ë“¤ì„ í†µí•© ì¤‘...")
        combined_summaries = "\n\n---\n\n".join(summaries)
        final_system_prompt = "ë‹¤ìŒì€ ë¬¸ì„œì˜ ê° ë¶€ë¶„ì„ ìš”ì•½í•œ ë‚´ìš©ë“¤ì…ë‹ˆë‹¤. ì´ë“¤ì„ ì¢…í•©í•˜ì—¬ ì „ì²´ ë¬¸ì„œì— ëŒ€í•œ í•˜ë‚˜ì˜ ì™„ì„±ëœ ìš”ì•½ì„ ì‘ì„±í•˜ì„¸ìš”. ì¤‘ë³µì„ ì œê±°í•˜ê³  ë…¼ë¦¬ì  íë¦„ì„ ë§ì¶”ì„¸ìš”."
        messages = [SystemMessage(content=final_system_prompt), HumanMessage(content=combined_summaries)]
        final_summary = llm.invoke(messages).content
    return final_summary, full_text

def handle_pdf_extraction(pdf_file, header_px: int, footer_px: int):
    if not pdf_file: return "PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.", [], None
    output_dir = Path("outputs/pdf_extraction"); output_dir.mkdir(parents=True, exist_ok=True)
    doc = fitz.open(pdf_file.name)
    full_text = ""; image_files = []
    for page_num, page in enumerate(doc):
        rect = page.rect
        text = page.get_text("text", clip=(0, header_px, rect.width, rect.height - footer_px))
        full_text += f"--- Page {page_num + 1} ---\n{text}\n\n"
        for img_index, img in enumerate(page.get_images(full=True)):
            xref = img[0]; base_image = doc.extract_image(xref)
            image_bytes, image_ext = base_image["image"], base_image["ext"]
            img_filename = f"page{page_num+1}_img{img_index+1}.{image_ext}"
            img_path = output_dir / img_filename
            with open(img_path, "wb") as f: f.write(image_bytes)
            image_files.append(str(img_path))
    doc.close()
    return full_text, image_files, str(output_dir)

def handle_pdf_table_extraction(pdf_file):
    if not pdf_file: return None, "PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.", None
    try: import camelot; import tabula
    except ImportError: return None, "í…Œì´ë¸” ì¶”ì¶œ ë¼ì´ë¸ŒëŸ¬ë¦¬(camelot-py[cv], tabula-py)ê°€ ì—†ìŠµë‹ˆë‹¤.", None
    output_dir = Path("outputs/table_extraction"); output_dir.mkdir(parents=True, exist_ok=True)
    pdf_path, log_text = pdf_file.name, ""
    log_text += log("Camelot (lattice)ìœ¼ë¡œ ì¶”ì¶œ ì‹œë„ ì¤‘...")
    try:
        tables_camelot = camelot.read_pdf(pdf_path, flavor='lattice', pages='all')
        if tables_camelot.n > 0:
            log_text += log(f"Camelotìœ¼ë¡œ {tables_camelot.n}ê°œì˜ í‘œ ë°œê²¬.")
            df, excel_path = tables_camelot[0].df, output_dir / "extracted_tables_camelot.xlsx"
            df.to_excel(excel_path, index=False); return df, log_text, str(excel_path)
    except Exception as e: log_text += log(f"Camelot (lattice) ì‹¤íŒ¨: {e}")
    log_text += log("Tabulaë¡œ ì¶”ì¶œ ì‹œë„ ì¤‘...")
    try:
        tables_tabula = tabula.read_pdf(pdf_path, pages='all', multiple_tables=True)
        if tables_tabula:
            log_text += log(f"Tabulaë¡œ {len(tables_tabula)}ê°œì˜ í‘œ ë°œê²¬.")
            df, excel_path = tables_tabula[0], output_dir / "extracted_tables_tabula.xlsx"
            df.to_excel(excel_path, index=False); return df, log_text, str(excel_path)
    except Exception as e: log_text += log(f"Tabula ì‹¤íŒ¨: {e}")
    return None, log_text + log("í‘œë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."), None

def handle_rag_indexing(pdf_files, chunk_size: int, chunk_overlap: int, progress=gr.Progress()):
    if not pdf_files: return "ìƒ‰ì¸í•  PDF íŒŒì¼ì„ í•˜ë‚˜ ì´ìƒ ì—…ë¡œë“œí•˜ì„¸ìš”.", gr.update(interactive=False)
    log_text = ""
    try:
        progress(0, desc="ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘..."); STATE.rag_embeddings = OllamaEmbeddings(model="nomic-embed-text")
        log_text += log("ì„ë² ë”© ëª¨ë¸(nomic-embed-text) ë¡œë“œ ì™„ë£Œ.")
        all_docs = []
        for pdf_file in progress.tqdm(pdf_files, desc="PDF íŒŒì¼ ì²˜ë¦¬ ì¤‘..."):
            loader = PyPDFLoader(pdf_file.name)
            docs = loader.load()
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
            splits = text_splitter.split_documents(docs)
            all_docs.extend(splits)
        log_text += log(f"ì´ {len(pdf_files)}ê°œ PDFì—ì„œ {len(all_docs)}ê°œì˜ ì²­í¬ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")
        progress(0.8, desc="ChromaDBì— ë²¡í„° ì €ì¥ ì¤‘...")
        STATE.rag_vectorstore = Chroma.from_documents(documents=all_docs, embedding=STATE.rag_embeddings, persist_directory=DEFAULT_PERSIST_DIR)
        log_text += log("ë²¡í„° DB ìƒ‰ì¸ ì™„ë£Œ. ì´ì œ ì§ˆë¬¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        return log_text, gr.update(interactive=True)
    except Exception as e: return f"ìƒ‰ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", gr.update(interactive=False)

def handle_rag_chat_stream(message: str, history: list, temperature: float):
    if not STATE.rag_vectorstore: yield "ë¨¼ì € PDF íŒŒì¼ì„ ìƒ‰ì¸í•´ì£¼ì„¸ìš”."; return
    llm = STATE.get_llm(temperature=temperature)
    retriever = STATE.rag_vectorstore.as_retriever(search_kwargs={"k": 5})
    
    history.append({"role": "user", "content": message})
    
    retrieved_docs = retriever.invoke(message)
    STATE.last_rag_sources = [{"source": doc.metadata.get('source', 'N/A'), "page": doc.metadata.get('page', 'N/A'), "content": doc.page_content} for doc in retrieved_docs]
    system_prompt = ("ë„ˆëŠ” ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì— ê¸°ë°˜í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì•¼. " "ì»¨í…ìŠ¤íŠ¸ì— ë‹µë³€ì˜ ê·¼ê±°ê°€ ì—†ìœ¼ë©´, ì¶”ì¸¡í•˜ì§€ ë§ê³  'ì œê³µëœ ë¬¸ì„œì—ì„œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'ë¼ê³  ë§í•´.")
    prompt_template = ChatPromptTemplate.from_messages([("system", system_prompt), ("system", "ì»¨í…ìŠ¤íŠ¸:\n{context}"), ("human", "{input}")])
    chain = create_stuff_documents_chain(llm, prompt_template)
    
    history.append({"role": "assistant", "content": ""})
    
    response = ""
    for chunk in chain.stream({"input": message, "context": retrieved_docs}):
        response += chunk
        history[-1]["content"] = response
        yield history

def update_rag_sources_display():
    if not STATE.last_rag_sources: return "ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."
    md = "### ğŸ“š ë‹µë³€ì— ì°¸ì¡°ëœ ë¬¸ì„œ\n\n"
    for i, src in enumerate(STATE.last_rag_sources):
        md += f"**[ì¶œì²˜ {i+1}]**\n- **íŒŒì¼**: `{os.path.basename(src['source'])}` (í˜ì´ì§€: {src['page']})\n- **ë‚´ìš© ì¼ë¶€**: {src['content'][:200]}...\n\n"
    return md

def handle_single_image_analysis(image, prompt: str, vlm_model: str, use_refine: bool, temperature: float):
    if image is None: return "ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.", ""
    analysis = call_vlm([image.name], prompt, vlm_model)
    if not use_refine: return analysis, analysis
    refine_prompt = (f"ë‹¤ìŒì€ ì´ë¯¸ì§€ì— ëŒ€í•œ 1ì°¨ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤. ì´ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ, ì „ë¬¸ ë°ì´í„° í•´ì„¤ê°€ì²˜ëŸ¼ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ì§€ë§Œ ì •ë³´ ë°€ë„ê°€ ë†’ì€ ìµœì¢… ìš”ì•½ì„ ì‘ì„±í•´ì£¼ì„¸ìš”. ì†Œì œëª©, ë¶ˆë¦¿í¬ì¸íŠ¸ ë“±ì„ í™œìš©í•˜ì—¬ ê°€ë…ì„±ì„ ë†’ì—¬ì£¼ì„¸ìš”.\n\n--- 1ì°¨ ë¶„ì„ ê²°ê³¼ ---\n{analysis}")
    llm = STATE.get_llm(temperature=temperature)
    refined_analysis = llm.invoke(refine_prompt).content
    return refined_analysis, analysis

def handle_image_comparison(image1, image2, prompt: str, vlm_model: str, use_refine: bool, temperature: float):
    if image1 is None or image2 is None: return "ë‘ ê°œì˜ ì´ë¯¸ì§€ë¥¼ ëª¨ë‘ ì—…ë¡œë“œí•˜ì„¸ìš”.", ""
    comparison = call_vlm([image1.name, image2.name], prompt, vlm_model)
    if not use_refine: return comparison, comparison
    refine_prompt = (f"ë‹¤ìŒì€ ë‘ ì´ë¯¸ì§€ì— ëŒ€í•œ ë¹„êµ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤. ì´ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ, ê²°ê³¼ë¥¼ ëª…í™•í•œ êµ¬ì¡°ë¡œ ì¬ì •ë¦¬í•´ì£¼ì„¸ìš”.\nì˜ˆ: '## ê³µí†µì ', '## ì°¨ì´ì ', '## ìš”ì•½ ê²°ë¡ 'ê³¼ ê°™ì´ ì†Œì œëª©ê³¼ ë¶ˆë¦¿í¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.\n\n--- 1ì°¨ ë¹„êµ ê²°ê³¼ ---\n{comparison}")
    llm = STATE.get_llm(temperature=temperature)
    refined_comparison = llm.invoke(refine_prompt).content
    return refined_comparison, comparison

# [FIX] Mermaid handler now returns raw markdown string, not a component
def handle_mermaid_generation(idea: str, diagram_type: str, use_llm: bool):
    if not idea.strip(): return "", "", "ì•„ì´ë””ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”."
    if not use_llm:
        code, status = idea, "LLM ë¯¸ì‚¬ìš©"
    else:
        llm = STATE.get_llm(temperature=0.1)
        system_prompt = (f"ë‹¹ì‹ ì€ Mermaid ë‹¤ì´ì–´ê·¸ë¨ ì½”ë“œ ìƒì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì•„ì´ë””ì–´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ìš”ì²­ëœ `{diagram_type}` íƒ€ì…ì˜ ìœ íš¨í•œ Mermaid ì½”ë“œë§Œ ìƒì„±í•˜ì„¸ìš”. ì½”ë“œë¸”ë¡(```mermaid ... ```)ìœ¼ë¡œ ê°ì‹¸ì„œ ì¶œë ¥í•˜ê³ , ë‹¤ë¥¸ ì„¤ëª…ì€ ë§ë¶™ì´ì§€ ë§ˆì„¸ìš”.")
        response = llm.invoke([SystemMessage(content=system_prompt), HumanMessage(content=idea)]).content
        match = re.search(r"```mermaid\s*([\s\S]*?)```", response)
        code, status = (match.group(1).strip(), "Mermaid ì½”ë“œ ìƒì„± ì™„ë£Œ.") if match else (response, "ì½”ë“œ ë¸”ë¡ ì¶”ì¶œ ì‹¤íŒ¨.")
    return f"```mermaid\n{code}\n```", code, status

def handle_sd_prompt_generation(idea: str):
    if not idea.strip(): return "", "", "ì•„ì´ë””ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”."
    llm = STATE.get_llm(temperature=0.8)
    system_prompt = ("ë‹¹ì‹ ì€ Stable Diffusion í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì•„ì´ë””ì–´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì´ë¯¸ì§€ ìƒì„±ì— ìµœì í™”ëœ ìƒì„¸í•œ í”„ë¡¬í”„íŠ¸ì™€ ë„¤ê±°í‹°ë¸Œ í”„ë¡¬í”„íŠ¸ë¥¼ ë§Œë“œì„¸ìš”.\nì¶œë ¥ì€ ë°˜ë“œì‹œ JSON í˜•ì‹ì´ì–´ì•¼ í•˜ë©°, 'prompt'ì™€ 'negative_prompt' ë‘ ê°œì˜ í‚¤ë¥¼ ê°€ì ¸ì•¼ í•©ë‹ˆë‹¤. ë‹¤ë¥¸ ì„¤ëª…ì€ ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”. 'best quality, masterpiece, 8k, photorealistic' ë“±ì˜ í‚¤ì›Œë“œë¥¼ ì ê·¹ í™œìš©í•˜ì„¸ìš”.")
    response = llm.invoke([SystemMessage(content=system_prompt), HumanMessage(content=f"ì•„ì´ë””ì–´: {idea}")]).content
    data = safe_json_extract(response)
    if isinstance(data, dict):
        prompt, negative = data.get("prompt", idea), data.get("negative_prompt", "low quality, worst quality, blurry")
        return prompt, negative, "í”„ë¡¬í”„íŠ¸ ìƒì„± ì™„ë£Œ."
    return idea, "", f"JSON íŒŒì‹± ì‹¤íŒ¨. ì›ë³¸ ì‘ë‹µ:\n{response}"

def handle_structured_ocr(image, use_llm: bool):
    if image is None: return None, "", None, "ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”."
    if easyocr is None: return None, "EasyOCRì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", None, ""
    try:
        reader = easyocr.Reader(['ko', 'en'])
        np_img = np.array(image)
        ocr_results = reader.readtext(np_img)
        overlay, draw = image.copy(), ImageDraw.Draw(overlay)
        full_text = "\n".join([res[1] for res in ocr_results])
        for (bbox, text, prob) in ocr_results:
            draw.polygon([tuple(p) for p in bbox], outline="cyan", width=2)
        if not use_llm: return overlay, full_text, None, "LLM ë¯¸ì‚¬ìš©"
        system_prompt = ("ë‹¤ìŒì€ ì˜ìˆ˜ì¦ ë˜ëŠ” í…Œì´ë¸”ì—ì„œ OCRë¡œ ì¶”ì¶œí•œ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤. ì´ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ êµ¬ì¡°í™”ëœ JSONìœ¼ë¡œ ë³€í™˜í•˜ì„¸ìš”. ì˜ìˆ˜ì¦ì˜ ê²½ìš° 'vendor', 'date', 'total', 'items' ([{description, qty, price}]) ë“±ì„ í¬í•¨í•˜ê³ , ì¼ë°˜ í…Œì´ë¸”ì˜ ê²½ìš° 'columns'ì™€ 'rows'ë¥¼ í¬í•¨í•˜ì—¬ í‘œ í˜•ì‹ìœ¼ë¡œ ë§Œë“œì„¸ìš”. JSON ì™¸ì˜ ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” ì ˆëŒ€ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”.")
        llm = STATE.get_llm(temperature=0.1)
        response = llm.invoke([SystemMessage(content=system_prompt), HumanMessage(content=full_text)]).content
        json_data = safe_json_extract(response)
        df = None
        if isinstance(json_data, dict):
            if 'items' in json_data and isinstance(json_data['items'], list): df = pd.DataFrame(json_data['items'])
            elif 'rows' in json_data and 'columns' in json_data: df = pd.DataFrame(json_data['rows'], columns=json_data['columns'])
        return overlay, full_text, df, json.dumps(json_data, ensure_ascii=False, indent=2)
    except Exception as e: return None, str(e), None, ""

def handle_sequential_thinking(query: str, temperature: float, progress=gr.Progress()):
    if not query.strip(): return "", "", "", "ì§ˆì˜ë¥¼ ì…ë ¥í•˜ì„¸ìš”."
    llm = STATE.get_llm(temperature=temperature)
    progress(0.1, desc="[1/3] ë¶„ì„ ì¤‘...")
    analysis_prompt = ("ë‹¹ì‹ ì€ ì‹ ì¤‘í•œ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì§ˆì˜ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ í•µì‹¬ ìŸì , ê°€ì •, ì ‘ê·¼ ê³„íšì„ ê°„ê²°í•œ ë¶ˆë¦¿ í¬ì¸íŠ¸ë¡œ ì •ë¦¬í•˜ì„¸ìš”. ìµœì¢… ë‹µë³€ì´ ì•„ë‹Œ, ë¶„ì„ ìš”ì•½ë§Œ ì œì‹œí•˜ì„¸ìš”.")
    analysis = llm.invoke([SystemMessage(content=analysis_prompt), HumanMessage(content=query)]).content
    progress(0.5, desc="[2/3] ìµœì¢… ë‹µë³€ ìƒì„± ì¤‘...")
    answer_prompt = ("ì•ì„  ë¶„ì„ì„ ë°”íƒ•ìœ¼ë¡œ, ì‚¬ìš©ìì˜ ì§ˆì˜ì— ëŒ€í•œ ìµœì¢…ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”. ëª…í™•í•˜ê³  êµ¬ì¡°ì ì¸ í˜•ì‹ìœ¼ë¡œ ì œì‹œí•˜ì„¸ìš”.")
    answer = llm.invoke([SystemMessage(content=answer_prompt), HumanMessage(content=f"ì§ˆì˜: {query}\n\n--- ë¶„ì„ ìš”ì•½ ---\n{analysis}")]).content
    progress(0.8, desc="[3/3] ë‹µë³€ ê²€ì¦ ì¤‘...")
    reflection_prompt = ("ë‹¹ì‹ ì€ ì „ë¬¸ ê²€ì¦ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ìµœì¢… ë‹µë³€ì„ ê²€í† í•˜ê³ , ì ì¬ì ì¸ ì˜¤ë¥˜, ëˆ„ë½, ê°œì„ ì ì„ ì²´í¬ë¦¬ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ì œì‹œí•˜ì„¸ìš”.")
    reflection = llm.invoke([SystemMessage(content=reflection_prompt), HumanMessage(content=f"ì§ˆì˜: {query}\n\n--- ìµœì¢… ë‹µë³€ ---\n{answer}")]).content
    return analysis, answer, reflection, "ëª¨ë“  ë‹¨ê³„ ì™„ë£Œ."

def handle_web_rag(query: str, temperature: float, progress=gr.Progress()):
    llm = STATE.get_llm(temperature=temperature)
    progress(0, desc="ì›¹ ê²€ìƒ‰ ì¤‘...")
    search = DuckDuckGoSearchResults()
    search_results = search.run(query)
    links = re.findall(r'https?://[^\s,\]]+', search_results)
    if not links: return "ê´€ë ¨ ì›¹ í˜ì´ì§€ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.", search_results, ""
    progress(0.3, desc=f"{len(links)}ê°œ ë§í¬ì—ì„œ ì½˜í…ì¸  ìˆ˜ì§‘ ì¤‘...")
    try:
        loader = WebBaseLoader(web_paths=links[:3], bs_kwargs={"parse_only": "p"})
        docs = loader.load()
        context = "\n\n---\n\n".join([doc.page_content for doc in docs])
    except Exception as e:
        context = f"ì½˜í…ì¸  ìˆ˜ì§‘ ì‹¤íŒ¨: {e}"
    progress(0.7, desc="ë‹µë³€ ìƒì„± ì¤‘...")
    system_prompt = ("ë„ˆëŠ” ì œê³µëœ ìµœì‹  ì›¹ ê²€ìƒ‰ ê²°ê³¼ì— ê¸°ë°˜í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” AIì•¼. ì»¨í…ìŠ¤íŠ¸ì— ê·¼ê±°í•˜ì—¬ ì •í™•í•˜ê²Œ ë‹µë³€í•´ì¤˜.")
    response = llm.invoke([SystemMessage(content=system_prompt), HumanMessage(content=f"ì›¹ ê²€ìƒ‰ ê²°ê³¼:\n{context}\n\n--- ì§ˆë¬¸ ---\n{query}")]).content
    return response, search_results, "\n".join(links[:3])

# ==================================================================================
# ğŸ’ Gradio UI ë¹Œë“œ
# ==================================================================================
def build_ui():
    with gr.Blocks(theme=gr.themes.Soft(), title="DeepSeek-R1 AI Toolkit") as demo:
        gr.Markdown("# ğŸš€ DeepSeek-R1 AI Toolkit (All-in-One)")
        gr.Markdown("38ê°œ ì˜ˆì œ ì½”ë“œë¥¼ í†µí•©í•œ ë¡œì»¬ AI ì• í”Œë¦¬ì¼€ì´ì…˜ì…ë‹ˆë‹¤. ëª¨ë“  ê¸°ëŠ¥ì€ ë¡œì»¬ Ollama ì„œë²„ë¥¼ í†µí•´ ì‹¤í–‰ë©ë‹ˆë‹¤.")

        with gr.Tab("ğŸ’¬ ì±—ë´‡ (Chatbots)"):
            with gr.Row():
                with gr.Column(scale=1):
                    chatbot_persona = gr.Dropdown(list(PERSONAS.keys()), value="ê¸°ë³¸ ìƒë‹´ì‚¬", label="ğŸ¤– í˜ë¥´ì†Œë‚˜ ì„ íƒ")
                    chatbot_temp = gr.Slider(0.0, 1.5, value=0.7, step=0.1, label="ì°½ì˜ì„± (Temperature)")
                with gr.Column(scale=3):
                    chatbot_window = gr.Chatbot(height=500, label="ëŒ€í™”ì°½", type="messages")
                    with gr.Row():
                        chatbot_msg = gr.Textbox(label="ë©”ì‹œì§€ ì…ë ¥", placeholder="ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”...", scale=4, container=False)
                        chatbot_submit_btn = gr.Button("ì „ì†¡", variant="primary", scale=1)
                        chatbot_clear = gr.Button("ì´ˆê¸°í™”")

            def on_chatbot_submit(message, history, persona, temperature):
                stream = handle_chatbot_stream(message, history, persona, temperature)
                for new_history in stream:
                    yield new_history, ""

            chatbot_msg.submit(on_chatbot_submit, [chatbot_msg, chatbot_window, chatbot_persona, chatbot_temp], [chatbot_window, chatbot_msg])
            chatbot_submit_btn.click(on_chatbot_submit, [chatbot_msg, chatbot_window, chatbot_persona, chatbot_temp], [chatbot_window, chatbot_msg])
            chatbot_clear.click(lambda: [], None, chatbot_window, queue=False)

        with gr.Tab("ğŸ“„ PDF & ë¬¸ì„œ (Documents)"):
            with gr.Tabs():
                with gr.TabItem("ğŸ“‘ PDF ìš”ì•½"):
                    gr.Markdown("PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ ê¸´ ë‚´ìš©ì„ ì²­í¬ ë‹¨ìœ„ë¡œ ìš”ì•½í•œ í›„, ë‹¤ì‹œ ìµœì¢… ìš”ì•½ë³¸ìœ¼ë¡œ í†µí•©í•©ë‹ˆë‹¤.")
                    with gr.Row():
                        pdf_sum_input = gr.File(label="PDF íŒŒì¼ ì—…ë¡œë“œ", file_types=[".pdf"])
                        with gr.Column():
                             pdf_sum_chunk_size = gr.Slider(1000, 8000, value=4000, step=500, label="ì²­í¬ í¬ê¸°")
                             pdf_sum_temp = gr.Slider(0.0, 1.0, value=0.2, step=0.1, label="ì°½ì˜ì„±")
                             pdf_sum_btn = gr.Button("ìš”ì•½ ì‹¤í–‰", variant="primary")
                    pdf_sum_output = gr.Textbox(label="ìµœì¢… ìš”ì•½ ê²°ê³¼", lines=15)
                    with gr.Accordion("ì¶”ì¶œëœ ì „ì²´ í…ìŠ¤íŠ¸", open=False):
                        pdf_sum_fulltext = gr.Textbox(label="PDF ì›ë³¸ í…ìŠ¤íŠ¸", lines=10)
                    pdf_sum_btn.click(handle_pdf_summary, [pdf_sum_input, pdf_sum_chunk_size, pdf_sum_temp], [pdf_sum_output, pdf_sum_fulltext])

                with gr.TabItem("âœ‚ï¸ í…ìŠ¤íŠ¸/ì´ë¯¸ì§€ ì¶”ì¶œ"):
                    # ... (UI Code for this sub-tab remains the same)
                    gr.Markdown("PDFì—ì„œ í—¤ë”ì™€ í‘¸í„°ë¥¼ ì œì™¸í•œ ë³¸ë¬¸ í…ìŠ¤íŠ¸ì™€ í¬í•¨ëœ ëª¨ë“  ì´ë¯¸ì§€ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.")
                    with gr.Row():
                        pdf_ext_input = gr.File(label="PDF íŒŒì¼ ì—…ë¡œë“œ", file_types=[".pdf"])
                        with gr.Column():
                            pdf_ext_header = gr.Slider(0, 200, value=60, label="í—¤ë” ë†’ì´ (px)")
                            pdf_ext_footer = gr.Slider(0, 200, value=60, label="í‘¸í„° ë†’ì´ (px)")
                            pdf_ext_btn = gr.Button("ì¶”ì¶œ ì‹¤í–‰", variant="primary")
                    with gr.Row():
                         pdf_ext_text = gr.Textbox(label="ì¶”ì¶œëœ í…ìŠ¤íŠ¸", lines=15, scale=2)
                         with gr.Column(scale=1):
                             pdf_ext_gallery = gr.Gallery(label="ì¶”ì¶œëœ ì´ë¯¸ì§€", height=400)
                             pdf_ext_path = gr.Textbox(label="ì´ë¯¸ì§€ ì €ì¥ ê²½ë¡œ")
                    pdf_ext_btn.click(handle_pdf_extraction, [pdf_ext_input, pdf_ext_header, pdf_ext_footer], [pdf_ext_text, pdf_ext_gallery, pdf_ext_path])

                with gr.TabItem("ğŸ“Š í‘œ ì¶”ì¶œ (Camelot & Tabula)"):
                    # ... (UI Code for this sub-tab remains the same)
                    gr.Markdown("PDF ë‚´ë¶€ì˜ í‘œë¥¼ ì¸ì‹í•˜ì—¬ ë°ì´í„°í”„ë ˆì„ê³¼ ì—‘ì…€ íŒŒì¼ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤. (ì²˜ë¦¬ ì‹œê°„ì´ ë‹¤ì†Œ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
                    pdf_tbl_input = gr.File(label="í‘œê°€ í¬í•¨ëœ PDF ì—…ë¡œë“œ", file_types=[".pdf"])
                    pdf_tbl_btn = gr.Button("í‘œ ì¶”ì¶œ ì‹¤í–‰", variant="primary")
                    pdf_tbl_df = gr.Dataframe(label="ì¶”ì¶œëœ í‘œ (ê°€ì¥ í° í‘œ ê¸°ì¤€)")
                    pdf_tbl_log = gr.Textbox(label="ì²˜ë¦¬ ë¡œê·¸", lines=5)
                    pdf_tbl_file = gr.File(label="ì—‘ì…€ íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œ")
                    pdf_tbl_btn.click(handle_pdf_table_extraction, [pdf_tbl_input], [pdf_tbl_df, pdf_tbl_log, pdf_tbl_file])

        with gr.Tab("ğŸ§  RAG ì±—ë´‡ (RAG Chatbot)"):
            with gr.Row():
                with gr.Column(scale=1):
                    gr.Markdown("##### 1. ë¬¸ì„œ ìƒ‰ì¸")
                    rag_pdfs = gr.File(label="ìƒ‰ì¸í•  PDF íŒŒì¼ë“¤", file_count="multiple", file_types=[".pdf"])
                    rag_chunk_size = gr.Slider(200, 2000, value=1000, label="ì²­í¬ í¬ê¸°")
                    rag_chunk_overlap = gr.Slider(0, 500, value=100, label="ì²­í¬ ì¤‘ì²©")
                    rag_index_btn = gr.Button("PDF ìƒ‰ì¸ ì‹œì‘", variant="primary")
                    rag_status = gr.Textbox(label="ìƒ‰ì¸ ìƒíƒœ", lines=10)
                with gr.Column(scale=2):
                    gr.Markdown("##### 2. ë¬¸ì„œ ê¸°ë°˜ ëŒ€í™”")
                    rag_chatbot = gr.Chatbot(height=450, label="ë¬¸ì„œ ê¸°ë°˜ ëŒ€í™”ì°½", type="messages")
                    with gr.Accordion("ì°¸ì¡°ëœ ë¬¸ì„œ ì†ŒìŠ¤", open=False):
                        rag_sources_md = gr.Markdown()
                    rag_chat_temp = gr.Slider(0.0, 1.0, value=0.3, label="ì°½ì˜ì„±")
                    rag_msg = gr.Textbox(label="ì§ˆë¬¸ ì…ë ¥", placeholder="ìƒ‰ì¸ëœ ë¬¸ì„œì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”...", interactive=False, container=False)
                    rag_submit_btn = gr.Button("ì „ì†¡", variant="primary")

            rag_index_btn.click(handle_rag_indexing, [rag_pdfs, rag_chunk_size, rag_chunk_overlap], [rag_status, rag_msg])

            def on_rag_submit(message, history, temperature):
                stream = handle_rag_chat_stream(message, history, temperature)
                for new_history in stream:
                    yield new_history, ""
            
            rag_msg.submit(on_rag_submit, [rag_msg, rag_chatbot, rag_chat_temp], [rag_chatbot, rag_msg]).then(update_rag_sources_display, None, rag_sources_md)
            rag_submit_btn.click(on_rag_submit, [rag_msg, rag_chatbot, rag_chat_temp], [rag_chatbot, rag_msg]).then(update_rag_sources_display, None, rag_sources_md)

        with gr.Tab("ğŸ–¼ï¸ ì´ë¯¸ì§€ ë¶„ì„ (Vision)"):
            # ... (UI Code for this tab remains the same)
            gr.Markdown("Ollamaì— ì„¤ì¹˜ëœ Vision Language Model(VLM)ì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤. `deepseek-r1`ì´ ì•„ë‹Œ VLMì´ ë¨¼ì € ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.")
            vlm_model_dd = gr.Dropdown(["qwen2.5-vl", "llava:latest"], value="qwen2.5-vl", label="VLM ëª¨ë¸ ì„ íƒ", info="Ollamaì— í•´ë‹¹ ëª¨ë¸ì´ pull ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.")
            with gr.Tabs():
                with gr.TabItem("ğŸ‘ï¸ ë‹¨ì¼ ì´ë¯¸ì§€ ì„¤ëª…"):
                    with gr.Row():
                        vision_img_single = gr.Image(type="pil", label="ì´ë¯¸ì§€ ì—…ë¡œë“œ")
                        with gr.Column():
                            vision_prompt_single = gr.Textbox(label="ìš”ì²­ í”„ë¡¬í”„íŠ¸", value="ì´ ì´ë¯¸ì§€ì— ëŒ€í•´ ìƒì„¸íˆ ì„¤ëª…í•´ì¤˜.", lines=3)
                            vision_refine_single = gr.Checkbox(label="DeepSeek-R1ìœ¼ë¡œ ê²°ê³¼ ì •ì œ", value=True)
                            vision_btn_single = gr.Button("ë¶„ì„ ì‹¤í–‰", variant="primary")
                    vision_output_single = gr.Textbox(label="ìµœì¢… ë¶„ì„ ê²°ê³¼", lines=10)
                    with gr.Accordion("VLM ì›ë³¸ ì¶œë ¥", open=False):
                        vision_raw_single = gr.Textbox(label="1ì°¨ ë¶„ì„ ê²°ê³¼(VLM)")
                    vision_btn_single.click(handle_single_image_analysis, [vision_img_single, vision_prompt_single, vlm_model_dd, vision_refine_single, chatbot_temp], [vision_output_single, vision_raw_single])
                with gr.TabItem("ğŸ†š ë‘ ì´ë¯¸ì§€ ë¹„êµ"):
                    with gr.Row():
                        vision_img1 = gr.Image(type="pil", label="ì´ë¯¸ì§€ 1")
                        vision_img2 = gr.Image(type="pil", label="ì´ë¯¸ì§€ 2")
                    vision_prompt_comp = gr.Textbox(label="ë¹„êµ ìš”ì²­ í”„ë¡¬í”„íŠ¸", value="ë‘ ì´ë¯¸ì§€ì˜ ê³µí†µì ê³¼ ì°¨ì´ì ì„ ë¹„êµ ë¶„ì„í•´ì¤˜.", lines=3)
                    vision_refine_comp = gr.Checkbox(label="DeepSeek-R1ìœ¼ë¡œ ê²°ê³¼ ì •ì œ", value=True)
                    vision_btn_comp = gr.Button("ë¹„êµ ì‹¤í–‰", variant="primary")
                    vision_output_comp = gr.Textbox(label="ìµœì¢… ë¹„êµ ê²°ê³¼", lines=10)
                    with gr.Accordion("VLM ì›ë³¸ ì¶œë ¥", open=False):
                        vision_raw_comp = gr.Textbox(label="1ì°¨ ë¹„êµ ê²°ê³¼(VLM)")
                    vision_btn_comp.click(handle_image_comparison, [vision_img1, vision_img2, vision_prompt_comp, vlm_model_dd, vision_refine_comp, chatbot_temp], [vision_output_comp, vision_raw_comp])

        with gr.Tab("ğŸ› ï¸ ê³ ê¸‰ ìƒì„±ê¸° (Advanced Generators)"):
            with gr.Tabs():
                with gr.TabItem("ğŸŒŠ Mermaid ë‹¤ì´ì–´ê·¸ë¨"):
                    with gr.Row():
                        with gr.Column():
                            mermaid_idea = gr.Textbox(label="ì•„ì´ë””ì–´ ë˜ëŠ” Mermaid ì½”ë“œ", lines=5, placeholder="ì˜ˆ: ì‚¬ìš©ì -> ë¡œê·¸ì¸ -> ë©”ì¸ í˜ì´ì§€")
                            mermaid_type = gr.Dropdown(["flowchart", "sequenceDiagram", "gantt", "pie", "classDiagram"], value="flowchart", label="ë‹¤ì´ì–´ê·¸ë¨ íƒ€ì…")
                            mermaid_use_llm = gr.Checkbox(value=True, label="LLMìœ¼ë¡œ ì½”ë“œ ìƒì„±/ë³´ì •")
                            mermaid_btn = gr.Button("ìƒì„±/ë Œë”ë§", variant="primary")
                        with gr.Column():
                            mermaid_output = gr.Markdown(label="Mermaid ë Œë”ë§")
                            mermaid_code = gr.Code(label="ìƒì„±ëœ ì½”ë“œ", language="markdown")
                            mermaid_status = gr.Textbox(label="ìƒíƒœ", interactive=False)
                    def on_mermaid_generate(idea, dtype, use_llm):
                        md_text, code_text, status_text = handle_mermaid_generation(idea, dtype, use_llm)
                        return md_text, code_text, status_text
                    mermaid_btn.click(on_mermaid_generate, [mermaid_idea, mermaid_type, mermaid_use_llm], [mermaid_output, mermaid_code, mermaid_status])

                with gr.TabItem("ğŸ¨ Stable Diffusion í”„ë¡¬í”„íŠ¸"):
                    sd_idea = gr.Textbox(label="ì´ë¯¸ì§€ ì•„ì´ë””ì–´", placeholder="ì˜ˆ: ì€í•˜ìˆ˜ë¥¼ ë°°ê²½ìœ¼ë¡œ ìº í•‘í•˜ëŠ” ìš°ì£¼ë¹„í–‰ì‚¬", lines=3)
                    sd_btn = gr.Button("í”„ë¡¬í”„íŠ¸ ìƒì„±", variant="primary")
                    sd_prompt = gr.Textbox(label="âœ… ìƒì„±ëœ í”„ë¡¬í”„íŠ¸", lines=4)
                    sd_neg_prompt = gr.Textbox(label="ğŸš« ìƒì„±ëœ ë„¤ê±°í‹°ë¸Œ í”„ë¡¬í”„íŠ¸", lines=2)
                    sd_status = gr.Textbox(label="ìƒíƒœ", interactive=False)
                    sd_btn.click(handle_sd_prompt_generation, [sd_idea], [sd_prompt, sd_neg_prompt, sd_status])
                
                with gr.TabItem("ğŸ§¾ êµ¬ì¡°í™”ëœ OCR"):
                    ocr_img_input = gr.Image(type="pil", label="ì˜ìˆ˜ì¦ ë˜ëŠ” í‘œ ì´ë¯¸ì§€")
                    ocr_use_llm = gr.Checkbox(value=True, label="DeepSeek-R1ìœ¼ë¡œ êµ¬ì¡°í™”")
                    ocr_btn = gr.Button("OCR ì‹¤í–‰", variant="primary")
                    with gr.Row():
                        ocr_overlay = gr.Image(label="OCR ì¸ì‹ ì˜ì—­")
                        ocr_df = gr.Dataframe(label="êµ¬ì¡°í™”ëœ ë°ì´í„°")
                    with gr.Accordion("ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ë° JSON", open=False):
                        ocr_text = gr.Textbox(label="ì „ì²´ í…ìŠ¤íŠ¸")
                        ocr_json = gr.Code(label="JSON ê²°ê³¼", language="json")
                    ocr_btn.click(handle_structured_ocr, [ocr_img_input, ocr_use_llm], [ocr_overlay, ocr_text, ocr_df, ocr_json])

        with gr.Tab("ğŸŒ ì›¹ & ë„êµ¬ (Agents)"):
            gr.Markdown("ì›¹ ê²€ìƒ‰(DuckDuckGo)ì„ í†µí•´ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ê³  ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤.")
            web_rag_query = gr.Textbox(label="ì›¹ì— ì§ˆë¬¸í•˜ê¸°", placeholder="ì˜ˆ: 2025ë…„ ì „ê¸°ì°¨ ì‹œì¥ ì „ë§ì€?", lines=2)
            web_rag_btn = gr.Button("ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„±", variant="primary")
            web_rag_answer = gr.Textbox(label="ë‹µë³€", lines=10)
            with gr.Accordion("ê²€ìƒ‰ ê³¼ì • ë° ì°¸ì¡° ë§í¬", open=False):
                web_rag_results = gr.Textbox(label="DuckDuckGo ê²€ìƒ‰ ê²°ê³¼")
                web_rag_links = gr.Textbox(label="ì°¸ì¡°ëœ URL")
            web_rag_btn.click(handle_web_rag, [web_rag_query, chatbot_temp], [web_rag_answer, web_rag_results, web_rag_links])

        with gr.Tab("ğŸ¤” ìˆœì°¨ì  ì‚¬ê³  (Sequential Thinking)"):
            gr.Markdown("`ë¶„ì„` â†’ `ë‹µë³€` â†’ `ê²€ì¦`ì˜ 3ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ì„ í†µí•´ ê¹Šì´ ìˆëŠ” ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.")
            seq_query = gr.Textbox(label="ê¹Šì€ ì‚¬ê³ ê°€ í•„ìš”í•œ ì§ˆë¬¸", placeholder="ì˜ˆ: ì¸ê³µì§€ëŠ¥ì´ ì €ì‘ê¶Œë²•ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì€?", lines=3)
            seq_btn = gr.Button("ì‚¬ê³  ì‹œì‘", variant="primary")
            seq_status = gr.Textbox(label="ì§„í–‰ ìƒíƒœ", interactive=False)
            with gr.Row():
                seq_analysis = gr.Markdown(label="## 1. ë¶„ì„ (Analysis)")
                seq_answer = gr.Markdown(label="## 2. ìµœì¢… ë‹µë³€ (Final Answer)")
                seq_reflection = gr.Markdown(label="## 3. ê²€ì¦ (Reflection)")
            seq_btn.click(handle_sequential_thinking, [seq_query, chatbot_temp], [seq_analysis, seq_answer, seq_reflection, seq_status])

    return demo

if __name__ == "__main__":
    app = build_ui()
    app.launch(server_name="0.0.0.0", server_port=7860, inbrowser=True)

````