# =============================
# ğŸš€ DeepSeek-R1 AI Toolkit (ë¹ ë¥¸ ì‹¤í–‰ ìµœì í™” ë²„ì „) â€” Gradio queue ìˆ˜ì •
# =============================

# ---------- í™˜ê²½/ì„¤ì¹˜ ìµœì†Œí™” & ìºì‹± ----------
print("ğŸ“¦ í™˜ê²½ ì ê²€ ì¤‘...")

import os, shutil, subprocess, time, re, json
from datetime import datetime
from pathlib import Path

def sh(cmd: str):
    return subprocess.run(cmd, shell=True, check=False, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)

# Java (tabula/camelot í•„ìš”) â€” ì—†ì„ ë•Œë§Œ ì„¤ì¹˜
if shutil.which("java") is None:
    print("ğŸ‘‰ Java ì„¤ì¹˜")
    sh("apt-get update -qq && apt-get install -y openjdk-11-jdk-headless")
os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-11-openjdk-amd64'

# pip íŒ¨í‚¤ì§€ â€” í•„ìš”í•œ ê²ƒë§Œ & ì—†ì„ ë•Œë§Œ
def ensure(mod: str, pip_name: str = None):
    pip_name = pip_name or mod
    try:
        __import__(mod)
    except Exception:
        print(f"ğŸ“¥ install {pip_name}")
        sh(f"pip install -q {pip_name}")

print("ğŸ í•„ìˆ˜ íŒ¨í‚¤ì§€ í™•ì¸")
for mod, pipn in [
    ("gradio","gradio"),
    ("ollama","ollama"),
    ("fitz","pymupdf"),
    ("pandas","pandas"),
    ("PIL","Pillow"),
    ("numpy","numpy"),
    ("langchain_community","langchain-community"),
    ("langchain_chroma","langchain-chroma"),
    ("langchain_text_splitters","langchain-text-splitters"),
    ("beautifulsoup4","beautifulsoup4"),
    ("duckduckgo_search","duckduckgo-search"),
]:
    ensure(mod, pipn)

# Ollama ì„¤ì¹˜
print("ğŸ¦™ Ollama ì¤€ë¹„")
if shutil.which("ollama") is None:
    sh("curl -fsSL https://ollama.com/install.sh | sh")

# ì„œë²„ ë°±ê·¸ë¼ìš´ë“œ
subprocess.Popen(["ollama","serve"])
time.sleep(3)

# ëª¨ë¸ êµ¬ì„±: ë¹ ë¥¸ ëª¨ë¸ / ë¬´ê±°ìš´ ëª¨ë¸ / VLM
FAST_MODEL  = "qwen2.5:7b-instruct"   # ë¹ ë¥¸ ìƒì„±/ë„êµ¬ íƒ­
HEAVY_MODEL = "deepseek-r1"           # RAG ë“± ê¸´ ì¶”ë¡  í•„ìš” ì‹œ
VLM_MODEL   = "qwen2.5-vl"            # ë¹„ì „-í…ìŠ¤íŠ¸
EMB_MODEL   = "nomic-embed-text"      # ì„ë² ë”©

print("ğŸ“¥ í•„ìš”í•œ ëª¨ë¸ pull")
for m in [FAST_MODEL, HEAVY_MODEL, VLM_MODEL, EMB_MODEL]:
    sh(f"ollama pull {m}")

# ê°„ë‹¨ ì›œì—…(ì²« í† í° ì§€ì—° ê°ì†Œ)
print("ğŸ”¥ ëª¨ë¸ ì›œì—…")
sh(f'''python - <<'PY'
import ollama
for m in ["{FAST_MODEL}", "{HEAVY_MODEL}"]:
    try:
        ollama.chat(model=m, messages=[{{"role":"user","content":"hi"}}], options={{"num_predict":8}})
    except Exception as e:
        print("warmup err:", m, e)
PY''')
print("âœ… í™˜ê²½ ì¤€ë¹„ ì™„ë£Œ")

# =============================
# ğŸ§  ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ ì½”ë“œ
# =============================
from typing import Optional, List, Dict
import gradio as gr
import ollama
import fitz  # PyMuPDF
import pandas as pd
import numpy as np
from PIL import Image, ImageDraw

from langchain_community.chat_models import ChatOllama
from langchain_community.embeddings import OllamaEmbeddings
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage

from langchain_community.document_loaders import PyPDFLoader, WebBaseLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.tools import DuckDuckGoSearchResults

# ---------- ì „ì—­ ----------
class GlobalState:
    def __init__(self):
        self.cache = {}  # (model, temp, opts_key) -> ChatOllama
        self.rag_vectorstore: Optional[Chroma] = None
        self.rag_embeddings: Optional[OllamaEmbeddings] = None
        self.last_rag_sources: List[Dict] = []

    def get_llm(self, model_name=FAST_MODEL, temperature=0.7, **model_kwargs) -> ChatOllama:
        # ì£¼ìš” ì†ë„ì˜µì…˜: num_predict(ì‘ë‹µê¸¸ì´), num_ctx
        if "num_predict" not in model_kwargs:
            model_kwargs["num_predict"] = 256
        opts_key = tuple(sorted(model_kwargs.items()))
        key = (model_name, float(temperature), opts_key)
        if key not in self.cache:
            self.cache[key] = ChatOllama(
                model=model_name,
                temperature=temperature,
                model_kwargs=model_kwargs
            )
        return self.cache[key]

STATE = GlobalState()
DEFAULT_PERSIST_DIR = "./chroma_db_store"
os.makedirs(DEFAULT_PERSIST_DIR, exist_ok=True)

# ---------- í—¬í¼ ----------
def log(message: str) -> str:
    ts = datetime.now().strftime('%H:%M:%S')
    return f"[{ts}] {message}\n"

def strip_think_tags(text: str) -> str:
    return re.sub(r"<think>.*?</think>", "", text, flags=re.DOTALL|re.IGNORECASE).strip()

def safe_json_extract(text: str):
    """ë¹ ë¥´ê³  ì•ˆì „í•œ JSON ì¶”ì¶œ"""
    if not isinstance(text, str):
        return None
    m = re.search(r"```json\s*(\{[\s\S]*?\}|\[[\s\S]*?\])\s*```", text, flags=re.I)
    if m:
        try: return json.loads(m.group(1))
        except: pass
    text_wo = re.sub(r"```(?!json)[\s\S]*?```", "", text, flags=re.I)
    cands = []
    for p in (r"\{[\s\S]*?\}", r"\[[\s\S]*?\]"):
        for mm in re.finditer(p, text_wo):
            cands.append(mm.group(0))
    for chunk in sorted(cands, key=len, reverse=True):
        try: return json.loads(chunk)
        except: pass
    try: return json.loads(text_wo.strip())
    except: return None

def call_vlm(image_paths: list, prompt: str, vlm_model: str) -> str:
    try:
        messages = [{"role": "user", "content": prompt, "images": image_paths}]
        res = ollama.chat(model=vlm_model, messages=messages, options={"num_predict":256})
        return res.get("message", {}).get("content", "VLM í˜¸ì¶œ ì‹¤íŒ¨")
    except Exception as e:
        return f"VLM ì˜¤ë¥˜: {e}"

# =============================
# 1) ğŸ’¬ ì±—ë´‡
# =============================
PERSONAS = {
    "ê¸°ë³¸ ìƒë‹´ì‚¬": "ë„ˆëŠ” ì‚¬ìš©ìë¥¼ ë„ì™€ì£¼ëŠ” ì¹œì ˆí•˜ê³  ìœ ëŠ¥í•œ ìƒë‹´ì‚¬ì•¼.",
    "ë°±ì„¤ê³µì£¼ ë§ˆë²•ê±°ìš¸": "ë„ˆëŠ” ë°±ì„¤ê³µì£¼ ì´ì•¼ê¸° ì† ë§ˆë²• ê±°ìš¸ì´ì•¼. í’ˆìœ„ ìˆê³  ìš´ìœ¨ê° ìˆê²Œ ë‹µí•´ì¤˜.",
    "ìœ ì¹˜ì›ìƒ": "ë„ˆëŠ” 5ì‚´ ìœ ì¹˜ì›ìƒì´ì•¼. ì§§ê³  ì‰¬ìš´ ë§ë¡œ ê·€ì—½ê²Œ ëŒ€ë‹µí•´.",
}

def handle_chatbot_stream(message: str, history: list, persona: str, temperature: float):
    if not message.strip(): return
    llm = STATE.get_llm(model_name=FAST_MODEL, temperature=temperature, num_predict=256)
    system_prompt = PERSONAS.get(persona, PERSONAS["ê¸°ë³¸ ìƒë‹´ì‚¬"])
    msgs = [SystemMessage(content=system_prompt)]
    for t in history:
        msgs.append(HumanMessage(content=t["content"]) if t["role"]=="user" else AIMessage(content=t["content"]))
    msgs.append(HumanMessage(content=message))
    stream = llm.stream(msgs)
    history.append({"role":"user","content":message})
    history.append({"role":"assistant","content":""})
    for ch in stream:
        history[-1]["content"] += ch.content
        yield history

# =============================
# 2) ğŸ“„ PDF & ë¬¸ì„œ
# =============================
def handle_pdf_summary(pdf_file, chunk_size: int, temperature: float, progress=gr.Progress()):
    if not pdf_file: return "PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.", ""
    llm = STATE.get_llm(model_name=FAST_MODEL, temperature=temperature, num_predict=256)
    progress(0, desc="PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...")
    doc = fitz.open(pdf_file.name)
    full_text = "\n".join([p.get_text() for p in doc]); doc.close()
    if not full_text.strip(): return "PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", full_text
    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=int(chunk_size*0.1))
    chunks = splitter.split_text(full_text)
    system_prompt = "ë‹¤ìŒ í…ìŠ¤íŠ¸ëŠ” ë¬¸ì„œì˜ ì¼ë¶€ì…ë‹ˆë‹¤. í•µì‹¬ì„ ì •í™•Â·ê°„ê²°íˆ ìš”ì•½í•˜ì„¸ìš”."
    summaries = []
    for c in progress.tqdm(chunks, desc="ê° ì²­í¬ ìš”ì•½ ì¤‘..."):
        s = llm.invoke([SystemMessage(content=system_prompt), HumanMessage(content=c)]).content
        summaries.append(s)
    if len(summaries)==1:
        final = summaries[0]
    else:
        progress(0.85, desc="ë¶€ë¶„ ìš”ì•½ í†µí•© ì¤‘...")
        final = llm.invoke([
            SystemMessage(content="ë¶€ë¶„ ìš”ì•½ë“¤ì„ ì¢…í•©í•´ ì „ì²´ ìš”ì•½ 10~15ì¤„."),
            HumanMessage(content="\n\n---\n\n".join(summaries))
        ]).content
    return final, full_text

def handle_pdf_extraction(pdf_file, header_px: int, footer_px: int):
    if not pdf_file: return "PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.", [], None
    out = Path("outputs/pdf_extraction"); out.mkdir(parents=True, exist_ok=True)
    doc = fitz.open(pdf_file.name)
    full_text, images = "", []
    for i, page in enumerate(doc):
        rect = page.rect
        text = page.get_text("text", clip=(0, header_px, rect.width, rect.height-footer_px))
        full_text += f"--- Page {i+1} ---\n{text}\n\n"
        for idx, img in enumerate(page.get_images(full=True)):
            xref = img[0]; base = doc.extract_image(xref)
            path = out / f"page{i+1}_img{idx+1}.{base['ext']}"
            with open(path,"wb") as f: f.write(base["image"])
            images.append(str(path))
    doc.close()
    return full_text, images, str(out)

def handle_pdf_table_extraction(pdf_file):
    if not pdf_file: return None, "PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.", None
    # ì§€ì—° ì„í¬íŠ¸
    try:
        import camelot; import tabula
    except Exception:
        return None, "í…Œì´ë¸” ì¶”ì¶œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", None
    os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-11-openjdk-amd64'
    out = Path("outputs/table_extraction"); out.mkdir(parents=True, exist_ok=True)
    pdf_path = pdf_file.name; log_text = ""
    log_text += log("Camelotìœ¼ë¡œ ì¶”ì¶œ ì‹œë„ ì¤‘...")
    try:
        tables = camelot.read_pdf(pdf_path, flavor='lattice', pages='all')
        if tables.n>0:
            df = tables[0].df; xls = out/"extracted_tables_camelot.xlsx"
            df.to_excel(xls, index=False); return df, log_text+log(f"{tables.n}ê°œ í‘œ ë°œê²¬"), str(xls)
    except Exception as e:
        log_text += log(f"Camelot ì‹¤íŒ¨: {e}")
    log_text += log("Tabulaë¡œ ì¶”ì¶œ ì‹œë„ ì¤‘...")
    try:
        frames = tabula.read_pdf(pdf_path, pages='all', multiple_tables=True)
        if frames:
            df = frames[0]; xls = out/"extracted_tables_tabula.xlsx"
            df.to_excel(xls, index=False); return df, log_text+log(f"{len(frames)}ê°œ í‘œ ë°œê²¬"), str(xls)
    except Exception as e:
        log_text += log(f"Tabula ì‹¤íŒ¨: {e}")
    return None, log_text+log("í‘œë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."), None

# =============================
# 3) ğŸ§  RAG ì±—ë´‡ (ë¬´ê±°ìš´ ëª¨ë¸ë§Œ ì‚¬ìš©)
# =============================
def handle_rag_indexing(pdf_files, chunk_size: int, chunk_overlap: int, progress=gr.Progress()):
    if not pdf_files: return "ìƒ‰ì¸í•  PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.", gr.update(interactive=False)
    log_text = ""
    try:
        progress(0, desc="ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
        STATE.rag_embeddings = OllamaEmbeddings(model=EMB_MODEL)
        log_text += log("ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.")
        all_docs = []
        for f in progress.tqdm(pdf_files, desc="PDF íŒŒì¼ ì²˜ë¦¬ ì¤‘..."):
            loader = PyPDFLoader(f.name)
            docs = loader.load()
            splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
            all_docs.extend(splitter.split_documents(docs))
        log_text += log(f"ì´ {len(pdf_files)}ê°œ PDFì—ì„œ {len(all_docs)}ê°œ ì²­í¬ ìƒì„±.")
        progress(0.85, desc="ChromaDB ì €ì¥ ì¤‘...")
        STATE.rag_vectorstore = Chroma.from_documents(
            documents=all_docs,
            embedding=STATE.rag_embeddings,
            persist_directory=DEFAULT_PERSIST_DIR
        )
        log_text += log("ë²¡í„° DB ìƒ‰ì¸ ì™„ë£Œ. ì§ˆë¬¸ ê°€ëŠ¥.")
        return log_text, gr.update(interactive=True)
    except Exception as e:
        return f"ìƒ‰ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", gr.update(interactive=False)

def handle_rag_chat_stream(message: str, history: list, temperature: float):
    if not STATE.rag_vectorstore:
        yield "ë¨¼ì € PDF íŒŒì¼ì„ ìƒ‰ì¸í•´ì£¼ì„¸ìš”."; return
    llm = STATE.get_llm(model_name=HEAVY_MODEL, temperature=temperature, num_predict=512)
    retriever = STATE.rag_vectorstore.as_retriever(search_kwargs={"k": 5})
    history.append({"role":"user","content":message})
    docs = retriever.invoke(message)
    STATE.last_rag_sources = [
        {"source": d.metadata.get('source','N/A'), "page": d.metadata.get('page','N/A'), "content": d.page_content}
        for d in docs
    ]
    system_prompt = "ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì— ê¸°ë°˜í•´ ì •í™•íˆ ë‹µí•˜ì„¸ìš”. ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë‹µí•˜ê¸°."
    prompt_template = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("system", "ì»¨í…ìŠ¤íŠ¸:\n{context}"),
        ("human", "{input}")
    ])
    chain = create_stuff_documents_chain(llm, prompt_template)
    history.append({"role":"assistant","content":""})
    resp = ""
    for ch in chain.stream({"input": message, "context": docs}):
        resp += ch; history[-1]["content"] = resp; yield history

def update_rag_sources_display():
    if not STATE.last_rag_sources: return "ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."
    md = "### ğŸ“š ë‹µë³€ì— ì°¸ì¡°ëœ ë¬¸ì„œ\n\n"
    for i, s in enumerate(STATE.last_rag_sources):
        md += f"**[ì¶œì²˜ {i+1}]**\n- **íŒŒì¼**: `{os.path.basename(s['source'])}` (í˜ì´ì§€: {s['page']})\n- **ë‚´ìš© ì¼ë¶€**: {s['content'][:200]}...\n\n"
    return md

# =============================
# 4) ğŸ–¼ï¸ ì´ë¯¸ì§€ ë¶„ì„
# =============================
def handle_single_image_analysis(image, prompt: str, vlm_model: str, use_refine: bool, temperature: float):
    if image is None: return "ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.", ""
    analysis = call_vlm([image.name], prompt, vlm_model)
    if not use_refine: return analysis, analysis
    llm = STATE.get_llm(model_name=FAST_MODEL, temperature=temperature, num_predict=200)
    refined = llm.invoke(f"ë‹¤ìŒ ë¶„ì„ì„ ê°„ê²°í•˜ê³  ì •ë³´ê°€ í’ë¶€í•˜ê²Œ ì •ë¦¬:\n{analysis}").content
    return refined, analysis

def handle_image_comparison(image1, image2, prompt: str, vlm_model: str, use_refine: bool, temperature: float):
    if image1 is None or image2 is None: return "ë‘ ì´ë¯¸ì§€ë¥¼ ëª¨ë‘ ì—…ë¡œë“œí•˜ì„¸ìš”.", ""
    comparison = call_vlm([image1.name, image2.name], prompt, vlm_model)
    if not use_refine: return comparison, comparison
    llm = STATE.get_llm(model_name=FAST_MODEL, temperature=temperature, num_predict=220)
    refined = llm.invoke(f"ë‹¤ìŒ ë¹„êµê²°ê³¼ë¥¼ í•­ëª©í™”í•´ ì •ë¦¬:\n{comparison}").content
    return refined, comparison

# =============================
# 5) ğŸ› ï¸ ê³ ê¸‰ ìƒì„±ê¸°
# =============================
def handle_mermaid_generation(idea: str, diagram_type: str, use_llm: bool):
    if not idea.strip(): return "", "", "ì•„ì´ë””ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”."
    if not use_llm:
        code, status = idea, "LLM ë¯¸ì‚¬ìš©"
    else:
        llm = STATE.get_llm(model_name=FAST_MODEL, temperature=0.1, num_predict=192)
        sys = f"Mermaid {diagram_type} íƒ€ì…ì˜ 'ìœ íš¨í•œ' ì½”ë“œë§Œ ì½”ë“œë¸”ë¡ìœ¼ë¡œ ì¶œë ¥. ì„¤ëª… ê¸ˆì§€."
        resp = llm.invoke([SystemMessage(content=sys), HumanMessage(content=idea)]).content
        m = re.search(r"```mermaid\s*([\s\S]*?)```", resp)
        if m: code, status = m.group(1).strip(), "Mermaid ì½”ë“œ ìƒì„± ì™„ë£Œ."
        else:
            code = re.sub(r"^.*?mermaid", "", resp, flags=re.S|re.I).strip()
            status = "ì½”ë“œ ë¸”ë¡ ë¯¸ê²€ì¶œ â†’ ë³¸ë¬¸ì—ì„œ ì¶”ì¶œ"
    return f"```mermaid\n{code}\n```", code, status

def handle_sd_prompt_generation(idea: str):
    if not idea.strip(): return "", "", "ì•„ì´ë””ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”."
    llm = STATE.get_llm(model_name=FAST_MODEL, temperature=0.6, num_predict=160)
    sys = "Stable Diffusion í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´. JSONìœ¼ë¡œë§Œ ì‘ë‹µí•˜ê³  keys=['prompt','negative_prompt'] í¬í•¨."
    resp = llm.invoke([SystemMessage(content=sys), HumanMessage(content=f"ì•„ì´ë””ì–´: {idea}")]).content
    data = safe_json_extract(resp)
    if isinstance(data, dict):
        return data.get("prompt", idea), data.get("negative_prompt","low quality, blurry"), "í”„ë¡¬í”„íŠ¸ ìƒì„± ì™„ë£Œ."
    return idea, "", "JSON íŒŒì‹± ì‹¤íŒ¨. ì›ë³¸ ì‘ë‹µ:\n"+resp

def handle_structured_ocr(image, use_llm: bool):
    if image is None: return None, "", None, "ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”."
    # ì§€ì—° ì„í¬íŠ¸
    try:
        import easyocr
    except Exception:
        return None, "EasyOCR ë¯¸ì„¤ì¹˜ ë˜ëŠ” ë¡œë”© ì‹¤íŒ¨.", None, ""
    reader = easyocr.Reader(['ko','en'])
    np_img = np.array(image)
    results = reader.readtext(np_img)
    overlay = image.copy(); draw = ImageDraw.Draw(overlay)
    full_text = "\n".join([r[1] for r in results])
    for (bbox, text, prob) in results:
        draw.polygon([tuple(p) for p in bbox], outline="cyan", width=2)
    if not use_llm: return overlay, full_text, None, "LLM ë¯¸ì‚¬ìš©"
    llm = STATE.get_llm(model_name=FAST_MODEL, temperature=0.1, num_predict=220)
    resp = llm.invoke([
        SystemMessage(content="OCR í…ìŠ¤íŠ¸ë¥¼ êµ¬ì¡°í™” JSONìœ¼ë¡œ ë³€í™˜. í‘œë©´ êµ¬ì¡° ì¸ì‹ ì‹œ rows/columns or items ì‚¬ìš©."),
        HumanMessage(content=full_text)
    ]).content
    data = safe_json_extract(resp); df = None
    if isinstance(data, dict):
        if 'items' in data and isinstance(data['items'], list):
            df = pd.DataFrame(data['items'])
        elif 'rows' in data and 'columns' in data:
            df = pd.DataFrame(data['rows'], columns=data['columns'])
    return overlay, full_text, df, (json.dumps(data, ensure_ascii=False, indent=2) if data else "")

# =============================
# 6) ğŸŒ ì›¹ & ë„êµ¬
# =============================
def handle_sequential_thinking(query: str, temperature: float, progress=gr.Progress()):
    if not query.strip(): return "", "", "", "ì§ˆì˜ë¥¼ ì…ë ¥í•˜ì„¸ìš”."
    llm = STATE.get_llm(model_name=FAST_MODEL, temperature=temperature, num_predict=256)
    progress(0.1, desc="[1/3] ë¶„ì„ ì¤‘...")
    analysis = llm.invoke([SystemMessage(content="í•µì‹¬ ìŸì ì„ 3~5ì¤„ë¡œ."), HumanMessage(content=query)]).content
    progress(0.5, desc="[2/3] ìµœì¢… ë‹µë³€ ìƒì„± ì¤‘...")
    answer = llm.invoke([SystemMessage(content="ê°„ê²°Â·ì •í™•Â·êµ¬ì¡°í™”ëœ ë‹µë³€ 8~12ì¤„."), HumanMessage(content=f"{query}\n\në¶„ì„:{analysis}")]).content
    progress(0.8, desc="[3/3] ê²€ì¦ ì¤‘...")
    reflection = llm.invoke([SystemMessage(content="ëª¨í˜¸ì„±/ëˆ„ë½/ê³¼ì¥ ì ê²€ í›„ 3ì¤„ ì œì•ˆ."), HumanMessage(content=f"ë‹µë³€:{answer}")]).content
    return analysis, answer, reflection, "ëª¨ë“  ë‹¨ê³„ ì™„ë£Œ."

def handle_web_rag(query: str, temperature: float, progress=gr.Progress()):
    llm = STATE.get_llm(model_name=FAST_MODEL, temperature=temperature, num_predict=256)
    progress(0, desc="ì›¹ ê²€ìƒ‰ ì¤‘...")
    search = DuckDuckGoSearchResults()
    search_results = search.run(query)
    links = re.findall(r'https?://[^\s,```<>"]+', search_results)
    if not links: return "ê´€ë ¨ ì›¹ í˜ì´ì§€ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.", search_results, ""
    links = links[:2]  # ì†ë„ ìœ„í•´ ìƒìœ„ 2ê°œë§Œ
    progress(0.35, desc=f"{len(links)}ê°œ ë§í¬ ìˆ˜ì§‘ ì¤‘...")
    try:
        # bs_kwargsëŠ” ë²„ì „ë³„ ì°¨ì´ê°€ ìˆì–´ ì œê±°(ì•ˆì •ì„± ìš°ì„ )
        loader = WebBaseLoader(web_paths=links)
        docs = loader.load()
        ctx = "\n\n---\n\n".join([d.page_content for d in docs])
    except Exception as e:
        ctx = f"ì½˜í…ì¸  ìˆ˜ì§‘ ì‹¤íŒ¨: {e}"
    progress(0.7, desc="ë‹µë³€ ìƒì„± ì¤‘...")
    resp = llm.invoke([SystemMessage(content="ì›¹ ê²°ê³¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•Â·ê°„ê²°íˆ ë‹µë³€."), HumanMessage(content=f"ê²€ìƒ‰ ê²°ê³¼:\n{ctx}\n\nì§ˆë¬¸: {query}")]).content
    return resp, search_results, "\n".join(links)

# =============================
# 7) Gradio UI
# =============================
def build_ui():
    with gr.Blocks(theme=gr.themes.Soft(), title="DeepSeek-R1 AI Toolkit") as demo:
        gr.Markdown("# ğŸš€ DeepSeek-R1 AI Toolkit (All-in-One)")
        gr.Markdown("38ê°œ ì˜ˆì œë¥¼ í†µí•©í•œ ë¡œì»¬ AI ì• í”Œë¦¬ì¼€ì´ì…˜ì…ë‹ˆë‹¤. (ì†ë„ ìµœì í™”)")

        # ì±—ë´‡
        with gr.Tab("ğŸ’¬ ì±—ë´‡"):
            with gr.Row():
                with gr.Column(scale=1):
                    persona = gr.Dropdown(list(PERSONAS.keys()), value="ê¸°ë³¸ ìƒë‹´ì‚¬", label="í˜ë¥´ì†Œë‚˜ ì„ íƒ")
                    temp = gr.Slider(0.0, 1.5, value=0.7, step=0.1, label="Temperature")
                with gr.Column(scale=3):
                    win = gr.Chatbot(height=500, label="ëŒ€í™”ì°½", type="messages")
                    with gr.Row():
                        msg = gr.Textbox(label="ë©”ì‹œì§€ ì…ë ¥", placeholder="ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”...", scale=4)
                        btn = gr.Button("ì „ì†¡", variant="primary", scale=1)
                        clr = gr.Button("ì´ˆê¸°í™”")

            def on_submit(m, h, p, t):
                for nh in handle_chatbot_stream(m, h, p, t):
                    yield nh, ""
            msg.submit(on_submit, [msg, win, persona, temp], [win, msg])
            btn.click(on_submit, [msg, win, persona, temp], [win, msg])
            clr.click(lambda: [], None, win)

        # PDF & ë¬¸ì„œ
        with gr.Tab("ğŸ“„ PDF & ë¬¸ì„œ"):
            with gr.Tabs():
                with gr.TabItem("PDF ìš”ì•½"):
                    with gr.Row():
                        f_in = gr.File(label="PDF íŒŒì¼", file_types=[".pdf"])
                        with gr.Column():
                            chunk = gr.Slider(1000, 8000, value=4000, step=500, label="ì²­í¬ í¬ê¸°")
                            t = gr.Slider(0.0, 1.0, value=0.2, step=0.1, label="Temperature")
                            run = gr.Button("ìš”ì•½ ì‹¤í–‰", variant="primary")
                    out = gr.Textbox(label="ìš”ì•½ ê²°ê³¼", lines=15)
                    with gr.Accordion("ì „ì²´ í…ìŠ¤íŠ¸", open=False):
                        raw = gr.Textbox(label="ì›ë³¸ í…ìŠ¤íŠ¸", lines=10)
                    run.click(handle_pdf_summary, [f_in, chunk, t], [out, raw])

                with gr.TabItem("í…ìŠ¤íŠ¸/ì´ë¯¸ì§€ ì¶”ì¶œ"):
                    with gr.Row():
                        f_in2 = gr.File(label="PDF íŒŒì¼", file_types=[".pdf"])
                        with gr.Column():
                            head = gr.Slider(0, 200, value=60, label="í—¤ë” ë†’ì´ (px)")
                            foot = gr.Slider(0, 200, value=60, label="í‘¸í„° ë†’ì´ (px)")
                            run2 = gr.Button("ì¶”ì¶œ ì‹¤í–‰", variant="primary")
                    with gr.Row():
                        tx = gr.Textbox(label="ì¶”ì¶œëœ í…ìŠ¤íŠ¸", lines=15, scale=2)
                        with gr.Column(scale=1):
                            gal = gr.Gallery(label="ì¶”ì¶œëœ ì´ë¯¸ì§€", height=400)
                            pth = gr.Textbox(label="ì €ì¥ ê²½ë¡œ")
                    run2.click(handle_pdf_extraction, [f_in2, head, foot], [tx, gal, pth])

                with gr.TabItem("í‘œ ì¶”ì¶œ"):
                    f_tbl = gr.File(label="PDF íŒŒì¼", file_types=[".pdf"])
                    run3 = gr.Button("í‘œ ì¶”ì¶œ ì‹¤í–‰", variant="primary")
                    df = gr.Dataframe(label="ì¶”ì¶œëœ í‘œ")
                    logbx = gr.Textbox(label="ì²˜ë¦¬ ë¡œê·¸", lines=5)
                    xfile = gr.File(label="ì—‘ì…€ ë‹¤ìš´ë¡œë“œ")
                    run3.click(handle_pdf_table_extraction, [f_tbl], [df, logbx, xfile])

        # RAG
        with gr.Tab("ğŸ§  RAG ì±—ë´‡"):
            with gr.Row():
                with gr.Column(scale=1):
                    gr.Markdown("##### 1. ë¬¸ì„œ ìƒ‰ì¸")
                    f_multi = gr.File(label="PDF íŒŒì¼ë“¤", file_count="multiple", file_types=[".pdf"])
                    cs = gr.Slider(200, 2000, value=1000, label="ì²­í¬ í¬ê¸°")
                    co = gr.Slider(0, 500, value=100, label="ì²­í¬ ì¤‘ì²©")
                    idx = gr.Button("ìƒ‰ì¸ ì‹œì‘", variant="primary")
                    stat = gr.Textbox(label="ìƒ‰ì¸ ìƒíƒœ", lines=10)
                with gr.Column(scale=2):
                    gr.Markdown("##### 2. ë¬¸ì„œ ê¸°ë°˜ ëŒ€í™”")
                    rag_chat = gr.Chatbot(height=450, label="ëŒ€í™”ì°½", type="messages")
                    with gr.Accordion("ì°¸ì¡° ë¬¸ì„œ", open=False):
                        rag_src = gr.Markdown()
                    rtemp = gr.Slider(0.0, 1.0, value=0.3, label="Temperature")
                    rmsg = gr.Textbox(label="ì§ˆë¬¸", placeholder="ë¬¸ì„œì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”...", interactive=False)
                    rsend = gr.Button("ì „ì†¡", variant="primary")

            idx.click(handle_rag_indexing, [f_multi, cs, co], [stat, rmsg])

            def on_rag_submit(m, h, t):
                for nh in handle_rag_chat_stream(m, h, t):
                    yield nh, ""
            rmsg.submit(on_rag_submit, [rmsg, rag_chat, rtemp], [rag_chat, rmsg]).then(update_rag_sources_display, None, rag_src)
            rsend.click(on_rag_submit, [rmsg, rag_chat, rtemp], [rag_chat, rmsg]).then(update_rag_sources_display, None, rag_src)

        # ì´ë¯¸ì§€ ë¶„ì„
        with gr.Tab("ğŸ–¼ï¸ ì´ë¯¸ì§€ ë¶„ì„"):
            vlm_model = gr.Dropdown([VLM_MODEL, "llava:latest"], value=VLM_MODEL, label="VLM ëª¨ë¸")
            with gr.Tabs():
                with gr.TabItem("ë‹¨ì¼ ì´ë¯¸ì§€"):
                    with gr.Row():
                        img_single = gr.Image(type="pil", label="ì´ë¯¸ì§€")
                        with gr.Column():
                            iprompt = gr.Textbox(label="í”„ë¡¬í”„íŠ¸", value="ì´ ì´ë¯¸ì§€ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”.", lines=3)
                            irefine = gr.Checkbox(label="LLMìœ¼ë¡œ ì •ì œ", value=True)
                            itemp = gr.Slider(0.0, 1.0, value=0.7, label="Temperature")
                            ibtn = gr.Button("ë¶„ì„", variant="primary")
                    iout = gr.Textbox(label="ë¶„ì„ ê²°ê³¼", lines=10)
                    with gr.Accordion("VLM ì›ë³¸", open=False):
                        iraw = gr.Textbox(label="ì›ë³¸ ê²°ê³¼")
                    ibtn.click(handle_single_image_analysis, [img_single, iprompt, vlm_model, irefine, itemp], [iout, iraw])

                with gr.TabItem("ì´ë¯¸ì§€ ë¹„êµ"):
                    with gr.Row():
                        img1 = gr.Image(type="pil", label="ì´ë¯¸ì§€ 1")
                        img2 = gr.Image(type="pil", label="ì´ë¯¸ì§€ 2")
                    cprompt = gr.Textbox(label="í”„ë¡¬í”„íŠ¸", value="ë‘ ì´ë¯¸ì§€ë¥¼ ë¹„êµí•´ì£¼ì„¸ìš”.", lines=3)
                    crefine = gr.Checkbox(label="LLMìœ¼ë¡œ ì •ì œ", value=True)
                    ctemp = gr.Slider(0.0, 1.0, value=0.7, label="Temperature")
                    cbtn = gr.Button("ë¹„êµ", variant="primary")
                    cout = gr.Textbox(label="ë¹„êµ ê²°ê³¼", lines=10)
                    with gr.Accordion("VLM ì›ë³¸", open=False):
                        craw = gr.Textbox(label="ì›ë³¸ ê²°ê³¼")
                    cbtn.click(handle_image_comparison, [img1, img2, cprompt, vlm_model, crefine, ctemp], [cout, craw])

        # ê³ ê¸‰ ìƒì„±ê¸°
        with gr.Tab("ğŸ› ï¸ ê³ ê¸‰ ìƒì„±ê¸°"):
            with gr.Tabs():
                with gr.TabItem("Mermaid ë‹¤ì´ì–´ê·¸ë¨"):
                    with gr.Row():
                        with gr.Column():
                            m_idea = gr.Textbox(label="ì•„ì´ë””ì–´", lines=5)
                            m_type = gr.Dropdown(["flowchart","sequenceDiagram","gantt","pie","classDiagram"], value="flowchart", label="ë‹¤ì´ì–´ê·¸ë¨ íƒ€ì…")
                            m_llm = gr.Checkbox(value=True, label="LLM ì‚¬ìš©")
                            m_btn = gr.Button("ìƒì„±", variant="primary")
                        with gr.Column():
                            m_out = gr.Markdown(label="ë Œë”ë§")
                            m_code = gr.Code(label="ì½”ë“œ", language="markdown")
                            m_status = gr.Textbox(label="ìƒíƒœ")
                    m_btn.click(handle_mermaid_generation, [m_idea, m_type, m_llm], [m_out, m_code, m_status])

                with gr.TabItem("SD í”„ë¡¬í”„íŠ¸"):
                    sd_idea = gr.Textbox(label="ì•„ì´ë””ì–´", lines=3)
                    sd_btn = gr.Button("ìƒì„±", variant="primary")
                    sd_prompt = gr.Textbox(label="í”„ë¡¬í”„íŠ¸", lines=4)
                    sd_negative = gr.Textbox(label="ë„¤ê±°í‹°ë¸Œ í”„ë¡¬í”„íŠ¸", lines=2)
                    sd_status = gr.Textbox(label="ìƒíƒœ")
                    sd_btn.click(handle_sd_prompt_generation, [sd_idea], [sd_prompt, sd_negative, sd_status])

                with gr.TabItem("êµ¬ì¡°í™” OCR"):
                    ocr_img = gr.Image(type="pil", label="ì´ë¯¸ì§€")
                    ocr_use_llm = gr.Checkbox(value=True, label="LLMìœ¼ë¡œ êµ¬ì¡°í™”")
                    ocr_btn = gr.Button("OCR ì‹¤í–‰", variant="primary")
                    with gr.Row():
                        ocr_overlay = gr.Image(label="ì¸ì‹ ì˜ì—­")
                        ocr_df = gr.Dataframe(label="êµ¬ì¡°í™” ë°ì´í„°")
                    with gr.Accordion("í…ìŠ¤íŠ¸ ë° JSON", open=False):
                        ocr_text = gr.Textbox(label="í…ìŠ¤íŠ¸")
                        ocr_json = gr.Code(label="JSON", language="json")
                    ocr_btn.click(handle_structured_ocr, [ocr_img, ocr_use_llm], [ocr_overlay, ocr_text, ocr_df, ocr_json])

        # ì›¹ & ë„êµ¬
        with gr.Tab("ğŸŒ ì›¹ & ë„êµ¬"):
            wq = gr.Textbox(label="ì›¹ ê²€ìƒ‰ ì§ˆë¬¸", lines=2)
            wt = gr.Slider(0.0, 1.0, value=0.7, label="Temperature")
            wb = gr.Button("ê²€ìƒ‰ ë° ë‹µë³€", variant="primary")
            wa = gr.Textbox(label="ë‹µë³€", lines=10)
            with gr.Accordion("ê²€ìƒ‰ ì •ë³´", open=False):
                wr = gr.Textbox(label="ê²€ìƒ‰ ê²°ê³¼")
                wl = gr.Textbox(label="ì°¸ì¡° URL")
            wb.click(handle_web_rag, [wq, wt], [wa, wr, wl])

        # ìˆœì°¨ì  ì‚¬ê³ 
        with gr.Tab("ğŸ¤” ìˆœì°¨ì  ì‚¬ê³ "):
            sq = gr.Textbox(label="ì§ˆë¬¸", lines=3)
            st = gr.Slider(0.0, 1.0, value=0.7, label="Temperature")
            sb = gr.Button("ì‚¬ê³  ì‹œì‘", variant="primary")
            s_status = gr.Textbox(label="ìƒíƒœ")
            with gr.Row():
                s_a = gr.Markdown(label="ë¶„ì„")
                s_ans = gr.Markdown(label="ë‹µë³€")
                s_ref = gr.Markdown(label="ê²€ì¦")
            sb.click(handle_sequential_thinking, [sq, st], [s_a, s_ans, s_ref, s_status])

    return demo

# =============================
# ë©”ì¸ ì‹¤í–‰
# =============================
if __name__ == "__main__":
    print("ğŸš€ Gradio ì•± ì‹¤í–‰ ì¤‘...")
    app = build_ui()
    # â–¼ Gradio 4.x: queue ì¸ì ì¶•ì†Œ(í˜¸í™˜)
    app = app.queue(max_size=32)
    # share=False ê°€ í„°ë„ë§ ì§€ì—°ì„ ì¤„ì—¬ í›¨ì”¬ ë¹ ë¦„
    app.launch(server_name="0.0.0.0", server_port=7860, inbrowser=True, share=False, show_error=True)
