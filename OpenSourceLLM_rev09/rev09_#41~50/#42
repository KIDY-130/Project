#42
"""
ë¡œì»¬ LLM(Ollama + DeepSeek-R1) ê¸°ë°˜ ë†ê¸°êµ¬ ê³ ì¥ ì§„ë‹¨ ì–´ì‹œìŠ¤í„´íŠ¸ (Gradio UI)
- OpenAI Chat Completions ì½”ë“œë¥¼ ì˜¤í”ˆì†ŒìŠ¤ LLM í™˜ê²½ìœ¼ë¡œ ë³€í™˜
- Gradio UI ì œê³µ, JSON ì¶œë ¥ ê°•ì œ, íŒŒì‹± ì•ˆì •í™”, ì¬ì‹œë„(ìµœëŒ€ 3íšŒ), ì§€ì› ê¸°ë¡ ëˆ„ì 

ì‚¬ì „ ì¤€ë¹„:
  1) Ollama ì„¤ì¹˜ ë° ì‹¤í–‰ (ê¸°ë³¸: http://localhost:11434)
  2) ëª¨ë¸ ì„¤ì¹˜:  `ollama pull deepseek-r1`
ì˜ì¡´ì„±:
  pip install gradio requests

ì‹¤í–‰:
  python app.py
"""

import re
import json
import time
import requests
import gradio as gr
from typing import Dict, List, Optional, Tuple

# -----------------------------
# Ollama ì„¤ì •
# -----------------------------
OLLAMA_CHAT_URL = "http://localhost:11434/api/chat"
DEFAULT_MODEL = "deepseek-r1"

# -----------------------------
# ì‹œìŠ¤í…œ ë©”ì„¸ì§€ í…œí”Œë¦¿ (ì›ë³¸ ìœ ì§€)
# -----------------------------
SYSTEM_MESSAGE_TEMPLATE = """

ë‹¤ìŒì˜ ì „ì œ ì§€ì‹ê³¼ ì§€ì› ê¸°ë¡ì„ ë°”íƒ•ìœ¼ë¡œ ì§€ì›ì„ í•´ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤.
ëª¨ë“  ëŒ€í™”ì— ëŒ€í•œ ë‹µë³€ê³¼ ê·¸ ìš”ì•½ì„ ë‹¤ìŒê³¼ ê°™ì€ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•´ì£¼ì„¸ìš”.

## ì¶œë ¥ (JSON í˜•ì‹)
{{"content" : "(ë‹µë³€ ë‚´ìš©)", "summary" : "(ë‹µë³€ ìš”ì•½ ë‚´ìš©)" }}

## ëŒ€í™”ì˜ ì˜ˆ
USER->ê¸°ê³„ì˜ ìƒíƒœê°€ ë‚˜ì©ë‹ˆë‹¤
AI->{{"content" : "ì–´ë–¤ ê¸°ê³„ì—ì„œ ì–´ë–¤ ë¬¸ì œê°€ ë°œìƒí•˜ê³  ìˆìŠµë‹ˆê¹Œ?", "summary" : "ê¸°ê³„ì˜ ì¢…ë¥˜ì™€ ë¬¸ì œ í˜„ìƒì„ ì‚¬ìš©ìì—ê²Œ í™•ì¸."}}

USER->ê°ì‚¬í•©ë‹ˆë‹¤. (ì¢…ë£Œì˜ ì•”ì‹œ, ì¸ì‚¬, ê°ì‚¬ì˜ ë§ ë“±ë„ í¬í•¨)
AI->{{"content" : "ì²œë§Œì—ìš”. ì–¸ì œë“ ì§€ ìƒë‹´ì„ ìš”ì²­í•´ ì£¼ì„¸ìš”.","summary" : "ì§€ì› ì¢…ë£Œ."}}

## ì „ì œ ì§€ì‹
  - ë‹¹ì‹ ì€ ë†ê¸°êµ¬ ê³ ì¥ ì§„ë‹¨ì˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

## ì§€ì› ê¸°ë¡
  {support_context}

"""

# -----------------------------
# JSON íŒŒì‹± ìœ í‹¸ (ì•ˆì •ì„± ê°•í™”)
# -----------------------------
def extract_json(text: str) -> Dict:
    """
    ëª¨ë¸ ì¶œë ¥ì—ì„œ JSON ê°ì²´ í•œ ë©ì–´ë¦¬ë§Œ ì•ˆì •ì ìœ¼ë¡œ ì¶”ì¶œí•˜ì—¬ dictë¡œ ë°˜í™˜.
    1) ì „ì²´ë¥¼ ê·¸ëŒ€ë¡œ íŒŒì‹± ì‹œë„
    2) ì½”ë“œë¸”ë¡ ```json ... ``` ë˜ëŠ” ``` ... ``` ê°ì‹¼ êµ¬ê°„ íŒŒì‹± ì‹œë„
    3) ê· í˜• ì¡íŒ ì¤‘ê´„í˜¸ë¥¼ ì´ìš©í•´ ì²« JSON ì˜¤ë¸Œì íŠ¸ ì¶”ì¶œ
    ì‹¤íŒ¨ ì‹œ JSONDecodeErrorë¥¼ ë‹¤ì‹œ ë˜ì§
    """
    text = text.strip()

    # 1) ë°”ë¡œ íŒŒì‹± ì‹œë„
    try:
        return json.loads(text)
    except json.JSONDecodeError:
        pass

    # 2) ì½”ë“œë¸”ë¡ ë‚´ë¶€ íŒŒì‹± ì‹œë„
    fence_patterns = [
        r"```json\s*(\{.*?\})\s*```",
        r"```\s*(\{.*?\})\s*```",
    ]
    for pat in fence_patterns:
        m = re.search(pat, text, flags=re.DOTALL)
        if m:
            candidate = m.group(1)
            try:
                return json.loads(candidate)
            except json.JSONDecodeError:
                pass

    # 3) ì¤‘ê´„í˜¸ ê· í˜•ìœ¼ë¡œ ì²« ê°ì²´ ë½‘ê¸°
    start = text.find("{")
    while start != -1:
        balance = 0
        for i in range(start, len(text)):
            if text[i] == "{":
                balance += 1
            elif text[i] == "}":
                balance -= 1
                if balance == 0:
                    candidate = text[start : i + 1]
                    try:
                        return json.loads(candidate)
                    except json.JSONDecodeError:
                        break
        start = text.find("{", start + 1)

    # ëª¨ë‘ ì‹¤íŒ¨
    raise json.JSONDecodeError("Valid JSON object not found", text, 0)

# -----------------------------
# Ollama í˜¸ì¶œ
# -----------------------------
def ollama_chat(
    user_text: str,
    support_context: str,
    model: str = DEFAULT_MODEL,
    temperature: float = 0.2,
    top_p: float = 0.95,
    num_predict: Optional[int] = None,
    retry: int = 3,
    debug: bool = False,
) -> Tuple[Optional[Dict], str]:
    """
    Ollama /api/chat ë¡œ JSON ì‘ë‹µì„ ìœ ë„í•˜ê³  íŒŒì‹±í•˜ì—¬ dict ë°˜í™˜.
    - ì‹¤íŒ¨ ì‹œ ìµœëŒ€ retryíšŒ ì¬ì‹œë„
    - ë°˜í™˜: (parsed_json_or_none, raw_text)
    """

    # ì‹œìŠ¤í…œ ë©”ì‹œì§€ êµ¬ì„±
    system_msg = SYSTEM_MESSAGE_TEMPLATE.format(support_context=support_context)

    # ì‚¬ìš©ì ë©”ì‹œì§€ì— JSON í˜•ì‹ ì¤€ìˆ˜ ê°•ì œ
    user_prompt = (
        f"JSON í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•´ì£¼ì„¸ìš”: [{user_text}] + "
        f"ì ˆëŒ€ ë‹¤ë¥¸ í…ìŠ¤íŠ¸ë¥¼ ë§ë¶™ì´ì§€ ë§ê³ , "
        f'ë°˜ë“œì‹œ ì •í™•íˆ keys "content"ì™€ "summary"ë§Œ í¬í•¨í•œ JSON í•œ ê°œì²´ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.'
    )

    messages = [
        {"role": "system", "content": system_msg},
        {"role": "user", "content": user_prompt},
    ]

    options = {"temperature": temperature, "top_p": top_p}
    if num_predict and num_predict > 0:
        options["num_predict"] = int(num_predict)

    payload = {
        "model": model,
        "messages": messages,
        "stream": False,  # ì—¬ê¸°ì„œëŠ” ì¼ê´„ ì‘ë‹µ í›„ JSON íŒŒì‹±
        "options": options,
    }

    last_raw = ""
    for attempt in range(1, retry + 1):
        try:
            resp = requests.post(OLLAMA_CHAT_URL, json=payload, timeout=120)
            resp.raise_for_status()
            data = resp.json()
            raw = data.get("message", {}).get("content", "") or ""
            last_raw = raw

            try:
                parsed = extract_json(raw)
                return parsed, raw
            except json.JSONDecodeError:
                if debug:
                    print(f"[JSON íŒŒì‹± ì‹¤íŒ¨ - {attempt}/{retry}] ì›ë¬¸:\n{raw}\n")
                # ë‹¤ìŒ ì‹œë„: ëª¨ë¸ì— ë” ê°•í•˜ê²Œ ì§€ì‹œí•˜ê¸° ìœ„í•´ user ë©”ì‹œì§€ë¥¼ ë³´ê°•
                payload["messages"][-1]["content"] += (
                    " ì‘ë‹µ ì‹œ JSON ì´ì™¸ì˜ í…ìŠ¤íŠ¸(ì„¤ëª…/ì¸ì‚¬/ì½”ë“œë¸”ë¡)ëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”."
                )
                time.sleep(0.8)
                continue

        except requests.RequestException as e:
            last_raw = f"[ì—°ê²° ì˜¤ë¥˜] Ollama ì„œë²„ ìš”ì²­ ì‹¤íŒ¨: {e}"
            if debug:
                print(last_raw)
            time.sleep(0.8)
            continue

    return None, last_raw

# -----------------------------
# Gradio í•¸ë“¤ëŸ¬
# -----------------------------
def gr_handle(
    user_text: str,
    support_context_state: str,
    model: str,
    temperature: float,
    top_p: float,
    num_predict: Optional[int],
    debug: bool,
    chat_log: List[List[str]],
):
    """
    Gradio ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬
    - ì…ë ¥: ì‚¬ìš©ì ì§ˆë¬¸ ë° í˜„ì¬ support_context
    - ì¶œë ¥: (ë‹µë³€ content, ìš”ì•½ summary, ì—…ë°ì´íŠ¸ëœ support_context, ì±„íŒ… ë¡œê·¸)
    """
    chat_log = chat_log or []
    chat_log.append([f"USER: {user_text}", "ì‘ë‹µ ìƒì„± ì¤‘..."])

    parsed, raw = ollama_chat(
        user_text=user_text,
        support_context=support_context_state,
        model=model or DEFAULT_MODEL,
        temperature=temperature,
        top_p=top_p,
        num_predict=num_predict,
        retry=3,
        debug=debug,
    )

    if parsed is None:
        # íŒŒì‹± ì‹¤íŒ¨ ì‹œ rawë¥¼ ê·¸ëŒ€ë¡œ ë…¸ì¶œí•˜ë˜, ìš”ì•½ì€ ë™ì¼í•˜ê²Œ ì„¤ì •
        content = raw or "ì£„ì†¡í•©ë‹ˆë‹¤. ìš”ì²­ì„ ì²˜ë¦¬í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
        summary = content
        # ì§€ì› ê¸°ë¡ì—ëŠ” ê°€ëŠ¥í•œ í•œ ìš”ì•½ í˜•íƒœë¡œ ì¶•ì•½
        support_context_state += f"- user: {user_text}\n- AI: {summary}\n\n"
        chat_log[-1][1] = f"(íŒŒì‹± ì‹¤íŒ¨)\n{content}"
        return content, summary, support_context_state, chat_log

    # ì •ìƒ íŒŒì‹±
    content = parsed.get("content", "").strip()
    summary = parsed.get("summary", "").strip()

    # ì§€ì› ê¸°ë¡ ê°±ì‹ (ì›ë³¸ ì½”ë“œì™€ ë™ì¼í•œ í˜•ì‹)
    support_context_state += f"- user: {user_text}\n"
    support_context_state += f"- AI: {summary}\n\n"

    chat_log[-1][1] = f"CONTENT:\n{content}\n\nSUMMARY:\n{summary}"
    return content, summary, support_context_state, chat_log

def clear_all():
    return "", "", "", []

# -----------------------------
# Gradio UI
# -----------------------------
with gr.Blocks(title="ë†ê¸°êµ¬ ì§„ë‹¨ ì–´ì‹œìŠ¤í„´íŠ¸ (DeepSeek-R1 / Ollama)") as demo:
    gr.Markdown("## ğŸ§‘â€ğŸŒ¾ ë†ê¸°êµ¬ ê³ ì¥ ì§„ë‹¨ ì–´ì‹œìŠ¤í„´íŠ¸")
    gr.Markdown(
        "- ë¡œì»¬ **Ollama + DeepSeek-R1** ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤ (API í‚¤ ë¶ˆí•„ìš”)\n"
        "- ì‘ë‹µì€ **JSON({content, summary})** í˜•ì‹ìœ¼ë¡œ ê°•ì œë˜ì–´ íŒŒì‹± í›„ í‘œì‹œë©ë‹ˆë‹¤\n"
        "- ë‚´ë¶€ì ìœ¼ë¡œ **ì§€ì› ê¸°ë¡(support_context)**ì„ ëˆ„ì í•˜ì—¬ ë¬¸ë§¥ì„ ìœ ì§€í•©ë‹ˆë‹¤"
    )

    with gr.Row():
        with gr.Column(scale=1):
            with gr.Accordion("âš™ï¸ ì„¤ì •", open=True):
                model = gr.Textbox(label="ëª¨ë¸ëª…", value=DEFAULT_MODEL)
                temperature = gr.Slider(0.0, 1.5, value=0.2, step=0.05, label="temperature")
                top_p = gr.Slider(0.1, 1.0, value=0.95, step=0.05, label="top_p")
                num_predict = gr.Number(
                    label="num_predict (ìµœëŒ€ ìƒì„± í† í° ìˆ˜, ë¹„ìš°ë©´ ë¬´ì œí•œ)",
                    value=None,
                    precision=0
                )
                debug = gr.Checkbox(label="ë””ë²„ê·¸ ì¶œë ¥(ì½˜ì†”)", value=False)
                clear_btn = gr.Button("ğŸ§¹ ëª¨ë‘ ì´ˆê¸°í™”", variant="secondary")

        with gr.Column(scale=2):
            user_text = gr.Textbox(
                label="ì§ˆë¬¸ ì…ë ¥",
                placeholder="ì˜ˆ) íŠ¸ë™í„° ì‹œë™ì´ ê±¸ë¦¬ì§€ ì•ŠìŠµë‹ˆë‹¤. ë°°í„°ë¦¬ëŠ” ìƒˆê²ƒì…ë‹ˆë‹¤.",
                lines=3,
            )
            ask_btn = gr.Button("ì§ˆë¬¸ ì „ì†¡ ğŸš€", variant="primary")

            with gr.Row():
                content_out = gr.Textbox(label="ë‹µë³€(content)", lines=8)
                summary_out = gr.Textbox(label="ìš”ì•½(summary)", lines=8)

            chat_log = gr.Chatbot(label="ëŒ€í™” ë¡œê·¸ (ì›ë¬¸/ìš”ì•½ ë™ì‹œ í‘œì‹œ)", height=400)

    # ìƒíƒœ: ì§€ì› ê¸°ë¡ ë¬¸ìì—´
    support_context_state = gr.State("")

    ask_btn.click(
        fn=gr_handle,
        inputs=[user_text, support_context_state, model, temperature, top_p, num_predict, debug, chat_log],
        outputs=[content_out, summary_out, support_context_state, chat_log],
        queue=True,
    )
    user_text.submit(
        fn=gr_handle,
        inputs=[user_text, support_context_state, model, temperature, top_p, num_predict, debug, chat_log],
        outputs=[content_out, summary_out, support_context_state, chat_log],
        queue=True,
    )
    clear_btn.click(fn=clear_all, outputs=[content_out, summary_out, support_context_state, chat_log])

if __name__ == "__main__":
    demo.launch()
