#62
"""
RAG (FAISS + Ollama 임베딩 + DeepSeek-R1 LLM) 예제
- OpenAI API 의존성 제거, 로컬 Ollama 환경만 사용
- restaurants.txt → FAISS 인덱스 생성 & 로드
- LangChain Runnable 그래프(프롬프트 → LLM → 파서)
- CLI 루프 + Gradio UI 제공

준비물
1) Ollama 설치 후 아래 모델 pull
   - 임베딩:   `ollama pull nomic-embed-text`
   - LLM(생성): `ollama pull deepseek-r1:latest`
2) 같은 폴더에 restaurants.txt 파일 배치

메모
- 일부 DeepSeek-R1은 사고흐름(<think>...</think>)을 출력할 수 있어 UI에서 옵션으로 제거합니다.
- Windows 경로 문제시 '\\' 사용 또는 pathlib로 대체하세요.
"""
from __future__ import annotations
import os
import re
from typing import List

from langchain_text_splitters import CharacterTextSplitter
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.llms import Ollama as OllamaLLM

from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain.schema.document import Document

import gradio as gr

# -----------------------------
# 경로 설정
# -----------------------------
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
RESTAURANTS_TXT = os.path.join(CURRENT_DIR, "restaurants.txt")
FAISS_DIR = os.path.join(CURRENT_DIR, "restaurant-faiss")

# -----------------------------
# 임베딩 & LLM (Ollama)
# -----------------------------
# 임베딩 모델: 경량, 빠른 nomic-embed-text 권장. (bge-m3 등으로 교체 가능)
EMBED_MODEL = "nomic-embed-text"
# LLM 모델: DeepSeek-R1 (추론 태그 출력 가능)
LLM_MODEL = "deepseek-r1:latest"


def ensure_corpus() -> None:
    """필수 말뭉치 파일 존재 여부 확인."""
    if not os.path.isfile(RESTAURANTS_TXT):
        raise FileNotFoundError(
            f"restaurants.txt를 찾지 못했습니다: {RESTAURANTS_TXT}\n"
            "동일 폴더에 텍스트 파일을 준비하세요."
        )


def create_faiss_index(chunk_size: int = 300, chunk_overlap: int = 50) -> None:
    """restaurants.txt로부터 벡터 인덱스를 생성하여 로컬에 저장."""
    ensure_corpus()

    # 1) 문서 로드
    loader = TextLoader(RESTAURANTS_TXT, encoding="utf-8")
    documents: List[Document] = loader.load()

    # 2) 분할 (문서 리스트 → 작은 청크 리스트)
    splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    chunks: List[Document] = splitter.split_documents(documents)

    # 3) 임베딩 생성기 (Ollama)
    embeddings = OllamaEmbeddings(model=EMBED_MODEL)

    # 4) FAISS 인덱스 생성 & 저장
    db = FAISS.from_documents(chunks, embeddings)
    db.save_local(FAISS_DIR)
    print("[OK] FAISS 인덱스 생성 및 저장 완료:", FAISS_DIR)


def load_faiss_index() -> FAISS:
    """로컬에 저장된 FAISS 인덱스 로드."""
    if not os.path.isdir(FAISS_DIR):
        raise FileNotFoundError(
            f"FAISS 인덱스 폴더가 없습니다: {FAISS_DIR}\n먼저 create_faiss_index()를 실행해 생성하세요."
        )
    embeddings = OllamaEmbeddings(model=EMBED_MODEL)
    return FAISS.load_local(FAISS_DIR, embeddings, allow_dangerous_deserialization=True)


def format_docs(docs: List[Document]) -> str:
    """검색된 문서들의 내용을 하나의 컨텍스트 문자열로 합침."""
    return "\n\n".join(d.page_content for d in docs)


def build_chain(db: FAISS):
    """Retriever → Prompt → LLM → Parser 체인을 구성하여 반환."""
    retriever = db.as_retriever(search_kwargs={"k": 4})

    prompt_template = (
        "당신은 레스토랑 지식 베이스로부터 답을 요약하는 유능한 AI 비서입니다.\n"
        "아래 '맥락'에 포함된 정보만 사용해서 한국어로 간결하고 정확하게 답하세요.\n"
        "맥락에 없는 정보는 추정하지 말고 '주어진 정보로는 정확한 답변을 드릴 수 없습니다.'라고 답하세요.\n\n"
        "[맥락]\n{context}\n\n"
        "[질문]\n{question}\n\n"
        "[답변]"
    )

    prompt = PromptTemplate(template=prompt_template, input_variables=["context", "question"]) 

    llm = OllamaLLM(model=LLM_MODEL, temperature=0.2)

    chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()} 
        | prompt 
        | llm 
        | StrOutputParser()
    )
    return chain, retriever


_THINK_RE = re.compile(r"<think>[\s\S]*?</think>", re.IGNORECASE)

def strip_think(text: str) -> str:
    """DeepSeek-R1이 출력한 사고흐름 태그 제거."""
    return _THINK_RE.sub("", text).strip()


# -----------------------------
# CLI 루프 (원 스크립트와 유사한 인터랙션)
# -----------------------------

def cli_main():
    if not os.path.isdir(FAISS_DIR):
        print("[info] 인덱스가 없어 생성을 시작합니다...")
        create_faiss_index()

    db = load_faiss_index()
    chain, _ = build_chain(db)

    print("\n레스토랑 Q&A (종료: quit)\n--------------------------")
    while True:
        query = input("질문> ").strip()
        if query.lower() == "quit":
            break
        if not query:
            continue
        answer = chain.invoke(query)
        print("\n답변:", strip_think(answer), "\n")


# -----------------------------
# Gradio UI
# -----------------------------

def ui_search(query: str, k: int, remove_think: bool):
    if not os.path.isdir(FAISS_DIR):
        try:
            create_faiss_index()
        except Exception as e:
            return f"❌ 인덱스 생성 실패: {e}", ""

    try:
        db = load_faiss_index()
    except Exception as e:
        return f"❌ 인덱스 로드 실패: {e}", ""

    chain, retriever = build_chain(db)

    # 검색 미리보기
    hits: List[Document] = retriever.get_relevant_documents(query)[:k]
    preview = [f"**{i+1}.** " + (d.page_content.strip().replace("\n", " ")[:400] + ("..." if len(d.page_content) > 400 else "")) for i, d in enumerate(hits)]
    preview_md = "\n\n".join(preview) if hits else "(관련 문서 없음)"

    # LLM 답변
    raw = chain.invoke(query)
    answer = strip_think(raw) if remove_think else raw

    return answer, preview_md


def launch_ui():
    with gr.Blocks(theme=gr.themes.Soft()) as demo:
        gr.Markdown("""
        # 🍽️ 레스토랑 RAG (FAISS + Ollama + DeepSeek-R1)
        - 로컬 **Ollama** 임베딩으로 구축한 FAISS 인덱스에서 문서를 검색합니다.  
        - **DeepSeek-R1**으로 문맥 기반 답변을 생성합니다.  
        - 필요시 `<think>...</think>` 사고흐름을 제거할 수 있습니다.
        """)
        with gr.Row():
            query = gr.Textbox(label="질문", placeholder="예: 룸서비스 운영 시간은?", lines=2)
        with gr.Row():
            k = gr.Slider(1, 10, step=1, value=3, label="미리보기 문서 수")
            rm_think = gr.Checkbox(label="<think> 태그 제거", value=True)
        run = gr.Button("검색 및 답변 생성", variant="primary")

        answer = gr.Markdown(label="답변")
        ctx = gr.Markdown(label="관련 문서 미리보기")

        run.click(ui_search, inputs=[query, k, rm_think], outputs=[answer, ctx])

    demo.launch(server_name="0.0.0.0", server_port=7861)


if __name__ == "__main__":
    # CLI 실행 (원하는 경우 주석 처리 가능)
    try:
        cli_main()
    except KeyboardInterrupt:
        pass
    except Exception as e:
        print("[CLI 경고]", e)

    # Gradio UI 실행
    launch_ui()

