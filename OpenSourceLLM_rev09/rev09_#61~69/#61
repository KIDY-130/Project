#61
"""
FAISS ì§ˆì˜ ì˜ˆì œ (OpenAIEmbeddings â†’ OllamaEmbeddings ì „í™˜)
- ë¡œì»¬ Ollama ì„ë² ë”© ëª¨ë¸ ì‚¬ìš© (ì˜ˆ: nomic-embed-text)
- ê¸°ì¡´ì— ìƒì„±í•´ ë‘” FAISS ì¸ë±ìŠ¤(restaurant-faiss) ë¡œë“œ í›„ ê²€ìƒ‰
- Gradio GUI ì œê³µ (ê²€ìƒ‰ & ì„ íƒì ìœ¼ë¡œ DeepSeek-R1ë¡œ ë‹µë³€ ìƒì„±)

ì‚¬ì „ ì¤€ë¹„
1) Ollama ì„¤ì¹˜ í›„ í•„ìš”í•œ ëª¨ë¸ë“¤ ë°›ê¸°:
   - ì„ë² ë”©:   `ollama pull nomic-embed-text`
   - ìƒì„±í˜•(ì„ íƒ): `ollama pull deepseek-r1:latest`
2) ë ˆí¬/í´ë”ì— restaurants.txtë¡œë¶€í„° ë²¡í„° ì¸ë±ìŠ¤ë¥¼ ë¨¼ì € ë§Œë“¤ì–´ ë‘ì„¸ìš”
   (ì´ì „ ë‹¨ê³„ì—ì„œ ë§Œë“  restaurant-faiss í´ë”ë¥¼ ì¬ì‚¬ìš©)

ì£¼ì˜
- OpenAI API Key ë¶ˆí•„ìš”. .envë„ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
- Windows ê²½ë¡œ ë¬¸ì œì‹œ TextLoader ê²½ë¡œë¥¼ "\\"ë¡œ êµì²´í•˜ì„¸ìš”.
"""

from __future__ import annotations
import os
import asyncio
from typing import List

from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OllamaEmbeddings
from langchain.schema.document import Document

import gradio as gr

# -----------------------------
# ê²½ë¡œ ì„¤ì •
# -----------------------------
current_dir = os.path.dirname(os.path.abspath(__file__))
faiss_dir = os.path.join(current_dir, "restaurant-faiss")

# -----------------------------
# ì„ë² ë”© ëª¨ë¸ ì¤€ë¹„ (Ollama)
# -----------------------------
# nomic-embed-textëŠ” í…ìŠ¤íŠ¸ ì„ë² ë”©ìš© ê²½ëŸ‰ ëª¨ë¸ì…ë‹ˆë‹¤.
# í•„ìš”ì‹œ bge-m3 ë“±ìœ¼ë¡œ êµì²´ ê°€ëŠ¥í•©ë‹ˆë‹¤: model="bge-m3"
embeddings = OllamaEmbeddings(model="nomic-embed-text")

# -----------------------------
# ë²¡í„° DB ë¡œë“œ í—¬í¼
# -----------------------------

def load_faiss_or_raise() -> FAISS:
    if not os.path.isdir(faiss_dir):
        raise FileNotFoundError(
            f"FAISS ì¸ë±ìŠ¤ í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {faiss_dir}\n"
            "ë¨¼ì € ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•˜ì„¸ìš” (ì˜ˆ: ì´ì „ ë‹¨ê³„ì˜ ë¹Œë“œ ìŠ¤í¬ë¦½íŠ¸)."
        )
    # allow_dangerous_deserialization=True ëŠ” ì‹ ë¢°ëœ ë¡œì»¬ì—ì„œë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
    return FAISS.load_local(faiss_dir, embeddings, allow_dangerous_deserialization=True)


# -----------------------------
# ì½˜ì†” ì‹¤í–‰ìš© async main (ì› ì½”ë“œ í˜¸í™˜)
# -----------------------------
async def main() -> None:
    db = load_faiss_or_raise()

    # ê²€ìƒ‰í•  ì¿¼ë¦¬ ì˜ˆì‹œ (ì› ì½”ë“œì™€ ë™ì¼)
    query = "ìŒì‹ì ì˜ ë£¸ ì„œë¹„ìŠ¤ëŠ” ì–´ë–»ê²Œ ìš´ì˜ë˜ë‚˜ìš”?"

    # 1) í…ìŠ¤íŠ¸ ê¸°ë°˜ ìœ ì‚¬ ê²€ìƒ‰ (ìƒìœ„ 2ê°œ)
    results: List[Document] = db.similarity_search(query, k=2)
    print("[similarity_search ê²°ê³¼]\n")
    for i, doc in enumerate(results, 1):
        print(f"[{i}]", doc.page_content[:300].replace("\n", " ") + ("..." if len(doc.page_content) > 300 else ""))
    print()

    # 2) ì¿¼ë¦¬ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ê²€ìƒ‰
    embedding_vector_query = embeddings.embed_query(query)
    print("[ì„ë² ë”© ë²¡í„° ê¸¸ì´]:", len(embedding_vector_query), "\n")

    # ë¹„ë™ê¸° ë²¡í„° ê²€ìƒ‰ (ë²„ì „ë³„ ë©”ì„œë“œ ëª… ìƒì´ ê°€ëŠ¥ â†’ ì˜ˆì™¸ ì‹œ ë™ê¸° ë©”ì„œë“œë¡œ í´ë°±)
    try:
        docs: List[Document] = await db.asimilarity_search_by_vector(embedding_vector_query, k=2)
    except AttributeError:
        # ì¼ë¶€ ë²„ì „ì€ asimilarity_search_by_vector ë¯¸ì œê³µ â†’ ë™ê¸° ë©”ì„œë“œë¡œ ëŒ€ì²´
        docs = db.similarity_search_by_vector(embedding_vector_query, k=2)

    print("[ë²¡í„° ê¸°ë°˜ ê²€ìƒ‰ ì²« ê²°ê³¼]\n")
    if docs:
        print(docs[0].page_content)
    else:
        print("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")


# -----------------------------
# Gradio UI (ê²€ìƒ‰ & ì„ íƒì  ìƒì„±í˜• ì‘ë‹µ)
# -----------------------------
# ì„ íƒì ìœ¼ë¡œ DeepSeek-R1ì„ ì‚¬ìš©í•´, ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§§ì€ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
# ë¡œì»¬ì—ì„œ `ollama pull deepseek-r1:latest` í›„ ì‚¬ìš©í•˜ì„¸ìš”.
try:
    from langchain_community.llms import Ollama as OllamaLLM
    _HAS_LLM = True
except Exception:
    _HAS_LLM = False


def rag_answer(query: str, chunks: List[Document], model_name: str = "deepseek-r1:latest") -> str:
    if not _HAS_LLM:
        return "âš ï¸ DeepSeek-R1 LLM ëª¨ë“ˆì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. Ollama ì„¤ì¹˜ ë° ëª¨ë¸ pull í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”."
    if not chunks:
        return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë°”ê¿”ë³´ì„¸ìš”."

    # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (ìƒìœ„ ë¬¸ì„œ ì¼ë¶€ë§Œ ì‚¬ìš©)
    context = "\n\n".join([f"[ë¬¸ì„œ {i+1}]\n" + d.page_content for i, d in enumerate(chunks[:3])])

    prompt = (
        "ë‹¹ì‹ ì€ ìŒì‹ì  ê´€ë ¨ ë¬¸ì„œë¡œë¶€í„° ë‹µì„ ìš”ì•½í•˜ëŠ” ì¡°ë ¥ìì…ë‹ˆë‹¤. "
        "ì•„ë˜ ì»¨í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©í•˜ì—¬ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ ë‹µí•˜ì„¸ìš”.\n\n"
        f"[ì»¨í…ìŠ¤íŠ¸]\n{context}\n\n"
        f"[ì§ˆë¬¸]\n{query}\n\n"
        "[ì§€ì¹¨]\n- ì¶œì²˜ë¥¼ ë‚˜ì—´í•˜ì§€ ë§ê³  ë¬¸ì¥ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µí•˜ì„¸ìš”.\n"
        "- í™•ì‹¤í•˜ì§€ ì•Šì€ ë‚´ìš©ì€ ì¶”ì •í•˜ì§€ ë§ê³  'ë¬¸ì„œì— ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤'ë¼ê³  ë§í•˜ì„¸ìš”."
    )

    llm = OllamaLLM(model=model_name, temperature=0.2)
    return llm.invoke(prompt)


def ui_search(query: str, k: int, use_llm: bool, model_name: str) -> str:
    try:
        db = load_faiss_or_raise()
    except Exception as e:
        return f"âŒ ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}"

    if not query.strip():
        return "ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”."

    # 1) ê²€ìƒ‰
    hits: List[Document] = db.similarity_search(query, k=k)

    # 2) ê²°ê³¼ ë¬¸ìì—´ êµ¬ì„±
    result_lines = ["## ğŸ” ìœ ì‚¬ ë¬¸ì„œ ê²°ê³¼"]
    if not hits:
        result_lines.append("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        for i, d in enumerate(hits, 1):
            snippet = d.page_content.strip().replace("\n", " ")
            if len(snippet) > 400:
                snippet = snippet[:400] + "..."
            result_lines.append(f"**{i}.** {snippet}")

    # 3) ì„ íƒì ìœ¼ë¡œ ìƒì„±í˜• ë‹µë³€
    if use_llm:
        answer = rag_answer(query, hits, model_name=model_name or "deepseek-r1:latest")
        result_lines.append("\n---\n## ğŸ¤– DeepSeek-R1 ìš”ì•½ ë‹µë³€\n" + (answer or "(ë¹ˆ ì‘ë‹µ)"))

    return "\n\n".join(result_lines)


# UI êµ¬ì„±
with gr.Blocks(theme=gr.themes.Soft()) as demo:
    gr.Markdown("""
    # ğŸ½ï¸ ë ˆìŠ¤í† ë‘ ì§€ì‹ê²€ìƒ‰ (FAISS + Ollama)
    ë¡œì»¬ ì„ë² ë”©(`nomic-embed-text`)ìœ¼ë¡œ ë§Œë“  **FAISS ì¸ë±ìŠ¤**ë¥¼ ë¶ˆëŸ¬ì™€ ìœ ì‚¬ ë¬¸ì„œë¥¼ ì°¾ìŠµë‹ˆë‹¤.  
    ì„ íƒì ìœ¼ë¡œ **DeepSeek-R1**ìœ¼ë¡œ ìš”ì•½ ë‹µë³€ë„ ìƒì„±í•  ìˆ˜ ìˆì–´ìš”.
    """)

    with gr.Row():
        query_inp = gr.Textbox(label="ì§ˆë¬¸", placeholder="ì˜ˆ: ë£¸ì„œë¹„ìŠ¤ ìš´ì˜ì‹œê°„ ì•Œë ¤ì¤˜", lines=2)
    with gr.Row():
        k_inp = gr.Slider(1, 10, value=3, step=1, label="ê°€ì ¸ì˜¬ ë¬¸ì„œ ìˆ˜ (k)")
    with gr.Row():
        use_llm_chk = gr.Checkbox(label="DeepSeek-R1ë¡œ ìš”ì•½ ë‹µë³€ ìƒì„±", value=False)
        model_inp = gr.Textbox(label="Ollama LLM ëª¨ë¸ëª…", value="deepseek-r1:latest")
    run_btn = gr.Button("ê²€ìƒ‰ ì‹¤í–‰", variant="primary")

    out_md = gr.Markdown()

    run_btn.click(ui_search, inputs=[query_inp, k_inp, use_llm_chk, model_inp], outputs=out_md)


if __name__ == "__main__":
    # ì½˜ì†” í…ŒìŠ¤íŠ¸: python íŒŒì¼ì„ ì§ì ‘ ì‹¤í–‰í–ˆì„ ë•Œ ë¹„ë™ê¸° mainë„ ìˆ˜í–‰
    try:
        asyncio.run(main())
    except Exception as e:
        print("[ì£¼ì˜] ì½˜ì†” í…ŒìŠ¤íŠ¸(main) ì¤‘ ì˜¤ë¥˜:", e)
    
    # Gradio ì„œë²„ ì‹¤í–‰
    demo.launch(server_name="0.0.0.0", server_port=7860)
