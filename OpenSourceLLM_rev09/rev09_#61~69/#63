#63 o
"""
ëª©í‘œ: ê¸°ì¡´ OpenAI ê¸°ë°˜ LangChain RAG ì½”ë“œë¥¼ ì˜¤í”ˆì†ŒìŠ¤ LLM(DeepSeek-R1) + Ollama + Gradio UI ë¡œ ë³€í™˜

í•„ìˆ˜ ì¡°ê±´ ë°˜ì˜:
- API Key (sk-...) ì‚¬ìš© ì œê±°
- ë¡œì»¬ PCì— Ollama ê°€ ì„¤ì¹˜ë˜ì–´ ìˆê³ , deepseek-r1 ë° ì„ë² ë”© ëª¨ë¸ì„ pull í•´ ì‚¬ìš©í•œë‹¤ê³  ê°€ì •
- ë²¡í„°ìŠ¤í† ì–´: Chroma (ë¡œì»¬ í¼ì‹œìŠ¤í„´ìŠ¤)
- ì„ë² ë”©: OllamaEmbeddings (ì˜ˆ: nomic-embed-text)
- LLM: ChatOllama(model="deepseek-r1")
- ì›¹ ë¡œë”: WebBaseLoader ë¡œ Naver ê²½ì œ(101) ì„¹ì…˜ ìˆ˜ì§‘
- í…ìŠ¤íŠ¸ ë¶„í• : RecursiveCharacterTextSplitter (í† í° ì¸ì½”ë” ì˜ì¡´ ì œê±°)
- ê²€ìƒ‰: MMR ê¸°ë°˜ ê²€ìƒ‰
- Multi-Query(ì§ˆë¬¸ í™•ì¥) + RAG ì²´ì¸
- Gradio ë¡œ ê°„ë‹¨í•œ UI ì œê³µ (ì¸ë±ìŠ¤ ê°±ì‹  ë²„íŠ¼ / ì§ˆë¬¸ ì…ë ¥ / ì˜µì…˜)

ì‚¬ì „ ì¤€ë¹„(í„°ë¯¸ë„):
  ollama pull deepseek-r1
  ollama pull nomic-embed-text

í•„ìš” íŒ¨í‚¤ì§€(í„°ë¯¸ë„):
  pip install -U langchain langchain-community chromadb bs4 gradio

ì£¼ì˜:
- Naver í˜ì´ì§€ êµ¬ì¡°ê°€ ë°”ë€Œë©´ SoupStrainer ì„ íƒì(class_)ë¥¼ ì¡°ì •í•´ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ì²˜ìŒ ì¸ë±ì‹± ì‹œ ë‰´ìŠ¤ ì–‘/ì—°ê²° ìƒíƒœì— ë”°ë¼ ì‹œê°„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""

from __future__ import annotations
import os
import time
from typing import List, Dict, Any

# --- Web ìˆ˜ì§‘/íŒŒì‹± ---
import bs4
from langchain_community.document_loaders import WebBaseLoader

# --- ë¶„í• /ë²¡í„°ìŠ¤í† ì–´ ---
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import OllamaEmbeddings

# --- LLM / ì²´ì¸ êµ¬ì„± ---
from langchain_community.chat_models import ChatOllama
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain.prompts import ChatPromptTemplate

# --- UI ---
import gradio as gr

# =============================================
# ì„¤ì •ê°’
# =============================================
NAVER_ECON_URL = "https://news.naver.com/section/101"  # ê²½ì œ ì„¹ì…˜
CHROMA_DIR = "./chroma_db"
EMBED_MODEL = "nomic-embed-text"  # ollama ì„ë² ë”© ëª¨ë¸
LLM_MODEL = "deepseek-r1"         # ollama LLM ëª¨ë¸

# ê¸°ë³¸ ê²€ìƒ‰ íŒŒë¼ë¯¸í„°(MMR)
DEFAULT_K = 1
DEFAULT_FETCH_K = 4

# ì „ì—­ ìƒíƒœ (ê°„ë‹¨ ê´€ë¦¬)
vectorstore = None
retriever = None
last_index_time = None

# =============================================
# 1) ì¸ë±ì‹± íŒŒì´í”„ë¼ì¸
# =============================================

def load_news_documents() -> List[Any]:
    """Naver ê²½ì œ ì„¹ì…˜ì—ì„œ ë¬¸ì„œ ìˆ˜ì§‘.
    SoupStrainer ë¡œ í•„ìš”í•œ ë¸”ë¡ë§Œ íŒŒì‹±í•˜ì—¬ ì†ë„ â†‘
    """
    loader = WebBaseLoader(
        web_paths=(NAVER_ECON_URL,),
        bs_kwargs=dict(
            parse_only=bs4.SoupStrainer(
                class_=("sa_text", "sa_item_SECTION_HEADLINE")
            )
        ),
    )
    docs = loader.load()
    return docs


def split_documents(docs: List[Any]) -> List[Any]:
    """ë¬¸ì„œ ë¶„í• .
    - í† í° ê¸°ë°˜ ì¸ì½”ë”ì— ì˜ì¡´í•˜ì§€ ì•Šê³ , ë¬¸ì ë‹¨ìœ„ ë¶„í• ë¡œ ë‹¨ìˆœ/ê²¬ê³ í•˜ê²Œ êµ¬ì„±.
    - í•œêµ­ì–´ ë‰´ìŠ¤ ê¸°ì¤€ìœ¼ë¡œ 300ì/50ì ê²¹ì¹¨ì´ ë¬´ë‚œ.
    """
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=300,
        chunk_overlap=50,
        separators=["\n\n", "\n", "ã€‚", "ï¼", "!", "?", ".", " "]
    )
    return splitter.split_documents(docs)


def build_vectorstore(splits: List[Any]) -> Chroma:
    """Chroma ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ë° ë¡œì»¬ì— ì§€ì†í™”.
    OllamaEmbeddings ë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜¤í”ˆì†ŒìŠ¤ ì„ë² ë”©ìœ¼ë¡œ ì „í™˜.
    """
    embeddings = OllamaEmbeddings(model=EMBED_MODEL)
    vs = Chroma.from_documents(
        documents=splits,
        embedding=embeddings,
        persist_directory=CHROMA_DIR,
    )
    vs.persist()
    return vs


def build_retriever(vs: Chroma, k: int = DEFAULT_K, fetch_k: int = DEFAULT_FETCH_K):
    """MMR ê¸°ë°˜ retriever êµ¬ì„±."""
    return vs.as_retriever(
        search_type="mmr",
        search_kwargs={"k": k, "fetch_k": fetch_k},
    )


def build_index(k: int = DEFAULT_K, fetch_k: int = DEFAULT_FETCH_K):
    """ì „ì²´ ì¸ë±ì‹± íŒŒì´í”„ë¼ì¸ ìˆ˜í–‰.
    - ë‰´ìŠ¤ ìˆ˜ì§‘ â†’ ë¶„í•  â†’ Chroma êµ¬ì¶• â†’ Retriever êµ¬ì„±
    """
    global vectorstore, retriever, last_index_time

    docs = load_news_documents()
    splits = split_documents(docs)
    vectorstore = build_vectorstore(splits)
    retriever = build_retriever(vectorstore, k=k, fetch_k=fetch_k)
    last_index_time = time.strftime("%Y-%m-%d %H:%M:%S")


# =============================================
# 2) Multi-Query (ì§ˆë¬¸ í™•ì¥) êµ¬ì„±
# =============================================

def get_multiquery_generator():
    """ì‚¬ìš©ì ì§ˆë¬¸ì„ ë‹¤ê°ë„ë¡œ í™•ì¥í•˜ëŠ” LLM ì²´ì¸.
    DeepSeek-R1 ì„ ì‚¬ìš©í•˜ë©°, ì¶œë ¥ì€ ì¤„ë°”ê¿ˆìœ¼ë¡œ ë¶„ë¦¬ëœ 5ê°œ ë³€í˜• ì¿¼ë¦¬.
    """
    template = (
        """
        ë‹¹ì‹ ì€ ìˆ™ë ¨ëœ ê²€ìƒ‰ì–´ ìƒì„±ê¸°ì…ë‹ˆë‹¤. ì•„ë˜ ì›ë³¸ ì§ˆë¬¸ê³¼ ì£¼ì œ ë§¥ë½ì„ ë°”íƒ•ìœ¼ë¡œ,
        ë²¡í„° ê²€ìƒ‰ ì í•©ë„ê°€ ë†’ë„ë¡ ì˜ë¯¸ê°€ ì„œë¡œ ë‹¤ë¥¸ 5ê°œì˜ í•œêµ­ì–´ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ì„¸ìš”.
        - ê°ê° í•œ ì¤„ë¡œ ì¶œë ¥
        - ë¶ˆí•„ìš”í•œ ì„¤ëª… ê¸ˆì§€(ê²€ìƒ‰ì–´ë§Œ)
        ì›ë³¸ ì§ˆë¬¸: {question}
        """
    )
    prompt = ChatPromptTemplate.from_template(template)

    llm = ChatOllama(
        model=LLM_MODEL,
        temperature=0.2,
        # DeepSeek-R1 ì€ ì‚¬ìœ (Chain-of-Thought)ë¥¼ ì¥í™©íˆ ìƒì„±í•  ìˆ˜ ìˆì–´ ì§€ì‹œë¡œ ê°„ê²° ì¶œë ¥ ìœ ë„
        num_ctx=4096,
    )

    generate_queries = (
        prompt
        | llm
        | StrOutputParser()
        | (lambda x: [q.strip() for q in x.split("\n") if q.strip()])
    )
    return generate_queries


def get_unique_union(documents: List[List[Any]]) -> List[Any]:
    """ê²€ìƒ‰ ê²°ê³¼(ë¦¬ìŠ¤íŠ¸ì˜ ë¦¬ìŠ¤íŠ¸)ì˜ ì¤‘ë³µ ì œê±° í•©ì§‘í•©.
    LangChain ë¬¸ì„œ ê°ì²´ëŠ” í•´ì‹œê°€ ì–´ë ¤ì›Œ ë¬¸ìì—´ ì§ë ¬í™”ë¡œ ì¤‘ë³µ ì œê±°.
    """
    from langchain.load import dumps, loads

    flattened = [dumps(doc) for sublist in documents for doc in sublist]
    unique = list(set(flattened))
    return [loads(s) for s in unique]


# =============================================
# 3) RAG ì²´ì¸ ìƒì„±
# =============================================

def get_rag_chain(use_multiquery: bool = True):
    """Retriever + (ì˜µì…˜)Multi-Query + LLM ì„ ë¬¶ì–´ ìµœì¢… ë‹µë³€ ì²´ì¸ êµ¬ì„±."""
    if retriever is None:
        raise RuntimeError("ì¸ë±ìŠ¤ê°€ ì•„ì§ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € 'ì¸ë±ìŠ¤ ê°±ì‹ 'ì„ ì‹¤í–‰í•˜ì„¸ìš”.")

    # ë‹µë³€ í”„ë¡¬í”„íŠ¸ (ê°„ê²°, ì¶œì²˜ ìš”ì•½ í¬í•¨)
    template = (
        """
        ë‹¤ìŒì€ ë‰´ìŠ¤ ê¸°ì‚¬ì—ì„œ ì¶”ì¶œí•œ ë§¥ë½ì…ë‹ˆë‹¤. í•œêµ­ì–´ë¡œ ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µí•˜ì„¸ìš”.
        í•„ìš”ì‹œ í•µì‹¬ ê·¼ê±°(ê¸°ì‚¬ ìš”ì•½)ì™€ ì¶œì²˜ URLì„ í•¨ê»˜ ì œì‹œí•˜ì„¸ìš”.

        [ë§¥ë½]
        {context}

        [ì§ˆë¬¸]
        {question}
        """
    )
    prompt = ChatPromptTemplate.from_template(template)

    llm = ChatOllama(
        model=LLM_MODEL,
        temperature=0.2,
        num_ctx=4096,
    )

    def format_docs(docs: List[Any]) -> str:
        """ë¬¸ì„œë“¤ì„ í•˜ë‚˜ì˜ í° ë¬¸ìì—´ë¡œ í¬ë§·.
        ê° ì¡°ê°ì˜ ì¶œì²˜ URL(ê°€ëŠ¥í•˜ë©´)ê³¼ ì•ë¶€ë¶„ ìš”ì•½ í¬í•¨.
        """
        blocks = []
        for d in docs:
            src = d.metadata.get("source", "")
            snippet = d.page_content.strip().replace("\n", " ")[:400]
            blocks.append(f"- ì¶œì²˜: {src}\n  ë‚´ìš©: {snippet}")
        return "\n\n".join(blocks)

    if use_multiquery:
        # Multi-Query ì²´ì¸: ì§ˆë¬¸ â†’ 5ê°œ ë³€í˜• â†’ ê°ê° ê²€ìƒ‰ â†’ í•©ì§‘í•©
        generate_queries = get_multiquery_generator()
        retrieval_chain = generate_queries | retriever.map() | get_unique_union
        rag_chain = (
            {"context": retrieval_chain | format_docs, "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
        )
    else:
        # ë‹¨ì¼ ì¿¼ë¦¬ RAG
        rag_chain = (
            {"context": retriever | format_docs, "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
        )

    return rag_chain


# =============================================
# 4) Gradio UI ì½œë°±
# =============================================

def ui_build_index(k: int, fetch_k: int):
    """ì¸ë±ìŠ¤ ê°±ì‹  ë²„íŠ¼ ì½œë°±."""
    build_index(k=k, fetch_k=fetch_k)
    return f"ì¸ë±ìŠ¤ ê°±ì‹  ì™„ë£Œ: {last_index_time} (k={k}, fetch_k={fetch_k})"


def ui_query(question: str, use_multiquery: bool):
    """ì§ˆì˜ ë²„íŠ¼ ì½œë°±."""
    if not question or len(question.strip()) == 0:
        return "ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.", []

    chain = get_rag_chain(use_multiquery=use_multiquery)
    answer = chain.invoke(question.strip())

    # í”„ë¦¬ë·°ìš©ìœ¼ë¡œ í˜„ì¬ retriever ê¸°ì¤€ top ë¬¸ì„œë„ ë³´ì—¬ì¤€ë‹¤
    docs_preview = retriever.get_relevant_documents(question) if retriever else []
    rows = []
    for d in docs_preview:
        rows.append({
            "source": d.metadata.get("source", ""),
            "snippet": d.page_content.strip().replace("\n", " ")[:200],
        })
    return answer, rows


# =============================================
# 5) Gradio App
# =============================================
with gr.Blocks(theme=gr.themes.Soft(primary_hue=gr.themes.colors.green)) as demo:
    gr.Markdown("""
    # ğŸ” Naver ê²½ì œ ë‰´ìŠ¤ RAG (Ollama + DeepSeek-R1)
    - ì˜¤í”ˆì†ŒìŠ¤ LLM(DeepSeek-R1)ê³¼ ë¡œì»¬ ì„ë² ë”©ìœ¼ë¡œ ë™ì‘í•©ë‹ˆë‹¤.
    - **ì¸ë±ìŠ¤ ê°±ì‹ ** í›„ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.
    """)

    with gr.Row():
        k = gr.Slider(1, 5, value=DEFAULT_K, step=1, label="k (ìµœì¢… ë°˜í™˜ ë¬¸ì„œ ìˆ˜)")
        fetch_k = gr.Slider(1, 10, value=DEFAULT_FETCH_K, step=1, label="fetch_k (MMR ê³ ë ¤ ë¬¸ì„œ ìˆ˜)")
        idx_btn = gr.Button("ì¸ë±ìŠ¤ ê°±ì‹ ", variant="primary")

    idx_status = gr.Markdown("ì¸ë±ìŠ¤ê°€ ì•„ì§ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    with gr.Row():
        q = gr.Textbox(label="ì§ˆë¬¸ ì…ë ¥", placeholder="ì˜ˆ) êµ­ì±„ ë™í–¥ ìš”ì•½í•´ì¤˜ / ì§‘ê°’ ì „ë§ì€?", lines=2)
    with gr.Row():
        use_mq = gr.Checkbox(value=True, label="Multi-Query ì‚¬ìš©(ì§ˆë¬¸ í™•ì¥)")
        ask_btn = gr.Button("ì§ˆì˜", variant="primary")

    ans = gr.Markdown(label="ë‹µë³€")
    table = gr.Dataframe(headers=["source", "snippet"], datatype=["str", "str"], wrap=True, label="ì°¸ê³  ë¬¸ì„œ (ë¯¸ë¦¬ë³´ê¸°)")

    idx_btn.click(fn=ui_build_index, inputs=[k, fetch_k], outputs=idx_status)
    ask_btn.click(fn=ui_query, inputs=[q, use_mq], outputs=[ans, table])


if __name__ == "__main__":
    # ìµœì´ˆ ì‹¤í–‰ ì‹œ ì„ë² ë”©/LLM ëª¨ë¸ ë¯¸ë¦¬ ì•ˆë‚´
    print("[ì•ˆë‚´] Ollama ì—ì„œ ë‹¤ìŒ ëª¨ë¸ì´ ì¤€ë¹„ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤:")
    print("  - LLM: deepseek-r1   (ollama pull deepseek-r1)")
    print("  - Embedding: nomic-embed-text (ollama pull nomic-embed-text)\n")

    # ì•± ì‹¤í–‰
    demo.launch(server_name="0.0.0.0", server_port=7860)


# ollama pull deepseek-r1
# ollama pull nomic-embed-text
# pip install -U langchain langchain-community chromadb bs4 gradio


