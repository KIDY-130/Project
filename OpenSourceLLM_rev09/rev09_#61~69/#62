#62
"""
RAG (FAISS + Ollama ì„ë² ë”© + DeepSeek-R1 LLM) ì˜ˆì œ
- OpenAI API ì˜ì¡´ì„± ì œê±°, ë¡œì»¬ Ollama í™˜ê²½ë§Œ ì‚¬ìš©
- restaurants.txt â†’ FAISS ì¸ë±ìŠ¤ ìƒì„± & ë¡œë“œ
- LangChain Runnable ê·¸ë˜í”„(í”„ë¡¬í”„íŠ¸ â†’ LLM â†’ íŒŒì„œ)
- CLI ë£¨í”„ + Gradio UI ì œê³µ

ì¤€ë¹„ë¬¼
1) Ollama ì„¤ì¹˜ í›„ ì•„ë˜ ëª¨ë¸ pull
   - ì„ë² ë”©:   `ollama pull nomic-embed-text`
   - LLM(ìƒì„±): `ollama pull deepseek-r1:latest`
2) ê°™ì€ í´ë”ì— restaurants.txt íŒŒì¼ ë°°ì¹˜

ë©”ëª¨
- ì¼ë¶€ DeepSeek-R1ì€ ì‚¬ê³ íë¦„(<think>...</think>)ì„ ì¶œë ¥í•  ìˆ˜ ìˆì–´ UIì—ì„œ ì˜µì…˜ìœ¼ë¡œ ì œê±°í•©ë‹ˆë‹¤.
- Windows ê²½ë¡œ ë¬¸ì œì‹œ '\\' ì‚¬ìš© ë˜ëŠ” pathlibë¡œ ëŒ€ì²´í•˜ì„¸ìš”.
"""
from __future__ import annotations
import os
import re
from typing import List

from langchain_text_splitters import CharacterTextSplitter
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.llms import Ollama as OllamaLLM

from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain.schema.document import Document

import gradio as gr

# -----------------------------
# ê²½ë¡œ ì„¤ì •
# -----------------------------
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
RESTAURANTS_TXT = os.path.join(CURRENT_DIR, "restaurants.txt")
FAISS_DIR = os.path.join(CURRENT_DIR, "restaurant-faiss")

# -----------------------------
# ì„ë² ë”© & LLM (Ollama)
# -----------------------------
# ì„ë² ë”© ëª¨ë¸: ê²½ëŸ‰, ë¹ ë¥¸ nomic-embed-text ê¶Œì¥. (bge-m3 ë“±ìœ¼ë¡œ êµì²´ ê°€ëŠ¥)
EMBED_MODEL = "nomic-embed-text"
# LLM ëª¨ë¸: DeepSeek-R1 (ì¶”ë¡  íƒœê·¸ ì¶œë ¥ ê°€ëŠ¥)
LLM_MODEL = "deepseek-r1:latest"


def ensure_corpus() -> None:
    """í•„ìˆ˜ ë§ë­‰ì¹˜ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸."""
    if not os.path.isfile(RESTAURANTS_TXT):
        raise FileNotFoundError(
            f"restaurants.txtë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {RESTAURANTS_TXT}\n"
            "ë™ì¼ í´ë”ì— í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì¤€ë¹„í•˜ì„¸ìš”."
        )


def create_faiss_index(chunk_size: int = 300, chunk_overlap: int = 50) -> None:
    """restaurants.txtë¡œë¶€í„° ë²¡í„° ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•˜ì—¬ ë¡œì»¬ì— ì €ì¥."""
    ensure_corpus()

    # 1) ë¬¸ì„œ ë¡œë“œ
    loader = TextLoader(RESTAURANTS_TXT, encoding="utf-8")
    documents: List[Document] = loader.load()

    # 2) ë¶„í•  (ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ â†’ ì‘ì€ ì²­í¬ ë¦¬ìŠ¤íŠ¸)
    splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    chunks: List[Document] = splitter.split_documents(documents)

    # 3) ì„ë² ë”© ìƒì„±ê¸° (Ollama)
    embeddings = OllamaEmbeddings(model=EMBED_MODEL)

    # 4) FAISS ì¸ë±ìŠ¤ ìƒì„± & ì €ì¥
    db = FAISS.from_documents(chunks, embeddings)
    db.save_local(FAISS_DIR)
    print("[OK] FAISS ì¸ë±ìŠ¤ ìƒì„± ë° ì €ì¥ ì™„ë£Œ:", FAISS_DIR)


def load_faiss_index() -> FAISS:
    """ë¡œì»¬ì— ì €ì¥ëœ FAISS ì¸ë±ìŠ¤ ë¡œë“œ."""
    if not os.path.isdir(FAISS_DIR):
        raise FileNotFoundError(
            f"FAISS ì¸ë±ìŠ¤ í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {FAISS_DIR}\në¨¼ì € create_faiss_index()ë¥¼ ì‹¤í–‰í•´ ìƒì„±í•˜ì„¸ìš”."
        )
    embeddings = OllamaEmbeddings(model=EMBED_MODEL)
    return FAISS.load_local(FAISS_DIR, embeddings, allow_dangerous_deserialization=True)


def format_docs(docs: List[Document]) -> str:
    """ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì˜ ë‚´ìš©ì„ í•˜ë‚˜ì˜ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ë¡œ í•©ì¹¨."""
    return "\n\n".join(d.page_content for d in docs)


def build_chain(db: FAISS):
    """Retriever â†’ Prompt â†’ LLM â†’ Parser ì²´ì¸ì„ êµ¬ì„±í•˜ì—¬ ë°˜í™˜."""
    retriever = db.as_retriever(search_kwargs={"k": 4})

    prompt_template = (
        "ë‹¹ì‹ ì€ ë ˆìŠ¤í† ë‘ ì§€ì‹ ë² ì´ìŠ¤ë¡œë¶€í„° ë‹µì„ ìš”ì•½í•˜ëŠ” ìœ ëŠ¥í•œ AI ë¹„ì„œì…ë‹ˆë‹¤.\n"
        "ì•„ë˜ 'ë§¥ë½'ì— í¬í•¨ëœ ì •ë³´ë§Œ ì‚¬ìš©í•´ì„œ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ë‹µí•˜ì„¸ìš”.\n"
        "ë§¥ë½ì— ì—†ëŠ” ì •ë³´ëŠ” ì¶”ì •í•˜ì§€ ë§ê³  'ì£¼ì–´ì§„ ì •ë³´ë¡œëŠ” ì •í™•í•œ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'ë¼ê³  ë‹µí•˜ì„¸ìš”.\n\n"
        "[ë§¥ë½]\n{context}\n\n"
        "[ì§ˆë¬¸]\n{question}\n\n"
        "[ë‹µë³€]"
    )

    prompt = PromptTemplate(template=prompt_template, input_variables=["context", "question"]) 

    llm = OllamaLLM(model=LLM_MODEL, temperature=0.2)

    chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()} 
        | prompt 
        | llm 
        | StrOutputParser()
    )
    return chain, retriever


_THINK_RE = re.compile(r"<think>[\s\S]*?</think>", re.IGNORECASE)

def strip_think(text: str) -> str:
    """DeepSeek-R1ì´ ì¶œë ¥í•œ ì‚¬ê³ íë¦„ íƒœê·¸ ì œê±°."""
    return _THINK_RE.sub("", text).strip()


# -----------------------------
# CLI ë£¨í”„ (ì› ìŠ¤í¬ë¦½íŠ¸ì™€ ìœ ì‚¬í•œ ì¸í„°ë™ì…˜)
# -----------------------------

def cli_main():
    if not os.path.isdir(FAISS_DIR):
        print("[info] ì¸ë±ìŠ¤ê°€ ì—†ì–´ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        create_faiss_index()

    db = load_faiss_index()
    chain, _ = build_chain(db)

    print("\në ˆìŠ¤í† ë‘ Q&A (ì¢…ë£Œ: quit)\n--------------------------")
    while True:
        query = input("ì§ˆë¬¸> ").strip()
        if query.lower() == "quit":
            break
        if not query:
            continue
        answer = chain.invoke(query)
        print("\në‹µë³€:", strip_think(answer), "\n")


# -----------------------------
# Gradio UI
# -----------------------------

def ui_search(query: str, k: int, remove_think: bool):
    if not os.path.isdir(FAISS_DIR):
        try:
            create_faiss_index()
        except Exception as e:
            return f"âŒ ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}", ""

    try:
        db = load_faiss_index()
    except Exception as e:
        return f"âŒ ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}", ""

    chain, retriever = build_chain(db)

    # ê²€ìƒ‰ ë¯¸ë¦¬ë³´ê¸°
    hits: List[Document] = retriever.get_relevant_documents(query)[:k]
    preview = [f"**{i+1}.** " + (d.page_content.strip().replace("\n", " ")[:400] + ("..." if len(d.page_content) > 400 else "")) for i, d in enumerate(hits)]
    preview_md = "\n\n".join(preview) if hits else "(ê´€ë ¨ ë¬¸ì„œ ì—†ìŒ)"

    # LLM ë‹µë³€
    raw = chain.invoke(query)
    answer = strip_think(raw) if remove_think else raw

    return answer, preview_md


def launch_ui():
    with gr.Blocks(theme=gr.themes.Soft()) as demo:
        gr.Markdown("""
        # ğŸ½ï¸ ë ˆìŠ¤í† ë‘ RAG (FAISS + Ollama + DeepSeek-R1)
        - ë¡œì»¬ **Ollama** ì„ë² ë”©ìœ¼ë¡œ êµ¬ì¶•í•œ FAISS ì¸ë±ìŠ¤ì—ì„œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.  
        - **DeepSeek-R1**ìœ¼ë¡œ ë¬¸ë§¥ ê¸°ë°˜ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.  
        - í•„ìš”ì‹œ `<think>...</think>` ì‚¬ê³ íë¦„ì„ ì œê±°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        """)
        with gr.Row():
            query = gr.Textbox(label="ì§ˆë¬¸", placeholder="ì˜ˆ: ë£¸ì„œë¹„ìŠ¤ ìš´ì˜ ì‹œê°„ì€?", lines=2)
        with gr.Row():
            k = gr.Slider(1, 10, step=1, value=3, label="ë¯¸ë¦¬ë³´ê¸° ë¬¸ì„œ ìˆ˜")
            rm_think = gr.Checkbox(label="<think> íƒœê·¸ ì œê±°", value=True)
        run = gr.Button("ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„±", variant="primary")

        answer = gr.Markdown(label="ë‹µë³€")
        ctx = gr.Markdown(label="ê´€ë ¨ ë¬¸ì„œ ë¯¸ë¦¬ë³´ê¸°")

        run.click(ui_search, inputs=[query, k, rm_think], outputs=[answer, ctx])

    demo.launch(server_name="0.0.0.0", server_port=7861)


if __name__ == "__main__":
    # CLI ì‹¤í–‰ (ì›í•˜ëŠ” ê²½ìš° ì£¼ì„ ì²˜ë¦¬ ê°€ëŠ¥)
    try:
        cli_main()
    except KeyboardInterrupt:
        pass
    except Exception as e:
        print("[CLI ê²½ê³ ]", e)

    # Gradio UI ì‹¤í–‰
    launch_ui()

