#58
"""
ChatPDF (Gradio + Ollama + DeepSeek-R1 + Chroma)
- Closed LLM/OpenAI ì˜ì¡´ ì œê±°
- ë¡œì»¬/ì„œë²„ì— ì„¤ì¹˜ëœ Ollama ëª¨ë¸ ì‚¬ìš© (DeepSeek-R1)
- UI: Streamlit â†’ Gradio
- PDF ì—…ë¡œë“œ â†’ ë¶„í•  â†’ ì„ë² ë”© â†’ Chromaì— ì €ì¥ â†’ RAG ì§ˆì˜
- Streamlit Cloud/ì„œë²„ í™˜ê²½ì˜ sqlite ì´ìŠˆë¥¼ í”¼í•˜ê¸° ìœ„í•´ pysqlite3-binary ìš°íšŒ ì½”ë“œ ì ìš©

ì‚¬ì „ ì¤€ë¹„
1) Ollama ì„¤ì¹˜ ë° ì‹¤í–‰ (ë¡œì»¬ ë˜ëŠ” ì„œë²„): https://ollama.com
2) ëª¨ë¸ ë°›ê¸°
   - LLM:      $ ollama pull deepseek-r1:8b
   - Embedding: $ ollama pull nomic-embed-text
3) íŒ¨í‚¤ì§€ ì„¤ì¹˜
   $ pip install -r requirements.txt

ì‹¤í–‰
   $ python app.py
"""

# --- SQLite ëŒ€ì²´ (Streamlit Cloud/ì¼ë¶€ ë¦¬ëˆ…ìŠ¤ í™˜ê²½ìš©) ---
# ìš”ì²­ì— ë”°ë¼ ì£¼ì„ í•´ì œ(í™œì„±í™”) í•©ë‹ˆë‹¤.
__import__('pysqlite3')
import sys
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')

from __future__ import annotations
import os
import re
import tempfile
from typing import List

import gradio as gr

# LangChain & ë°ì´í„° ì²˜ë¦¬
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain.schema import Document
from langchain.callbacks.base import BaseCallbackHandler
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain.retrievers.multi_query import MultiQueryRetriever

# Ollama (LLM/Embeddings)
from langchain_ollama.llms import OllamaLLM
from langchain_ollama.embeddings import OllamaEmbeddings

# ------------------------------
# ì„¤ì •
# ------------------------------
LLM_MODEL = "deepseek-r1:8b"           # ì¶”ë¡ ìš©
EMBED_MODEL = "nomic-embed-text"       # ì„ë² ë”©ìš© (Ollama ì§€ì›)
CHUNK_SIZE = 600
CHUNK_OVERLAP = 80
TOP_K = 4
TEMPERATURE = 0.2
MAX_TOKENS = 1024

# DeepSeek-R1ì˜ <think> ë¸”ë¡ ì œê±°ìš© ì •ê·œì‹
THINK_BLOCK_PATTERN = re.compile(r"<think>.*?</think>", re.DOTALL)

def strip_reasoning(text: str) -> str:
    """DeepSeek-R1ì´ ë°˜í™˜í•˜ëŠ” ì‚¬ìœ  ë¸”ë¡(<think>...</think>) ì œê±°"""
    return THINK_BLOCK_PATTERN.sub("", text).strip()

# ------------------------------
# PDF â†’ Documents
# ------------------------------
def pdf_to_documents(pdf_bytes: bytes, filename: str) -> List[Document]:
    """ì—…ë¡œë“œëœ PDF ë°”ì´íŠ¸ë¥¼ ì„ì‹œíŒŒì¼ë¡œ ì €ì¥ í›„ LangChain Documentsë¡œ ë¡œë“œ"""
    tmpdir = tempfile.TemporaryDirectory()
    temp_path = os.path.join(tmpdir.name, filename)
    with open(temp_path, "wb") as f:
        f.write(pdf_bytes)
    loader = PyPDFLoader(temp_path)
    pages = loader.load_and_split()
    return pages

# ------------------------------
# Chroma ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
# ------------------------------
def build_vectorstore(docs: List[Document]):
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        length_function=len,
        is_separator_regex=False,
    )
    chunks = splitter.split_documents(docs)

    # Ollama ì„ë² ë”©
    embeddings = OllamaEmbeddings(model=EMBED_MODEL)
    # ë©”ëª¨ë¦¬í˜• Chroma ì¸ë±ìŠ¤ (í•„ìš” ì‹œ persist_directoryë¡œ ë””ìŠ¤í¬ ì €ì¥ ê°€ëŠ¥)
    vectordb = Chroma.from_documents(chunks, embedding=embeddings)
    return vectordb

# ------------------------------
# í”„ë¡¬í”„íŠ¸ (Hub ì˜ì¡´ ì œê±°, ìì²´ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©)
# ------------------------------
RAG_PROMPT = PromptTemplate.from_template(
    """You are a helpful assistant. Use the provided context to answer the question.
If the answer is not in the context, say you don't know briefly.

# Context
{context}

# Question
{question}

# Answer (be concise):"""
)

# ------------------------------
# ìŠ¤íŠ¸ë¦¬ë° í•¸ë“¤ëŸ¬ (Gradioì™€ í˜¸í™˜ë˜ë„ë¡ í† í° ëˆ„ì  ì œê³µ)
# ------------------------------
class StreamToBufferHandler(BaseCallbackHandler):
    def __init__(self):
        self.buffer = ""

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        self.buffer += token

    def read(self) -> str:
        return self.buffer

# ------------------------------
# íŒŒì´í”„ë¼ì¸ êµ¬ì„±
# ------------------------------
def build_chain(vectordb):
    # ë¡œì»¬ LLM (Ollama)
    llm = OllamaLLM(
        model=LLM_MODEL,
        temperature=TEMPERATURE,
        num_predict=MAX_TOKENS,
    )

    # ë‹¤ì¤‘ ì¿¼ë¦¬ë¡œ ë¦¬íŠ¸ë¦¬ë²„ë¥¼ ê°•í™” (ì§ˆë¬¸ ë³€í˜•ì„ LLMì´ ìƒì„±)
    retriever = MultiQueryRetriever.from_llm(
        retriever=vectordb.as_retriever(search_kwargs={"k": TOP_K}),
        llm=llm
    )

    def format_docs(docs: List[Document]) -> str:
        return "\n\n".join(d.page_content for d in docs)

    chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | RAG_PROMPT
        | llm
        | StrOutputParser()
    )
    return chain

# ------------------------------
# Gradio ìƒí˜¸ì‘ìš© í•¨ìˆ˜
# ------------------------------
def ingest_pdf(pdf_file):
    """PDF ì—…ë¡œë“œ í›„ ì¸ë±ìŠ¤ êµ¬ì¶•"""
    if pdf_file is None:
        return None, "â— PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”."
    docs = pdf_to_documents(pdf_file.read(), pdf_file.name)
    vectordb = build_vectorstore(docs)
    return vectordb, "âœ… ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ! ì´ì œ ì§ˆë¬¸í•´ ë³´ì„¸ìš”."

def ask_question(state, question):
    """ì§ˆë¬¸ â†’ RAG ì‹¤í–‰ (ìŠ¤íŠ¸ë¦¬ë°)"""
    if state is None:
        yield "â— ë¨¼ì € PDFë¥¼ ì—…ë¡œë“œí•˜ì—¬ ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•´ ì£¼ì„¸ìš”."
        return
    if not question or not question.strip():
        yield "â— ì§ˆë¬¸ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”."
        return

    chain = build_chain(state)
    # DeepSeek-R1ì˜ ì¶”ë¡  í† í°ì„ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë°›ì„ í•¸ë“¤ëŸ¬
    handler = StreamToBufferHandler()

    # LangChainì˜ ìŠ¤íŠ¸ë¦¬ë°ì€ ì½œë°±ê³¼ ì œë„ˆë ˆì´í„°ë¥¼ ì¡°í•©í•´ ì²˜ë¦¬
    # OllamaLLMì€ í† í° ë‹¨ìœ„ ì½œë°±ì„ í˜¸ì¶œí•¨
    # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ invoke í›„ <think> ì œê±°ë§Œ ìˆ˜í–‰ (ì‹¤ì‹œê°„ í† í° ì†¡ì¶œì´ í•„ìš”í•˜ë©´ LCELì˜ stream() í™œìš©)
    try:
        # stream ëª¨ë“œ: chain.stream(question) ì‚¬ìš© ê°€ëŠ¥ (LangChain 0.2+)
        # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ invoke í›„ ê²°ê³¼ ë°˜í™˜
        result = chain.invoke(question, config={"callbacks": [handler]})
        clean = strip_reasoning(result)
        # ì½œë°± buffer(ì¤‘ê°„ í† í°)ì™€ ìµœì¢… ë‹µì´ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ìµœì¢… í´ë¦° í…ìŠ¤íŠ¸ë¥¼ ì¶œë ¥
        yield clean if clean else (handler.read() or "âš ï¸ ë¹ˆ ì‘ë‹µì…ë‹ˆë‹¤.")
    except Exception as e:
        yield f"ğŸš« ì˜¤ë¥˜ ë°œìƒ: {e}"

# ------------------------------
# Gradio UI
# ------------------------------
with gr.Blocks(title="ChatPDF (Ollama + DeepSeek-R1)") as demo:
    gr.Markdown("# ChatPDF ğŸ“„ğŸ”  \në¡œì»¬/ì„œë²„ì˜ **Ollama + DeepSeek-R1**ê³¼ **Chroma**ë¡œ PDFë¥¼ ì§ˆì˜í•©ë‹ˆë‹¤.")

    with gr.Row():
        with gr.Column(scale=1):
            pdf = gr.File(label="PDF ì—…ë¡œë“œ", file_types=[".pdf"])
            build_btn = gr.Button("ì¸ë±ìŠ¤ ìƒì„±", variant="primary")
            status = gr.Markdown()

            # ìƒíƒœ: ë²¡í„°DB í•¸ë“¤ (ì„¸ì…˜ ë‚´ ì €ì¥)
            vectordb_state = gr.State()

        with gr.Column(scale=1):
            question = gr.Textbox(label="ì§ˆë¬¸", placeholder="ì˜ˆ) 2ì¥ ì‹¤í—˜ ê²°ê³¼ ìš”ì•½í•´ì¤˜", lines=2)
            ask_btn = gr.Button("ì§ˆë¬¸í•˜ê¸°", variant="secondary")
            answer = gr.Markdown()

    # ì´ë²¤íŠ¸ ë°”ì¸ë”©
    build_btn.click(fn=ingest_pdf, inputs=[pdf], outputs=[vectordb_state, status])
    ask_btn.click(fn=ask_question, inputs=[vectordb_state, question], outputs=[answer])

    gr.Markdown("---\n**íŒ**\n- ì—¬ëŸ¬ PDFë¥¼ í•©ì³ ì§ˆì˜í•˜ë ¤ë©´ í•˜ë‚˜ì˜ PDFë¡œ ë³‘í•© í›„ ì—…ë¡œë“œí•˜ì„¸ìš”.\n- ì„ë² ë”©/LLM ëª¨ë¸ì€ íŒŒì¼ ìƒë‹¨ ìƒìˆ˜(LLM_MODEL, EMBED_MODEL)ë¡œ ë³€ê²½ ê°€ëŠ¥í•©ë‹ˆë‹¤.")

if __name__ == "__main__":
    # Gradio ì„œë²„ ì‹¤í–‰
    # Cloudì—ì„œ ì™¸ë¶€ ê³µê°œ í•„ìš” ì‹œ server_name="0.0.0.0"
    demo.queue().launch(server_name="0.0.0.0", server_port=7860, show_api=False)
















â€˜â€™â€˜
# Core
gradio>=4.36.0
ollama>=0.3.0

# LangChain & integrations
langchain>=0.2.12
langchain-community>=0.2.10
langchain-core>=0.2.24
langchain-text-splitters>=0.2.2
langchain-chroma>=0.1.2
langchain-ollama>=0.1.0

# Vector store
chromadb>=0.5.3

# PDF
pypdf>=4.2.0

# SQLite shim for serverless containers (VERY IMPORTANT)
pysqlite3-binary

# (Optional) for better performance on some platforms
uvicorn
â€™â€˜â€™
