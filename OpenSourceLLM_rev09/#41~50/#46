#46
"""
OpenAI(API Key 필요) 예제를
➡ Ollama의 오픈소스 LLM(DeepSeek-R1)로 변환
+ Gradio UI까지 포함한 단일 파이썬 스크립트

[전제]
- 로컬 PC에 Ollama가 설치되어 있고, DeepSeek-R1 계열 모델을 받아둔 상태라고 가정합니다.
  (예: `ollama pull deepseek-r1:7b` 또는 `deepseek-r1:14b` 등)
- LangChain 최신 버전(0.2+) 기준으로 community 패키지 사용

[설치]
- 필요한 경우 아래를 한 번만 설치하세요.
  pip install -U langchain langchain-community gradio

[실행]
- python 이_파일명.py
- 브라우저에서 자동으로 열리는 Gradio 인터페이스 사용
"""

import os
from typing import Dict

import gradio as gr

# LangChain 핵심 모듈들
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain, SimpleSequentialChain, SequentialChain

# ✅ OpenAI → ❌ 사용하지 않음 (API Key 불필요)
# from langchain.llms import OpenAI
# from langchain.chat_models import ChatOpenAI

# ✅ Ollama용 LangChain community 드라이버 (로컬 오픈소스 LLM)
from langchain_community.llms import Ollama
from langchain_community.chat_models import ChatOllama


# ---------------------------------------------------------
# 1) 공통: 로컬 Ollama LLM 팩토리
# ---------------------------------------------------------
def make_llm(model: str = "deepseek-r1:7b", temperature: float = 0.5) -> Ollama:
    """Ollama LLM 인스턴스를 생성합니다.
    - 모델은 로컬에 설치된 Ollama 모델 이름을 사용합니다.
    - 예: deepseek-r1:7b, deepseek-r1:14b, qwen2.5:7b 등
    """
    return Ollama(model=model, temperature=temperature)


def make_chat_llm(model: str = "deepseek-r1:7b", temperature: float = 0.5) -> ChatOllama:
    """대화형 ChatOllama 인스턴스 생성."""
    return ChatOllama(model=model, temperature=temperature)


# ---------------------------------------------------------
# 2) 데모: 단일 프롬프트 → 코드 생성
#    (원문: OpenAI llm(text) 호출 부분 대체)
# ---------------------------------------------------------
def generate_code(prompt: str, model: str, temperature: float) -> str:
    llm = make_llm(model=model, temperature=temperature)
    # 간단 호출 (LLMChain 없이 바로)
    return llm(prompt)


# ---------------------------------------------------------
# 3) 데모: 피보나치(원문에 있던 파이썬 재귀 구현)
# ---------------------------------------------------------
def fibonacci(n: int) -> int:
    if n == 0:
        return 0
    elif n == 1:
        return 1
    else:
        return fibonacci(n - 1) + fibonacci(n - 2)


def fibonacci_first_k(k: int = 10) -> str:
    seq = [str(fibonacci(i)) for i in range(k)]
    return ", ".join(seq)


# ---------------------------------------------------------
# 4) LangChain 체인 변환 (ChatOpenAI → ChatOllama)
#    A) 테스트 코드 추가 요청 체인
# ---------------------------------------------------------
_test_prompt = PromptTemplate(
    input_variables=["product"],
    template=(
        "Python으로 다음 프로그램 코드에 테스트 코드를 추가하여 작성해주세요.: {product}\n"
        "- pytest 스타일 단위 테스트 예시 최소 2개 포함\n"
        "- 코드 블록은 ```python 으로 감싸기"
    ),
)


def run_test_chain(user_input: str, model: str, temperature: float) -> str:
    chat_llm = make_chat_llm(model=model, temperature=temperature)
    chain = LLMChain(llm=chat_llm, prompt=_test_prompt)
    return chain.run(user_input)


# ---------------------------------------------------------
#    B) 아이디어 → 계획(5개) → 기획서(1~2개) 연쇄 체인
#       (원문의 SimpleSequentialChain/SequentialChain 구조를 그대로 이식)
# ---------------------------------------------------------
_idea_prompt = PromptTemplate(
    input_variables=["job"],
    template=(
        "다음에 제시한 테마의 가치를 정의하고 새로운 비즈니스의 가능성을 개념적으로 800자 이내로 생각해주세요: {job}"
    ),
)

_plan_prompt = PromptTemplate(
    input_variables=["idea"],
    template=(
        "다음 아이디어로 구체적인 계획을 다섯 개 생각하고, 각 계획별로 세부 실행 사항을 나열하되, "
        "IT가 효과적으로 공헌 가능한 순서대로 제시해주세요: {idea}"
    ),
)

_proposal1_prompt = PromptTemplate(
    input_variables=["plan"],
    template=(
        "제시된 다섯 개의 계획 중에 첫 번째 계획만 쪼개서 기획서의 형태로 구성하고, 상세 내용을 포함해서 제시해주세요: {plan}"
    ),
)

_proposal2_prompt = PromptTemplate(
    input_variables=["plan"],
    template=(
        "제시된 다섯 개의 계획 중에 두 번째 계획만 쪼개서 기획서의 형태로 구성하고, 상세 내용을 포함해서 제시해주세요: {plan}"
    ),
)


def run_business_sequential(job: str, model: str, temperature: float) -> Dict[str, str]:
    """원문의 SequentialChain을 ChatOllama로 재현합니다."""
    chat_llm = make_chat_llm(model=model, temperature=temperature)

    idea_chain = LLMChain(llm=chat_llm, prompt=_idea_prompt, output_key="idea")
    plan_chain = LLMChain(llm=chat_llm, prompt=_plan_prompt, output_key="plan")
    proposal_chain_1 = LLMChain(llm=chat_llm, prompt=_proposal1_prompt, output_key="proposal_1")
    proposal_chain_2 = LLMChain(llm=chat_llm, prompt=_proposal2_prompt, output_key="proposal_2")

    overall_chain = SequentialChain(
        chains=[idea_chain, plan_chain, proposal_chain_1, proposal_chain_2],
        input_variables=["job"],
        output_variables=["idea", "plan", "proposal_1", "proposal_2"],
        verbose=False,
    )

    results = overall_chain(job)
    return results


# ---------------------------------------------------------
# 5) Gradio UI
# ---------------------------------------------------------
DEFAULT_MODEL = os.environ.get("OLLAMA_MODEL", "deepseek-r1:7b")

with gr.Blocks(title="OpenAI → Ollama(DeepSeek-R1) 변환 데모", theme=gr.themes.Soft()) as demo:
    gr.Markdown(
        """
        # OpenAI → Ollama(DeepSeek-R1) + LangChain + Gradio
        - **API Key 불필요**, 전부 **로컬 LLM**으로 실행됩니다.
        - 모델은 Ollama가 관리합니다. (예: `deepseek-r1:7b`)
        - 좌측 탭에서 데모를 선택하세요.
        """
    )

    with gr.Row():
        model_dd = gr.Textbox(value=DEFAULT_MODEL, label="Ollama 모델 이름", placeholder="deepseek-r1:7b")
        temp_sl = gr.Slider(0.0, 1.0, value=0.5, step=0.05, label="Temperature")

    with gr.Tab("① 단일 프롬프트 → 코드 생성"):
        prompt_in = gr.Textbox(
            value="피보나치 수열을 구하는 Python 프로그램을 작성해주세요.",
            label="프롬프트",
            lines=4,
        )
        out = gr.Markdown(label="LLM 응답")
        gr.Button("실행").click(
            fn=lambda p, m, t: generate_code(p, m, t),
            inputs=[prompt_in, model_dd, temp_sl],
            outputs=out,
        )

    with gr.Tab("② 테스트 코드 추가 체인"):
        t_in = gr.Textbox(
            value="피보나치 수열을 구한다",
            label="기능 설명(자연어)",
            lines=2,
        )
        t_out = gr.Markdown(label="생성 코드 + 테스트")
        gr.Button("체인 실행").click(
            fn=lambda s, m, t: run_test_chain(s, m, t),
            inputs=[t_in, model_dd, temp_sl],
            outputs=t_out,
        )

    with gr.Tab("③ 비즈니스 구상(연쇄 체인)"):
        job_in = gr.Textbox(value="농업", label="테마(예: 농업, 교육, 의료)")
        idea_out = gr.Markdown(label="아이디어")
        plan_out = gr.Markdown(label="계획(5개)")
        prop1_out = gr.Markdown(label="기획서 #1")
        prop2_out = gr.Markdown(label="기획서 #2")

        def _run(job, m, t):
            res = run_business_sequential(job, m, t)
            return res["idea"], res["plan"], res["proposal_1"], res["proposal_2"]

        gr.Button("연쇄 실행").click(
            fn=_run,
            inputs=[job_in, model_dd, temp_sl],
            outputs=[idea_out, plan_out, prop1_out, prop2_out],
        )

    with gr.Tab("④ 피보나치(파이썬 실행 예)"):
        k_in = gr.Slider(1, 30, value=10, step=1, label="앞에서부터 몇 개?")
        fib_out = gr.Textbox(label="출력", interactive=False)
        gr.Button("계산").click(lambda k: fibonacci_first_k(int(k)), inputs=k_in, outputs=fib_out)

    gr.Markdown(
        """
        ---
        ### 참고
        - 모델이 다운로드되지 않았다면 **터미널에서 먼저** 가져오세요:  
          `ollama pull deepseek-r1:7b`
        - 더 큰 모델을 쓰면 품질은 향상될 수 있으나 메모리 요구량도 커집니다.
        - 템플릿/체인은 원문의 OpenAI 기반 코드를 **ChatOllama/Ollama**로 치환하여 동작합니다.
        """
    )


if __name__ == "__main__":
    # 웹 UI 실행
    demo.launch()
