#40
import gradio as gr
import subprocess

# Ollamaë¥¼ í†µí•´ DeepSeek-R1 ëª¨ë¸ì— ìŠ¤íŠ¸ë¦¬ë° ì§ˆì˜í•˜ëŠ” í•¨ìˆ˜
def query_deepseek_stream(user_text):
    """
    Ollama CLIë¥¼ ì‚¬ìš©í•˜ì—¬ DeepSeek-R1 ëª¨ë¸ì„ ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¡œ ì‹¤í–‰í•˜ê³ ,
    í† í° ë‹¨ìœ„ë¡œ ì¶œë ¥ë˜ëŠ” ê²°ê³¼ë¥¼ yield í•˜ì—¬ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ êµ¬í˜„
    """
    try:
        # ollama runì„ subprocess.Popenìœ¼ë¡œ ì‹¤í–‰ (ì‹¤ì‹œê°„ stdout ì½ê¸° ê°€ëŠ¥)
        process = subprocess.Popen(
            ["ollama", "run", "deepseek-r1", user_text],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            bufsize=1
        )

        # stdoutì„ í•œ ì¤„ì”© ì½ìœ¼ë©´ì„œ yield
        for line in iter(process.stdout.readline, ""):
            if line.strip():
                yield line.strip()

        process.stdout.close()
        process.wait()
    except Exception as e:
        yield f"ì—ëŸ¬ ë°œìƒ: {str(e)}"


# Gradioìš© í•¨ìˆ˜ (ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥ ì§€ì›)
def chat_with_model(user_text):
    return query_deepseek_stream(user_text)


# Gradio ì¸í„°í˜ì´ìŠ¤ êµ¬ì„±
with gr.Blocks() as demo:
    gr.Markdown("## ğŸ’¬ DeepSeek-R1 ìŠ¤íŠ¸ë¦¬ë° ì±—ë´‡ (Ollama ê¸°ë°˜)")
    gr.Markdown("Python ì¬ê·€ í”¼ë³´ë‚˜ì¹˜ ì˜ˆì œ ì½”ë“œ ê°™ì€ ì§ˆë¬¸ì„ ì…ë ¥í•´ ë³´ì„¸ìš”!")

    with gr.Row():
        user_input = gr.Textbox(
            label="ì§ˆë¬¸ ì…ë ¥",
            placeholder="ì˜ˆ: Python ì¬ê·€ í”¼ë³´ë‚˜ì¹˜ ì½”ë“œ ì‘ì„±í•´ì£¼ì„¸ìš”."
        )
    with gr.Row():
        output = gr.Textbox(
            label="ëª¨ë¸ ì‘ë‹µ (ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°)",
            placeholder="ì—¬ê¸°ì— ëª¨ë¸ì˜ ë‹µë³€ì´ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥ë©ë‹ˆë‹¤."
        )

    # ë²„íŠ¼ ì´ë²¤íŠ¸ (ìŠ¤íŠ¸ë¦¬ë° ì§€ì›)
    submit_btn = gr.Button("ì „ì†¡")
    submit_btn.click(fn=chat_with_model, inputs=user_input, outputs=output)

# ì‹¤í–‰
if __name__ == "__main__":
    demo.launch()
