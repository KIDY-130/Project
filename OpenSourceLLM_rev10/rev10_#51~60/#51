#51
# ============================================================
# Gradio UI for Ollama (DeepSeek-R1) using LangChain
# ------------------------------------------------------------
# âœ… ì™„ì „ ë¡œì»¬: OpenAI API Key ë¶ˆí•„ìš”
# âœ… ëª¨ë¸: DeepSeek-R1 (Ollama)
# âœ… UI: Gradio
# ------------------------------------------------------------
# ì¤€ë¹„ì‚¬í•­ (í„°ë¯¸ë„ì—ì„œ 1íšŒë§Œ):
#   1) Ollama ì„¤ì¹˜: https://ollama.com/download
#   2) ëª¨ë¸ ë‹¤ìš´ë¡œë“œ:  ollama pull deepseek-r1:latest
#   3) Ollama ì„œë²„ ì‹¤í–‰(ë³´í†µ ìë™): http://localhost:11434
# ------------------------------------------------------------
# í•„ìš” íŒ¨í‚¤ì§€ ì„¤ì¹˜(ì—†ë‹¤ë©´ ì£¼ì„ í•´ì œí•˜ì—¬ ì„¤ì¹˜):
# !pip install gradio langchain langchain-core langchain-community langchain-experimental
# ë˜ëŠ” ìµœì‹  ollama í†µí•© íŒ¨í‚¤ì§€ë¥¼ ì“°ëŠ” ê²½ìš°:
# !pip install langchain-ollama
# ============================================================

import gradio as gr
from typing import Optional

# LangChain Prompt/Parser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# Ollamaìš© Chat ëª¨ë¸ import (ë²„ì „ì— ë”°ë¼ ê²½ë¡œê°€ ë‹¤ë¥¼ ìˆ˜ ìˆì–´ try/except ì²˜ë¦¬)
try:
    from langchain_ollama import ChatOllama  # ìµœì‹  ê¶Œì¥ ê²½ë¡œ
except Exception:
    from langchain_community.chat_models import ChatOllama  # êµ¬ë²„ì „ fallback


# -----------------------------
# LLM ìƒì„± ìœ í‹¸
# -----------------------------
def get_llm(model_name: str = "deepseek-r1:latest", temperature: float = 0.7) -> ChatOllama:
    """Ollamaì˜ DeepSeek-R1 Chat ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
    - model_name: Ollamaì— ì„¤ì¹˜ëœ ëª¨ë¸ íƒœê·¸(ì˜ˆ: deepseek-r1:latest)
    - temperature: ì°½ì˜ì„± ì •ë„(0.0~1.0 ê¶Œì¥)
    """
    return ChatOllama(
        model=model_name,
        temperature=temperature,
        # ì¼ë¶€ êµ¬í˜„ì€ ì•„ë˜ ì¸ìë¥¼ ì§€ì›í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¬¸ì œê°€ ë˜ë©´ ì œê±°í•˜ì„¸ìš”.
        num_ctx=8192,
        streaming=False,
    )


def build_chain(system_prompt: str, temperature: float, model_name: str):
    """í”„ë¡¬í”„íŠ¸ â†’ LLM â†’ ë¬¸ìì—´ íŒŒì„œ ì²´ì¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤."""
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("user", "{input}")
    ])
    llm = get_llm(model_name=model_name, temperature=temperature)
    output_parser = StrOutputParser()
    chain = prompt | llm | output_parser
    return chain


# -----------------------------
# Gradio í•¸ë“¤ëŸ¬
# -----------------------------

def infer(user_input: str, system_prompt: str, temperature: float, model_name: str) -> str:
    """ë‹¨ì¼ ì§ˆì˜ ì‘ë‹µ í•¸ë“¤ëŸ¬."""
    if not user_input.strip():
        return "ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."
    try:
        chain = build_chain(system_prompt=system_prompt, temperature=temperature, model_name=model_name)
        result = chain.invoke({"input": user_input})
        # LangChain ê°ì²´ê°€ ì˜¬ ìˆ˜ë„ ìˆì–´ ë¬¸ìì—´ ë³€í™˜
        return str(result)
    except Exception as e:
        return f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"


# ============================================================
# Gradio UI êµ¬ì„±
# ============================================================
with gr.Blocks(title="Ollama Â· DeepSeek-R1 Â· LangChain UI", theme=gr.themes.Soft()) as demo:
    gr.Markdown(
        """
        # ğŸ§ª Ollama Â· DeepSeek-R1 Â· LangChain
        OpenAI ëŒ€ì‹  **ë¡œì»¬ LLM(DeepSeek-R1, Ollama)**ìœ¼ë¡œ ë™ì‘í•˜ëŠ” ê°„ë‹¨í•œ ì±„íŒ… UIì…ë‹ˆë‹¤.
        - API Key ë¶ˆí•„ìš”
        - ëª¨ë¸ ì¤€ë¹„: `ollama pull deepseek-r1:latest`
        - âš™ï¸ ì•„ë˜ ì„¤ì •ì—ì„œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸/ì˜¨ë„/ëª¨ë¸ëª…ì„ ë°”ê¿€ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        """
    )

    with gr.Row():
        with gr.Column(scale=2):
            user_input = gr.Textbox(label="ì§ˆë¬¸ ì…ë ¥", placeholder="ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”! ì˜ˆ: hi", lines=6)
            btn = gr.Button("ğŸ’¬ ì‹¤í–‰", variant="primary")
            output = gr.Markdown()
        with gr.Column(scale=1):
            system_prompt = gr.Textbox(
                label="System Prompt",
                value="You are a helpful assistant.",
                lines=6,
            )
            temperature = gr.Slider(0.0, 1.5, value=0.7, step=0.1, label="Temperature")
            model_name = gr.Textbox(label="Ollama ëª¨ë¸ëª…", value="deepseek-r1:latest")
            gr.Markdown(
                """
                **Tips**
                - ëª¨ë¸ ì„¤ì¹˜: `ollama pull deepseek-r1:latest`
                - ë‹¤ë¥¸ ëª¨ë¸ì„ ì“°ê³  ì‹¶ë‹¤ë©´ ëª¨ë¸ëª…ì— ì˜ˆ: `llama3.1:8b-instruct` ë“±ì„ ì…ë ¥.
                - ì¶œë ¥ì´ ê¸¸ê±°ë‚˜ ëŠë¦¬ë©´ Temperatureë¥¼ ë‚®ì¶”ê±°ë‚˜, num_ctxë¥¼ ì¤„ì—¬ë³´ì„¸ìš”.
                """
            )

    btn.click(fn=infer, inputs=[user_input, system_prompt, temperature, model_name], outputs=output)


# ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰
if __name__ == "__main__":
    # ì™¸ë¶€ ì ‘ì†ì´ í•„ìš” ì—†ìœ¼ë©´ share=Falseë¡œ ë‘ì„¸ìš”.
    demo.launch(server_name="0.0.0.0", server_port=7860, share=False)


