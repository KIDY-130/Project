#48
# ============================================================
# OpenCode 변환본: Closed LLM(예: ChatGPT/OpenAI) → 오픈소스 LLM
# 실행 환경: 로컬 PC, Ollama + DeepSeek-R1 모델 사용
# UI: Gradio (간단한 탭형 인터페이스)
# ------------------------------------------------------------
# 필요한 패키지(미설치 시 주석 해제하여 설치하세요):
# !pip install gradio langchain langchain-community langchain-experimental wikipedia
# !pip install duckduckgo-search llama-index bs4 lxml html5lib
# ------------------------------------------------------------
# Ollama 설치 및 모델 준비(터미널에서 한 번만):
#   1) https://ollama.com/download 에서 Ollama 설치
#   2) deepseek-r1 모델 받기: `ollama pull deepseek-r1:latest`
#   3) 서버는 기본적으로 localhost:11434에서 가동됩니다.
# ============================================================

import os
import json
import gradio as gr

# -------------------------------
# LangChain: 오픈소스 LLM (Ollama)
# -------------------------------
# LangChain 버전에 따라 ChatOllama import 경로가 다를 수 있어 try/except 처리합니다.
try:
    # 권장(신규) 경로
    from langchain_ollama import ChatOllama
except Exception:
    # 구(community) 경로 (일부 버전)
    from langchain_community.chat_models import ChatOllama

# 도구/유틸
from langchain_experimental.tools import PythonREPLTool
from langchain.agents import initialize_agent
from langchain.tools import Tool

# Wikipedia(키 불필요)
from langchain_community.tools import WikipediaQueryRun
from langchain_community.utilities import WikipediaAPIWrapper

# DuckDuckGo 검색(키 불필요, Google API 대체)
from langchain_community.tools import DuckDuckGoSearchRun

# --------------------------------
# LlamaIndex: 로컬 LLM으로 설정하기
# --------------------------------
from llama_index.core import VectorStoreIndex, download_loader, Settings
try:
    # 권장(신규) 경로
    from llama_index.llms.ollama import Ollama as LlamaIndexOllama
except Exception:
    # 일부 구버전 fallback
    from llama_index.core.llms import Ollama as LlamaIndexOllama

# PDF 로더(한글 PDF 대응)
CJKPDFReader = download_loader("CJKPDFReader")
# 웹페이지 로더(BeautifulSoup 기반)
BeautifulSoupWebReader = download_loader("BeautifulSoupWebReader")

# ============================================================
# 공통 LLM 생성 함수
# ------------------------------------------------------------
# DeepSeek-R1은 "생각(Thought)" 토큰을 생성하는 추론형 모델입니다.
# LangChain/Agents와 함께 사용할 때는 온도, 컨텍스트 길이 등을 조정하세요.
# ============================================================
def make_chat_llm(temperature: float = 0.2):
    """
    Ollama의 DeepSeek-R1을 LangChain Chat 모델로 생성합니다.
    - 모델명: deepseek-r1:latest (사전에 `ollama pull` 필요)
    - 로컬 실행: http://localhost:11434
    """
    return ChatOllama(
        model="deepseek-r1:latest",
        temperature=temperature,
        # 아래 매개변수는 구현체마다 지원 여부가 다를 수 있습니다.
        # 지원하지 않는 경우 자동 무시되거나 에러가 날 수 있어 필요 시 주석 처리하세요.
        num_ctx=8192,
        streaming=False,
    )

def setup_llamaindex_llm(temperature: float = 0.2):
    """
    LlamaIndex 전역 LLM을 Ollama(DeepSeek-R1)로 설정합니다.
    """
    Settings.llm = LlamaIndexOllama(
        model="deepseek-r1:latest",
        request_timeout=60.0,
        temperature=temperature,
    )

# ============================================================
# 1) 스도쿠 퍼즐 풀이(원 코드: OpenAI 에이전트 → Ollama 에이전트)
# ------------------------------------------------------------
# 원본은 LLM이 PythonREPL과 Wikipedia 도구를 사용하게 했습니다.
# 여기서는 동일하게 PythonREPLTool + Wikipedia를 연결합니다.
# 또한, 보조로 파이썬 백트래킹 솔버를 구현하여
# LLM 불확실성을 줄이기 위한 옵션도 제공합니다.
# ============================================================

# 안전하고 확정적인 해를 위해 파이썬 백트래킹 솔버 구현
def solve_sudoku_backtracking(board):
    """
    board: 9x9 2D 리스트(0은 빈 칸)
    반환: 풀이된 9x9 리스트 또는 None
    """
    # 빈 칸 찾기
    def find_empty(b):
        for r in range(9):
            for c in range(9):
                if b[r][c] == 0:
                    return r, c
        return None

    # 유효성 검사
    def valid(b, r, c, val):
        # 행
        if any(b[r][i] == val for i in range(9)):
            return False
        # 열
        if any(b[i][c] == val for i in range(9)):
            return False
        # 3x3 박스
        br, bc = (r // 3) * 3, (c // 3) * 3
        for i in range(br, br + 3):
            for j in range(bc, bc + 3):
                if b[i][j] == val:
                    return False
        return True

    empty = find_empty(board)
    if not empty:
        return board
    r, c = empty
    for val in range(1, 10):
        if valid(board, r, c, val):
            board[r][c] = val
            if solve_sudoku_backtracking(board):
                return board
            board[r][c] = 0
    return None

def sudoku_with_agent():
    """
    LangChain Agent(DeepSeek-R1 + PythonREPLTool + Wikipedia)를 사용하여
    스도쿠 규칙 확인 → 풀이. (원 코드와 유사한 동작 흐름)
    파이썬 솔버 결과도 함께 반환합니다.
    """
    # 도구 준비
    python_repl = PythonREPLTool()
    wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())
    tools = [python_repl, wikipedia]

    # LLM 준비
    llm = make_chat_llm(temperature=0.5)

    # 에이전트 초기화(Zero-shot ReAct)
    agent = initialize_agent(
        tools,
        llm,
        agent="zero-shot-react-description",
        verbose=True,
        handle_parsing_errors=True,
    )

    puzzle = [
        [4, 0, 9, 1, 8, 0, 0, 0, 0],
        [1, 7, 0, 5, 0, 0, 0, 3, 0],
        [0, 3, 0, 0, 0, 0, 0, 0, 8],
        [0, 0, 0, 4, 0, 2, 0, 0, 0],
        [0, 9, 0, 0, 0, 0, 2, 4, 0],
        [2, 0, 3, 0, 0, 0, 0, 0, 5],
        [0, 1, 0, 0, 0, 0, 0, 0, 3],
        [3, 0, 7, 0, 0, 1, 8, 0, 9],
        [0, 0, 0, 0, 9, 7, 1, 6, 0],
    ]

    prompt = f"""
먼저 스도쿠의 규칙(행/열/3x3 박스에 1~9가 중복 없이 들어감)을 간단히 설명하고,
그 규칙에 따라 다음 스도쿠 퍼즐을 풀어주세요.
정답은 9x9 2차원 배열(파이썬 리스트) 그대로만 출력하세요.

퍼즐:
{json.dumps(puzzle, ensure_ascii=False)}
"""

    # LLM 기반 풀이 시도
    try:
        llm_answer = agent.run(prompt)
    except Exception as e:
        llm_answer = f"에이전트 실행 중 오류: {e}"

    # 파이썬 백트래킹 풀이(정확도 보장)
    solved = solve_sudoku_backtracking([row[:] for row in puzzle])
    solved_str = json.dumps(solved, ensure_ascii=False) if solved else "해를 찾지 못했습니다."

    return f"[LLM 에이전트 답변]\n{llm_answer}\n\n[백트래킹 솔버 답변]\n{solved_str}"

# ============================================================
# 2) DuckDuckGo Web 검색 Agent (OpenAI/Google API 대체)
# ------------------------------------------------------------
# 원본은 GoogleSearchAPIWrapper + ChatOpenAI 사용 → 키 요구.
# 본 변환은 DuckDuckGoSearchRun(키 불필요) + ChatOllama 사용.
# ============================================================
def search_agent(query: str):
    if not query.strip():
        return "질문을 입력하세요."
    llm = make_chat_llm(temperature=0.2)
    python_repl = PythonREPLTool()
    ddg = DuckDuckGoSearchRun(name="DuckDuckGo Search")

    tools = [
        python_repl,
        ddg,  # 이미 Tool 형태
    ]

    agent = initialize_agent(
        tools, llm, agent="zero-shot-react-description", verbose=True
    )

    try:
        return agent.run(query)
    except Exception as e:
        return f"에이전트 실행 중 오류: {e}"

# ============================================================
# 3) LlamaIndex + 웹페이지(데카르트) QA
# ------------------------------------------------------------
# 원본은 BeautifulSoupWebReader로 웹 문서를 적재 후 VectorStoreIndex 생성.
# 여기서는 LlamaIndex의 LLM을 Ollama로 설정하여 완전 오픈소스로 동작.
# ============================================================
def descartes_method_qa():
    setup_llamaindex_llm(temperature=0.0)
    try:
        documents = BeautifulSoupWebReader().load_data(
            urls=['https://www.gutenberg.org/cache/epub/59/pg59-images.html']
        )
        index = VectorStoreIndex.from_documents(documents=documents)
        query_engine = index.as_query_engine()
        q = "데카르트의 방법서설에서 과학적 방법에 대해 어떻게 설명하고 있나요?"
        ans = query_engine.query(q)
        return str(ans)
    except Exception as e:
        return f"LlamaIndex 처리 중 오류: {e}"

# ============================================================
# 4) 로컬 PDF(민법/형법) 인덱싱 및 질의
# ------------------------------------------------------------
# - ./civilcode.pdf, ./criminalcode.pdf 파일이 로컬에 존재해야 합니다.
# - 최초 1회 색인 생성 후 ./index_civilcode, ./index_criminalcode 폴더에 저장.
# - 이후 빠르게 로드하여 질의합니다.
# ============================================================
def build_index_from_pdf(pdf_path: str, persist_dir: str):
    setup_llamaindex_llm(temperature=0.0)
    loader = CJKPDFReader()
    documents = loader.load_data(file=pdf_path)
    index = VectorStoreIndex.from_documents(documents)
    index.storage_context.persist(persist_dir)
    return "색인 생성 완료"

from llama_index.core import StorageContext, load_index_from_storage

def load_index(persist_dir: str):
    setup_llamaindex_llm(temperature=0.0)
    storage_context = StorageContext.from_defaults(persist_dir=persist_dir)
    index = load_index_from_storage(storage_context)
    return index

def ensure_indexes():
    """
    민법/형법 색인이 없으면 생성하고, 있으면 로드합니다.
    """
    msgs = []
    # 민법
    if not os.path.exists("./index_civilcode"):
        if os.path.exists("./civilcode.pdf"):
            msgs.append(build_index_from_pdf("./civilcode.pdf", "./index_civilcode"))
        else:
            msgs.append("민법 PDF(civilcode.pdf)가 없습니다. 색인을 건너뜁니다.")
    # 형법
    if not os.path.exists("./index_criminalcode"):
        if os.path.exists("./criminalcode.pdf"):
            msgs.append(build_index_from_pdf("./criminalcode.pdf", "./index_criminalcode"))
        else:
            msgs.append("형법 PDF(criminalcode.pdf)가 없습니다. 색인을 건너뜁니다.")
    return "\n".join(msgs) if msgs else "이미 두 색인이 모두 존재합니다."

def law_qa(which: str, query: str):
    """
    which: '민법' 또는 '형법'
    query: 한국어 질문
    """
    if not query.strip():
        return "질문을 입력하세요."

    setup_llamaindex_llm(temperature=0.0)

    try:
        if which == "민법":
            if not os.path.exists("./index_civilcode"):
                return "민법 색인이 존재하지 않습니다. 먼저 '색인 생성'을 실행하세요."
            idx = load_index("./index_civilcode")
        else:
            if not os.path.exists("./index_criminalcode"):
                return "형법 색인이 존재하지 않습니다. 먼저 '색인 생성'을 실행하세요."
            idx = load_index("./index_criminalcode")

        qe = idx.as_query_engine()
        ans = qe.query(query)
        return str(ans)
    except Exception as e:
        return f"질의 중 오류: {e}"

# ============================================================
# 5) 범용 대화(LLM 단독 or 간단 체인)
# ------------------------------------------------------------
def general_chat(query: str):
    if not query.strip():
        return "질문을 입력하세요."
    llm = make_chat_llm(temperature=0.7)
    # 간단히 LLM에 직접 전달
    try:
        resp = llm.invoke(query)
        # LangChain 메시지 객체일 수 있으므로 문자열로 캐스팅
        return str(resp)
    except Exception as e:
        return f"대화 중 오류: {e}"

# ============================================================
# Gradio 인터페이스
# ------------------------------------------------------------
# 탭 구성:
#  - 스도쿠 풀이(에이전트 & 백트래킹 비교)
#  - 웹 검색 에이전트(DDG)
#  - 데카르트 웹문서 질의(LlamaIndex)
#  - 법률 PDF 색인/질의(LlamaIndex)
#  - 범용 대화
# ============================================================

with gr.Blocks(title="OpenCode · Ollama(DeepSeek-R1) Playground", theme=gr.themes.Soft()) as demo:
    gr.Markdown(
        """
        # OpenCode · Ollama(DeepSeek-R1) Playground
        오픈소스 LLM(DeepSeek-R1, Ollama)을 LangChain/LlamaIndex와 결합한 예제입니다.  
        - **API Key 불필요**, **로컬 추론**  
        - UI: Gradio  
        - 모델 준비: `ollama pull deepseek-r1:latest`
        """
    )

    with gr.Tab("① 스도쿠 풀이 (Agent + PythonREPL + Wikipedia)"):
        gr.Markdown("원본 OpenAI/ChatGPT 코드를 오픈소스 LLM으로 변환한 데모입니다.")
        btn_sudoku = gr.Button("스도쿠 풀기")
        out_sudoku = gr.Markdown()
        btn_sudoku.click(fn=sudoku_with_agent, inputs=None, outputs=out_sudoku)

    with gr.Tab("② 웹 검색 에이전트 (DuckDuckGo)"):
        gr.Markdown("Google API 대신 DuckDuckGo 검색 도구를 사용합니다(키 불필요).")
        q_search = gr.Textbox(label="검색/질의 입력", value="내일 서울의 날씨 예보는?")
        btn_search = gr.Button("질의 실행")
        out_search = gr.Markdown()
        btn_search.click(fn=search_agent, inputs=q_search, outputs=out_search)

    with gr.Tab("③ 데카르트 웹문서 QA (LlamaIndex)"):
        gr.Markdown("Gutenberg의 데카르트 문서를 로드해 질의합니다.")
        btn_descartes = gr.Button("질의 실행")
        out_descartes = gr.Markdown()
        btn_descartes.click(fn=descartes_method_qa, inputs=None, outputs=out_descartes)

    with gr.Tab("④ 법률 PDF 색인/질의 (민법·형법)"):
        gr.Markdown(
            """
            - 로컬에 `./civilcode.pdf`, `./criminalcode.pdf`가 있을 경우 색인을 생성/로드합니다.  
            - 최초 1회 색인 생성 후, 이후에는 빠르게 질의할 수 있습니다.
            """
        )
        with gr.Row():
            btn_build = gr.Button("색인 생성/확인")
            out_build = gr.Markdown()
        btn_build.click(fn=ensure_indexes, inputs=None, outputs=out_build)

        with gr.Row():
            which_law = gr.Radio(choices=["민법", "형법"], value="민법", label="법률 선택")
            q_law = gr.Textbox(label="질문", value="상속의 승인과 포기에 대해 한국어로 알려주세요.")
        btn_law = gr.Button("질의 실행")
        out_law = gr.Markdown()
        btn_law.click(fn=law_qa, inputs=[which_law, q_law], outputs=out_law)

    with gr.Tab("⑤ 범용 대화"):
        gr.Markdown("DeepSeek-R1과의 일반 대화")
        q_general = gr.Textbox(label="메시지", value="안녕하세요")
        btn_general = gr.Button("전송")
        out_general = gr.Markdown()
        btn_general.click(fn=general_chat, inputs=q_general, outputs=out_general)

    gr.Markdown(
        """
        ---  
        **주의:**  
        - Ollama가 로컬에서 실행 중이어야 합니다. (기본: `localhost:11434`)  
        - 일부 대화형 추론(DeepSeek-R1)은 '생각' 토큰을 내부적으로 생성하므로, 결과 형식이 약간 장황할 수 있습니다.  
        - 네트워크 리소스(웹, Gutenberg) 접근 시 시스템/네트워크 환경에 따라 시간이 소요될 수 있습니다.
        """
    )

if __name__ == "__main__":
    # 공유가 필요 없다면 share=False
    demo.launch(server_name="0.0.0.0", server_port=7860, share=False)