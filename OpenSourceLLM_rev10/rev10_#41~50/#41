#41
"""
ë¡œì»¬ LLM (Ollama + DeepSeek-R1) ìŠ¤íŠ¸ë¦¬ë° ì±—ë´‡ - Gradio UI
- ê¸°ì¡´ OpenAI chat.completions(stream=True) ì½˜ì†” REPLì„
  ë¡œì»¬ Ollama HTTP API ê¸°ë°˜ ìŠ¤íŠ¸ë¦¬ë° + Gradio ì›¹ UIë¡œ ë³€í™˜
- ì˜ì¡´ì„±: pip install gradio requests
- ì‚¬ì „ ì¤€ë¹„:
  1) Ollama ì„¤ì¹˜ ë° ì‹¤í–‰ (ê¸°ë³¸ í¬íŠ¸ 11434)
  2) deepseek-r1 ëª¨ë¸ ì„¤ì¹˜:  `ollama pull deepseek-r1`
ì‹¤í–‰:
  python app.py
"""

import json
import requests
import gradio as gr
from typing import List, Dict, Generator, Optional

# Ollama ì„œë²„ ê¸°ë³¸ ì—”ë“œí¬ì¸íŠ¸
OLLAMA_URL = "http://localhost:11434/api/chat"

# ê¸°ë³¸ ëª¨ë¸ëª… (ì›í•˜ë©´ UIì—ì„œ ë³€ê²½ ê°€ëŠ¥)
DEFAULT_MODEL = "deepseek-r1"

# ---- ìœ í‹¸ í•¨ìˆ˜ë“¤ ----

def build_messages(system_prompt: str, history: List[Dict[str, str]], user_text: str) -> List[Dict[str, str]]:
    """
    Ollama /api/chat í˜•ì‹ì˜ messages ë°°ì—´ ìƒì„±
    - role: 'system' | 'user' | 'assistant'
    - content: ë¬¸ìì—´
    """
    messages = []
    if system_prompt.strip():
        messages.append({"role": "system", "content": system_prompt.strip()})

    for turn in history:
        # historyëŠ” {"role": "user"/"assistant", "content": "..."} í˜•íƒœë¡œ ê´€ë¦¬
        messages.append({"role": turn["role"], "content": turn["content"]})

    messages.append({"role": "user", "content": user_text})
    return messages


def ollama_chat_stream(
    messages: List[Dict[str, str]],
    model: str = DEFAULT_MODEL,
    temperature: float = 0.2,
    top_p: float = 0.95,
    max_tokens: Optional[int] = None,
) -> Generator[str, None, None]:
    """
    Ollama /api/chat ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ (Server-Sent JSON lines)
    - stream=True ì¼ ë•Œ, ê° ë¼ì¸ì€ JSONì´ë©° {"message": {"content": "..."}, "done": bool} ë“±ì˜ í•„ë“œë¥¼ í¬í•¨
    - í† í° ë‹¨ìœ„ë¡œ contentë¥¼ ì´ì–´ë¶™ì—¬ yield
    """
    payload = {
        "model": model,
        "stream": True,
        "messages": messages,
        "options": {
            "temperature": temperature,
            "top_p": top_p,
        },
    }
    if max_tokens is not None:
        payload["options"]["num_predict"] = max_tokens  # OllamaëŠ” max_tokens ëŒ€ì‹  num_predict ì‚¬ìš©

    with requests.post(OLLAMA_URL, json=payload, stream=True) as resp:
        resp.raise_for_status()
        for line in resp.iter_lines(decode_unicode=True):
            if not line:
                continue
            try:
                data = json.loads(line)
            except json.JSONDecodeError:
                # í˜¹ì‹œ ì„ì—¬ ë“¤ì–´ì˜¨ ë¼ì¸ì´ JSONì´ ì•„ë‹ˆë©´ ë¬´ì‹œ
                continue

            # doneì´ë©´ ì¢…ë£Œ
            if data.get("done"):
                break

            # ì¦ë¶„ í† í°(content ì¡°ê°) ì¶”ì¶œ
            delta = (
                data.get("message", {}).get("content", "")
                or data.get("delta", "")  # ì¼ë¶€ ë²„ì „ í˜¸í™˜
            )
            if delta:
                yield delta


# ---- Gradio í•¸ë“¤ëŸ¬ ----

def gradio_chat_stream(
    user_text: str,
    chat_history_ui: List[List[str]],
    system_prompt: str,
    model: str,
    temperature: float,
    top_p: float,
    max_tokens: Optional[int],
    internal_history: List[Dict[str, str]],
):
    """
    Gradio ìŠ¤íŠ¸ë¦¬ë° í•¸ë“¤ëŸ¬
    - chat_history_ui: [[user, assistant], ...] í˜•íƒœ (Gradio Chatbot í‘œì‹œìš©)
    - internal_history: [{"role": "user"/"assistant", "content": "..."}] ë¬¸ë§¥ ìœ ì§€ìš©
    - yieldë¥¼ ì‚¬ìš©í•´ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ Chatbot ê°±ì‹ 
    """
    # Gradio UIìš© íˆìŠ¤í† ë¦¬ì— ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    chat_history_ui = chat_history_ui or []
    chat_history_ui.append([user_text, ""])  # ë¹ˆ ë´‡ ì‘ë‹µì„ ì±„ì›Œê°€ë©° ìŠ¤íŠ¸ë¦¬ë°

    # ë‚´ë¶€ ëŒ€í™” ì´ë ¥ êµ¬ì„±ìš© ë©”ì‹œì§€ ìƒì„±
    messages = build_messages(system_prompt, internal_history or [], user_text)

    # ëˆ„ì  ì¶œë ¥ ë²„í¼
    acc = ""

    try:
        for delta in ollama_chat_stream(
            messages=messages,
            model=model or DEFAULT_MODEL,
            temperature=temperature,
            top_p=top_p,
            max_tokens=max_tokens if max_tokens and max_tokens > 0 else None,
        ):
            acc += delta
            # ìµœì‹  í„´ì˜ assistant ë©”ì‹œì§€ ì—…ë°ì´íŠ¸
            chat_history_ui[-1][1] = acc
            # UI ì—…ë°ì´íŠ¸ + ë‚´ë¶€ ì´ë ¥ì€ ì•„ì§ ëˆ„ì í•˜ì§€ ì•ŠìŒ
            yield chat_history_ui, internal_history
    except requests.RequestException as e:
        err = f"[ì—°ê²° ì˜¤ë¥˜] Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}"
        chat_history_ui[-1][1] = err
        yield chat_history_ui, internal_history
        return

    # ìŠ¤íŠ¸ë¦¬ë° ì¢…ë£Œ í›„ ë‚´ë¶€ ì´ë ¥ì— ìµœì¢… í„´ ë°˜ì˜
    internal_history = internal_history or []
    internal_history.append({"role": "user", "content": user_text})
    internal_history.append({"role": "assistant", "content": acc})

    # ìµœì¢… ê°±ì‹  ìƒíƒœ ì „ë‹¬
    yield chat_history_ui, internal_history


def clear_history():
    """ëŒ€í™” ì´ë ¥ ì´ˆê¸°í™” (UIì™€ ë‚´ë¶€ ì´ë ¥ ë‘˜ ë‹¤)"""
    return [], []


# ---- Gradio UI êµ¬ì„± ----

with gr.Blocks(title="DeepSeek-R1 ìŠ¤íŠ¸ë¦¬ë° ì±—ë´‡ (Ollama)") as demo:
    gr.Markdown("## ğŸ’¬ DeepSeek-R1 ìŠ¤íŠ¸ë¦¬ë° ì±—ë´‡ (Ollama ê¸°ë°˜)")
    gr.Markdown(
        "- ì¢Œì¸¡ ì„¤ì •ì—ì„œ **ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸/ëª¨ë¸/ì˜¨ë„/í† í° ìˆ˜**ë¥¼ ì¡°ì ˆí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
        "- ëŒ€í™”ëŠ” **ë¡œì»¬ì—ì„œë§Œ** ì²˜ë¦¬ë˜ë©°, API í‚¤ê°€ í•„ìš” ì—†ìŠµë‹ˆë‹¤.\n"
        "- ê¸°ì¡´ ì½˜ì†” REPLì˜ `quit` ëŒ€ì‹ , ìƒë‹¨ì˜ **ëŒ€í™” ì´ˆê¸°í™”** ë²„íŠ¼ì„ ì‚¬ìš©í•˜ì„¸ìš”."
    )

    with gr.Row():
        with gr.Column(scale=1):
            with gr.Accordion("âš™ï¸ ì„¤ì •", open=True):
                system_prompt = gr.Textbox(
                    label="ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸",
                    value="ì‚¬ìš©ìì˜ ìš”êµ¬ì— ë”°ë¼ ì ì ˆí•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.",
                    lines=4,
                    placeholder="ì–´ì‹œìŠ¤í„´íŠ¸ì˜ ë§íˆ¬/ì—­í• /ì •ì±… ë“±ì„ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                )
                model = gr.Textbox(
                    label="ëª¨ë¸ëª…",
                    value=DEFAULT_MODEL,
                    placeholder="ì˜ˆ: deepseek-r1, qwen2.5, llama3 ë“± (ollamaì— ì„¤ì¹˜ëœ ëª¨ë¸)",
                )
                with gr.Row():
                    temperature = gr.Slider(0.0, 1.5, value=0.2, step=0.05, label="temperature")
                    top_p = gr.Slider(0.1, 1.0, value=0.95, step=0.05, label="top_p")
                max_tokens = gr.Number(
                    label="num_predict (ìµœëŒ€ í† í° ìˆ˜, ë¹ˆ ê°’ì´ë©´ ì œí•œ ì—†ìŒ)",
                    value=None,
                    precision=0
                )
                clear_btn = gr.Button("ğŸ§¹ ëŒ€í™” ì´ˆê¸°í™”", variant="secondary")

        with gr.Column(scale=2):
            chatbot = gr.Chatbot(height=480, type="messages", avatar_images=(None, None))
            user_input = gr.Textbox(
                label="ë©”ì‹œì§€ ì…ë ¥",
                placeholder="ì˜ˆ: Pythonì„ ì´ìš©í•´ ì¬ê·€ì ìœ¼ë¡œ í”¼ë³´ë‚˜ì¹˜ ìˆ˜ì—´ì„ êµ¬í•˜ëŠ” ì˜ˆì œ ì½”ë“œì™€ í…ŒìŠ¤íŠ¸ ì½”ë“œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.",
                lines=2,
            )
            send_btn = gr.Button("ì „ì†¡ ğŸš€", variant="primary")

    # ë‚´ë¶€ ìƒíƒœ(ë¬¸ë§¥ ìœ ì§€ìš©): [{"role": "...", "content": "..."}] ë¦¬ìŠ¤íŠ¸
    internal_history = gr.State([])

    # ì´ë²¤íŠ¸ ì—°ê²°: ì „ì†¡ ë²„íŠ¼/Enter ì…ë ¥ ì‹œ ìŠ¤íŠ¸ë¦¬ë°
    send_event = send_btn.click(
        fn=gradio_chat_stream,
        inputs=[user_input, chatbot, system_prompt, model, temperature, top_p, max_tokens, internal_history],
        outputs=[chatbot, internal_history],
        queue=True,
        api_name="chat",
    )
    user_input.submit(
        fn=gradio_chat_stream,
        inputs=[user_input, chatbot, system_prompt, model, temperature, top_p, max_tokens, internal_history],
        outputs=[chatbot, internal_history],
        queue=True,
    )

    # ì „ì†¡ í›„ ì…ë ¥ì°½ ë¹„ìš°ê¸°
    send_event.then(lambda: "", None, user_input)

    # ëŒ€í™” ì´ˆê¸°í™”
    clear_btn.click(fn=clear_history, outputs=[chatbot, internal_history])

if __name__ == "__main__":
    demo.launch()



