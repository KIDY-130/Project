#65
"""
ëª©í‘œ: OpenAI ê¸°ë°˜ ì›¹ ë‰´ìŠ¤ RAG ì½”ë“œë¥¼ ì˜¤í”ˆì†ŒìŠ¤ LLM(DeepSeek-R1) + Ollama + ë¡œì»¬ ì„ë² ë”© + Gradio UIë¡œ ë³€í™˜

í•µì‹¬ ì „í™˜
- ChatOpenAI â†’ ChatOllama(model="deepseek-r1")
- OpenAIEmbeddings â†’ OllamaEmbeddings(model="nomic-embed-text")
- API Key ì œê±°(ë¡œì»¬ LLM/ì„ë² ë”© ì‚¬ìš©)
- WebBaseLoader + bs4.SoupStrainerë¡œ ë„¤ì´ë²„ ê²½ì œ(101) ì„¹ì…˜ ìˆ˜ì§‘
- Text Splitter: RecursiveCharacterTextSplitter (í† í° ì¸ì½”ë” ì˜ì¡´ ì œê±°)
- Retriever: Chroma(MMR) + BM25 + EnsembleRetriever
- MultiQueryRetriever(ì§ˆë¬¸ í™•ì¥) + RAG-Fusion(RRF)ë¡œ ê°•ê±´í•œ ê²€ìƒ‰
- Gradio UI ì œê³µ

ì‚¬ì „ ì¤€ë¹„(í„°ë¯¸ë„)
  ollama pull deepseek-r1
  ollama pull nomic-embed-text

í•„ìš” íŒ¨í‚¤ì§€(í„°ë¯¸ë„)
  pip install -U langchain langchain-community chromadb bs4 gradio

ì£¼ì˜
- ë„¤ì´ë²„ í˜ì´ì§€ êµ¬ì¡°ê°€ ë³€ê²½ë˜ë©´ SoupStrainer ì„ íƒìë¥¼ ì¡°ì •í•´ì•¼ í•©ë‹ˆë‹¤.
- ìµœì´ˆ ì¸ë±ì‹±ì‹œ ë„¤íŠ¸ì›Œí¬ ìƒíƒœ/ë‰´ìŠ¤ëŸ‰ì— ë”°ë¼ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""
from __future__ import annotations
import os
import time
from typing import List, Dict, Any

# ---------------------- Loader / Parsing ----------------------
import bs4
from langchain_community.document_loaders import WebBaseLoader

# ---------------------- Split / VectorStore ----------------------
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import OllamaEmbeddings

# ---------------------- LLM / Retrievers / Chains ----------------------
from langchain_community.chat_models import ChatOllama
from langchain.retrievers import EnsembleRetriever
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# ---------------------- UI ----------------------
import gradio as gr

# ===================== ì„¤ì • =====================
NAVER_ECON_URL = "https://news.naver.com/section/101"  # ê²½ì œ ì„¹ì…˜
CHROMA_DIR = "./chroma_news_db"
LLM_MODEL = "deepseek-r1"
EMBED_MODEL = "nomic-embed-text"

DEFAULT_K = 1
DEFAULT_FETCH_K = 4

# ì „ì—­
vectorstore = None
bm25 = None
ensemble = None
retriever_mmr = None
last_index_time = None

# ===================== ì¸ë±ì‹± íŒŒì´í”„ë¼ì¸ =====================

def load_news_documents():
    """Naver ê²½ì œ ì„¹ì…˜ ë¬¸ì„œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    loader = WebBaseLoader(
        web_paths=(NAVER_ECON_URL,),
        bs_kwargs=dict(
            parse_only=bs4.SoupStrainer(
                class_=("sa_text", "sa_item_SECTION_HEADLINE")
            )
        ),
    )
    return loader.load()


def split_documents(docs: List[Any], chunk_size: int = 300, chunk_overlap: int = 50):
    """ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤(í† í° ì¸ì½”ë” ë¹„ì˜ì¡´)."""
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separators=["\n\n", "\n", "ã€‚", "ï¼", "!", "?", ".", " "]
    )
    return splitter.split_documents(docs)


def build_vectorstore(splits: List[Any]):
    """Chroma ë²¡í„°ìŠ¤í† ì–´ë¥¼ êµ¬ì„±/ì˜ì†í™”í•©ë‹ˆë‹¤."""
    embeddings = OllamaEmbeddings(model=EMBED_MODEL)
    vs = Chroma.from_documents(documents=splits, embedding=embeddings, persist_directory=CHROMA_DIR)
    vs.persist()
    return vs


def build_retrievers(splits: List[Any], vs: Chroma, k: int = DEFAULT_K, fetch_k: int = DEFAULT_FETCH_K):
    """MMR Chroma + BM25 + Ensemble êµ¬ì„±."""
    chroma_ret = vs.as_retriever(search_type="mmr", search_kwargs={"k": k, "fetch_k": fetch_k})
    bm25_ret = BM25Retriever.from_documents(splits)
    bm25_ret.k = max(2, k)
    ens = EnsembleRetriever(retrievers=[bm25_ret, chroma_ret], weights=[0.2, 0.8])
    return chroma_ret, bm25_ret, ens


def build_index(k: int = DEFAULT_K, fetch_k: int = DEFAULT_FETCH_K, chunk_size: int = 300, chunk_overlap: int = 50):
    """ë‰´ìŠ¤ ìˆ˜ì§‘ â†’ ë¶„í•  â†’ ì„ë² ë”©/Chroma â†’ ë¦¬íŠ¸ë¦¬ë²„ êµ¬ì„±."""
    global vectorstore, bm25, ensemble, retriever_mmr, last_index_time
    docs = load_news_documents()
    splits = split_documents(docs, chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    vectorstore = build_vectorstore(splits)
    retriever_mmr, bm25, ensemble = build_retrievers(splits, vectorstore, k=k, fetch_k=fetch_k)
    last_index_time = time.strftime("%Y-%m-%d %H:%M:%S")
    return len(docs), len(splits)


# ===================== Multi-Query & RRF =====================

def get_multiquery_retriever(base_retriever):
    """DeepSeek-R1ë¡œ ë‹¤ê°ë„ ì¿¼ë¦¬ë¥¼ ìƒì„±í•´ base_retriever(ì—¬ê¸°ì„œëŠ” Ensemble)ì— ì ìš©."""
    llm = ChatOllama(model=LLM_MODEL, temperature=0.2, num_ctx=4096)
    prompt = ChatPromptTemplate.from_template(
        """
        ì•„ë˜ ì›ë³¸ ì§ˆë¬¸ì„ ì˜ë¯¸ê°€ ë‹¤ë¥¸ 5ê°œ í•œêµ­ì–´ ê²€ìƒ‰ ì§ˆë¬¸ìœ¼ë¡œ ë³€í™˜í•˜ì„¸ìš”.
        - ê° ì¤„ì— ê²€ìƒ‰ ì§ˆë¬¸ 1ê°œë§Œ ì¶œë ¥
        - ë¶ˆí•„ìš”í•œ ì„¤ëª… ê¸ˆì§€
        ì›ë³¸ ì§ˆë¬¸: {question}
        """
    )
    mq = MultiQueryRetriever.from_llm(retriever=base_retriever, llm=llm, prompt=prompt)
    return mq


def reciprocal_rank_fusion(results: List[List[Any]], k: int = 60, top_n: int = 3, return_docs_only: bool = True):
    """RRF(Reciprocal Rank Fusion)ë¡œ ì—¬ëŸ¬ ê²°ê³¼ ì…‹ì„ ìœµí•©í•©ë‹ˆë‹¤."""
    from langchain.load import dumps, loads
    scores: Dict[str, float] = {}
    for docs in results:
        for rank, doc in enumerate(docs):
            key = dumps(doc)
            scores[key] = scores.get(key, 0.0) + 1.0 / (rank + k)
    ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:top_n]
    fused = [loads(k) for k, _ in ranked]
    return fused if return_docs_only else ranked


# ===================== RAG ì²´ì¸ =====================

def build_rag_chain(use_multiquery: bool = True, use_rrf: bool = True, rrf_top_n: int = 3):
    if ensemble is None:
        raise RuntimeError("ì¸ë±ìŠ¤ê°€ ì•„ì§ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € 'ì¸ë±ìŠ¤ ê°±ì‹ 'ì„ ì‹¤í–‰í•˜ì„¸ìš”.")

    llm = ChatOllama(model=LLM_MODEL, temperature=0.2, num_ctx=4096)

    prompt = ChatPromptTemplate.from_template(
        """
        ì•„ë˜ëŠ” ë‰´ìŠ¤ ê¸°ì‚¬ì—ì„œ ì¶”ì¶œí•œ ë§¥ë½ì…ë‹ˆë‹¤. í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ ë‹µí•˜ì„¸ìš”.
        í•„ìš” ì‹œ í•µì‹¬ ê·¼ê±°(ê¸°ì‚¬ ìš”ì•½)ì™€ ì¶œì²˜ URLì„ í•¨ê»˜ ì œì‹œí•˜ì„¸ìš”.

        [ë§¥ë½]
        {context}

        [ì§ˆë¬¸]
        {question}
        """
    )

    def format_docs(docs: List[Any]) -> str:
        blocks = []
        for d in docs:
            src = d.metadata.get("source", "")
            snippet = d.page_content.strip().replace("\n", " ")[:400]
            blocks.append(f"- ì¶œì²˜: {src}\n  ë‚´ìš©: {snippet}")
        return "\n\n".join(blocks)

    base_retriever = ensemble

    if use_multiquery and use_rrf:
        # MultiQuery â†’ Ensemble.map â†’ RRF ìœµí•©
        mq = get_multiquery_retriever(base_retriever)
        retrieval = mq.map() | (lambda results: reciprocal_rank_fusion(results, top_n=rrf_top_n))
    elif use_multiquery:
        mq = get_multiquery_retriever(base_retriever)
        retrieval = mq
    else:
        retrieval = base_retriever

    chain = (
        {"context": retrieval | format_docs, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    return chain


# ===================== Gradio UI =====================

def ui_build_index(k: int, fetch_k: int, chunk_size: int, chunk_overlap: int):
    n_docs, n_splits = build_index(k=int(k), fetch_k=int(fetch_k), chunk_size=int(chunk_size), chunk_overlap=int(chunk_overlap))
    return f"ì¸ë±ìŠ¤ ê°±ì‹  ì™„ë£Œ: {last_index_time} / ë¬¸ì„œ {n_docs}ê°œ / ì²­í¬ {n_splits}ê°œ\nMMR(k={k}, fetch_k={fetch_k})"


def ui_query(question: str, use_mq: bool, use_rrf: bool, rrf_top_n: int):
    if not question or not question.strip():
        return "ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.", []
    chain = build_rag_chain(use_multiquery=bool(use_mq), use_rrf=bool(use_rrf), rrf_top_n=int(rrf_top_n))
    answer = chain.invoke(question.strip())

    # ì°¸ê³  ë¬¸ì„œ í”„ë¦¬ë·° (í˜„ì¬ ensemble ê¸°ì¤€ ìƒìœ„ 3ê°œ)
    docs_preview = ensemble.get_relevant_documents(question)[:3]
    rows = []
    for d in docs_preview:
        rows.append({
            "source": d.metadata.get("source", ""),
            "snippet": d.page_content.strip().replace("\n", " ")[:200],
        })
    return answer, rows


with gr.Blocks(theme=gr.themes.Soft(primary_hue=gr.themes.colors.indigo)) as demo:
    gr.Markdown("""
    # ğŸ—ï¸ Naver ê²½ì œ ë‰´ìŠ¤ RAG (Ollama + DeepSeek-R1)
    - ì˜¤í”ˆì†ŒìŠ¤ LLM/ì„ë² ë”©ìœ¼ë¡œ ë™ì‘í•©ë‹ˆë‹¤.
    - **ì¸ë±ìŠ¤ ê°±ì‹ ** í›„ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.
    """)

    with gr.Accordion("ì¸ë±ì‹± ì„¤ì •", open=True):
        with gr.Row():
            k = gr.Slider(1, 5, value=DEFAULT_K, step=1, label="MMR k (ë°˜í™˜ ë¬¸ì„œ ìˆ˜)")
            fetch_k = gr.Slider(1, 12, value=DEFAULT_FETCH_K, step=1, label="MMR fetch_k (ê³ ë ¤ ë¬¸ì„œ ìˆ˜)")
        with gr.Row():
            cs = gr.Slider(200, 1200, value=300, step=50, label="chunk_size")
            co = gr.Slider(0, 300, value=50, step=10, label="chunk_overlap")
        idx_btn = gr.Button("ì¸ë±ìŠ¤ ê°±ì‹ ", variant="primary")
    idx_status = gr.Markdown("ì¸ë±ìŠ¤ë¥¼ ë¨¼ì € ê°±ì‹ í•˜ì„¸ìš”.")

    with gr.Row():
        q = gr.Textbox(label="ì§ˆë¬¸", placeholder="ì˜ˆ) í–¥í›„ ì§‘ê°’ì— ëŒ€í•´ì„œ ì•Œë ¤ì¤˜", lines=2)
    with gr.Row():
        use_mq = gr.Checkbox(value=True, label="Multi-Query ì‚¬ìš©(ì§ˆë¬¸ í™•ì¥)")
        use_rrf = gr.Checkbox(value=True, label="RAG-Fusion(RRF) ìœµí•©")
        rrf_top_n = gr.Slider(1, 5, value=3, step=1, label="RRF ìµœì¢… ë¬¸ì„œ ìˆ˜")
        ask_btn = gr.Button("ì§ˆì˜", variant="primary")

    ans = gr.Markdown(label="ë‹µë³€")
    table = gr.Dataframe(headers=["source", "snippet"], datatype=["str", "str"], wrap=True, label="ì°¸ê³  ë¬¸ì„œ (ìƒìœ„ 3ê°œ)")

    idx_btn.click(fn=ui_build_index, inputs=[k, fetch_k, cs, co], outputs=idx_status)
    ask_btn.click(fn=ui_query, inputs=[q, use_mq, use_rrf, rrf_top_n], outputs=[ans, table])


if __name__ == "__main__":
    print("[ì•ˆë‚´] Ollama ëª¨ë¸ ì¤€ë¹„ ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”:")
    print("  - LLM: deepseek-r1 (ollama pull deepseek-r1)")
    print("  - Embedding: nomic-embed-text (ollama pull nomic-embed-text)\n")
    demo.launch(server_name="0.0.0.0", server_port=7862)

# ollama pull deepseek-r1
# ollama pull nomic-embed-text
# !pip install -U langchain langchain-community chromadb bs4 gradio

