{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8f4332",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# =============================\n",
    "# 🚀 DeepSeek-R1 AI Toolkit (빠른 실행 최적화 버전) — Gradio queue 수정\n",
    "# =============================\n",
    "\n",
    "# ---------- 환경/설치 최소화 & 캐싱 ----------\n",
    "print(\"📦 환경 점검 중...\")\n",
    "\n",
    "import os, shutil, subprocess, time, re, json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "def sh(cmd: str):\n",
    "    return subprocess.run(cmd, shell=True, check=False, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "\n",
    "# Java (tabula/camelot 필요) — 없을 때만 설치\n",
    "if shutil.which(\"java\") is None:\n",
    "    print(\"👉 Java 설치\")\n",
    "    sh(\"apt-get update -qq && apt-get install -y openjdk-11-jdk-headless\")\n",
    "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-11-openjdk-amd64'\n",
    "\n",
    "# pip 패키지 — 필요한 것만 & 없을 때만\n",
    "def ensure(mod: str, pip_name: str = None):\n",
    "    pip_name = pip_name or mod\n",
    "    try:\n",
    "        __import__(mod)\n",
    "    except Exception:\n",
    "        print(f\"📥 install {pip_name}\")\n",
    "        sh(f\"pip install -q {pip_name}\")\n",
    "\n",
    "print(\"🐍 필수 패키지 확인\")\n",
    "for mod, pipn in [\n",
    "    (\"gradio\",\"gradio\"),\n",
    "    (\"ollama\",\"ollama\"),\n",
    "    (\"fitz\",\"pymupdf\"),\n",
    "    (\"pandas\",\"pandas\"),\n",
    "    (\"PIL\",\"Pillow\"),\n",
    "    (\"numpy\",\"numpy\"),\n",
    "    (\"langchain_community\",\"langchain-community\"),\n",
    "    (\"langchain_chroma\",\"langchain-chroma\"),\n",
    "    (\"langchain_text_splitters\",\"langchain-text-splitters\"),\n",
    "    (\"beautifulsoup4\",\"beautifulsoup4\"),\n",
    "    (\"duckduckgo_search\",\"duckduckgo-search\"),\n",
    "]:\n",
    "    ensure(mod, pipn)\n",
    "\n",
    "# Ollama 설치\n",
    "print(\"🦙 Ollama 준비\")\n",
    "if shutil.which(\"ollama\") is None:\n",
    "    sh(\"curl -fsSL https://ollama.com/install.sh | sh\")\n",
    "\n",
    "# 서버 백그라운드\n",
    "subprocess.Popen([\"ollama\",\"serve\"])\n",
    "time.sleep(3)\n",
    "\n",
    "# 모델 구성: 빠른 모델 / 무거운 모델 / VLM\n",
    "FAST_MODEL  = \"qwen2.5:7b-instruct\"   # 빠른 생성/도구 탭\n",
    "HEAVY_MODEL = \"deepseek-r1\"           # RAG 등 긴 추론 필요 시\n",
    "VLM_MODEL   = \"qwen2.5-vl\"            # 비전-텍스트\n",
    "EMB_MODEL   = \"nomic-embed-text\"      # 임베딩\n",
    "\n",
    "print(\"📥 필요한 모델 pull\")\n",
    "for m in [FAST_MODEL, HEAVY_MODEL, VLM_MODEL, EMB_MODEL]:\n",
    "    sh(f\"ollama pull {m}\")\n",
    "\n",
    "# 간단 웜업(첫 토큰 지연 감소)\n",
    "print(\"🔥 모델 웜업\")\n",
    "sh(f'''python - <<'PY'\n",
    "import ollama\n",
    "for m in [\"{FAST_MODEL}\", \"{HEAVY_MODEL}\"]:\n",
    "    try:\n",
    "        ollama.chat(model=m, messages=[{{\"role\":\"user\",\"content\":\"hi\"}}], options={{\"num_predict\":8}})\n",
    "    except Exception as e:\n",
    "        print(\"warmup err:\", m, e)\n",
    "PY''')\n",
    "print(\"✅ 환경 준비 완료\")\n",
    "\n",
    "# =============================\n",
    "# 🧠 메인 애플리케이션 코드\n",
    "# =============================\n",
    "from typing import Optional, List, Dict\n",
    "import gradio as gr\n",
    "import ollama\n",
    "import fitz  # PyMuPDF\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader, WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "\n",
    "# ---------- 전역 ----------\n",
    "class GlobalState:\n",
    "    def __init__(self):\n",
    "        self.cache = {}  # (model, temp, opts_key) -> ChatOllama\n",
    "        self.rag_vectorstore: Optional[Chroma] = None\n",
    "        self.rag_embeddings: Optional[OllamaEmbeddings] = None\n",
    "        self.last_rag_sources: List[Dict] = []\n",
    "\n",
    "    def get_llm(self, model_name=FAST_MODEL, temperature=0.7, **model_kwargs) -> ChatOllama:\n",
    "        # 주요 속도옵션: num_predict(응답길이), num_ctx\n",
    "        if \"num_predict\" not in model_kwargs:\n",
    "            model_kwargs[\"num_predict\"] = 256\n",
    "        opts_key = tuple(sorted(model_kwargs.items()))\n",
    "        key = (model_name, float(temperature), opts_key)\n",
    "        if key not in self.cache:\n",
    "            self.cache[key] = ChatOllama(\n",
    "                model=model_name,\n",
    "                temperature=temperature,\n",
    "                model_kwargs=model_kwargs\n",
    "            )\n",
    "        return self.cache[key]\n",
    "\n",
    "STATE = GlobalState()\n",
    "DEFAULT_PERSIST_DIR = \"./chroma_db_store\"\n",
    "os.makedirs(DEFAULT_PERSIST_DIR, exist_ok=True)\n",
    "\n",
    "# ---------- 헬퍼 ----------\n",
    "def log(message: str) -> str:\n",
    "    ts = datetime.now().strftime('%H:%M:%S')\n",
    "    return f\"[{ts}] {message}\\n\"\n",
    "\n",
    "def strip_think_tags(text: str) -> str:\n",
    "    return re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL|re.IGNORECASE).strip()\n",
    "\n",
    "def safe_json_extract(text: str):\n",
    "    \"\"\"빠르고 안전한 JSON 추출\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return None\n",
    "    m = re.search(r\"```json\\s*(\\{[\\s\\S]*?\\}|\\[[\\s\\S]*?\\])\\s*```\", text, flags=re.I)\n",
    "    if m:\n",
    "        try: return json.loads(m.group(1))\n",
    "        except: pass\n",
    "    text_wo = re.sub(r\"```(?!json)[\\s\\S]*?```\", \"\", text, flags=re.I)\n",
    "    cands = []\n",
    "    for p in (r\"\\{[\\s\\S]*?\\}\", r\"\\[[\\s\\S]*?\\]\"):\n",
    "        for mm in re.finditer(p, text_wo):\n",
    "            cands.append(mm.group(0))\n",
    "    for chunk in sorted(cands, key=len, reverse=True):\n",
    "        try: return json.loads(chunk)\n",
    "        except: pass\n",
    "    try: return json.loads(text_wo.strip())\n",
    "    except: return None\n",
    "\n",
    "def call_vlm(image_paths: list, prompt: str, vlm_model: str) -> str:\n",
    "    try:\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt, \"images\": image_paths}]\n",
    "        res = ollama.chat(model=vlm_model, messages=messages, options={\"num_predict\":256})\n",
    "        return res.get(\"message\", {}).get(\"content\", \"VLM 호출 실패\")\n",
    "    except Exception as e:\n",
    "        return f\"VLM 오류: {e}\"\n",
    "\n",
    "# =============================\n",
    "# 1) 💬 챗봇\n",
    "# =============================\n",
    "PERSONAS = {\n",
    "    \"기본 상담사\": \"너는 사용자를 도와주는 친절하고 유능한 상담사야.\",\n",
    "    \"백설공주 마법거울\": \"너는 백설공주 이야기 속 마법 거울이야. 품위 있고 운율감 있게 답해줘.\",\n",
    "    \"유치원생\": \"너는 5살 유치원생이야. 짧고 쉬운 말로 귀엽게 대답해.\",\n",
    "}\n",
    "\n",
    "def handle_chatbot_stream(message: str, history: list, persona: str, temperature: float):\n",
    "    if not message.strip(): return\n",
    "    llm = STATE.get_llm(model_name=FAST_MODEL, temperature=temperature, num_predict=256)\n",
    "    system_prompt = PERSONAS.get(persona, PERSONAS[\"기본 상담사\"])\n",
    "    msgs = [SystemMessage(content=system_prompt)]\n",
    "    for t in history:\n",
    "        msgs.append(HumanMessage(content=t[\"content\"]) if t[\"role\"]==\"user\" else AIMessage(content=t[\"content\"]))\n",
    "    msgs.append(HumanMessage(content=message))\n",
    "    stream = llm.stream(msgs)\n",
    "    history.append({\"role\":\"user\",\"content\":message})\n",
    "    history.append({\"role\":\"assistant\",\"content\":\"\"})\n",
    "    for ch in stream:\n",
    "        history[-1][\"content\"] += ch.content\n",
    "        yield history\n",
    "\n",
    "# =============================\n",
    "# 2) 📄 PDF & 문서\n",
    "# =============================\n",
    "def handle_pdf_summary(pdf_file, chunk_size: int, temperature: float, progress=gr.Progress()):\n",
    "    if not pdf_file: return \"PDF 파일을 업로드하세요.\", \"\"\n",
    "    llm = STATE.get_llm(model_name=FAST_MODEL, temperature=temperature, num_predict=256)\n",
    "    progress(0, desc=\"PDF에서 텍스트 추출 중...\")\n",
    "    doc = fitz.open(pdf_file.name)\n",
    "    full_text = \"\\n\".join([p.get_text() for p in doc]); doc.close()\n",
    "    if not full_text.strip(): return \"PDF에서 텍스트를 추출할 수 없습니다.\", full_text\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=int(chunk_size*0.1))\n",
    "    chunks = splitter.split_text(full_text)\n",
    "    system_prompt = \"다음 텍스트는 문서의 일부입니다. 핵심을 정확·간결히 요약하세요.\"\n",
    "    summaries = []\n",
    "    for c in progress.tqdm(chunks, desc=\"각 청크 요약 중...\"):\n",
    "        s = llm.invoke([SystemMessage(content=system_prompt), HumanMessage(content=c)]).content\n",
    "        summaries.append(s)\n",
    "    if len(summaries)==1:\n",
    "        final = summaries[0]\n",
    "    else:\n",
    "        progress(0.85, desc=\"부분 요약 통합 중...\")\n",
    "        final = llm.invoke([\n",
    "            SystemMessage(content=\"부분 요약들을 종합해 전체 요약 10~15줄.\"),\n",
    "            HumanMessage(content=\"\\n\\n---\\n\\n\".join(summaries))\n",
    "        ]).content\n",
    "    return final, full_text\n",
    "\n",
    "def handle_pdf_extraction(pdf_file, header_px: int, footer_px: int):\n",
    "    if not pdf_file: return \"PDF 파일을 업로드하세요.\", [], None\n",
    "    out = Path(\"outputs/pdf_extraction\"); out.mkdir(parents=True, exist_ok=True)\n",
    "    doc = fitz.open(pdf_file.name)\n",
    "    full_text, images = \"\", []\n",
    "    for i, page in enumerate(doc):\n",
    "        rect = page.rect\n",
    "        text = page.get_text(\"text\", clip=(0, header_px, rect.width, rect.height-footer_px))\n",
    "        full_text += f\"--- Page {i+1} ---\\n{text}\\n\\n\"\n",
    "        for idx, img in enumerate(page.get_images(full=True)):\n",
    "            xref = img[0]; base = doc.extract_image(xref)\n",
    "            path = out / f\"page{i+1}_img{idx+1}.{base['ext']}\"\n",
    "            with open(path,\"wb\") as f: f.write(base[\"image\"])\n",
    "            images.append(str(path))\n",
    "    doc.close()\n",
    "    return full_text, images, str(out)\n",
    "\n",
    "def handle_pdf_table_extraction(pdf_file):\n",
    "    if not pdf_file: return None, \"PDF 파일을 업로드하세요.\", None\n",
    "    # 지연 임포트\n",
    "    try:\n",
    "        import camelot; import tabula\n",
    "    except Exception:\n",
    "        return None, \"테이블 추출 라이브러리가 설치되지 않았습니다.\", None\n",
    "    os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-11-openjdk-amd64'\n",
    "    out = Path(\"outputs/table_extraction\"); out.mkdir(parents=True, exist_ok=True)\n",
    "    pdf_path = pdf_file.name; log_text = \"\"\n",
    "    log_text += log(\"Camelot으로 추출 시도 중...\")\n",
    "    try:\n",
    "        tables = camelot.read_pdf(pdf_path, flavor='lattice', pages='all')\n",
    "        if tables.n>0:\n",
    "            df = tables[0].df; xls = out/\"extracted_tables_camelot.xlsx\"\n",
    "            df.to_excel(xls, index=False); return df, log_text+log(f\"{tables.n}개 표 발견\"), str(xls)\n",
    "    except Exception as e:\n",
    "        log_text += log(f\"Camelot 실패: {e}\")\n",
    "    log_text += log(\"Tabula로 추출 시도 중...\")\n",
    "    try:\n",
    "        frames = tabula.read_pdf(pdf_path, pages='all', multiple_tables=True)\n",
    "        if frames:\n",
    "            df = frames[0]; xls = out/\"extracted_tables_tabula.xlsx\"\n",
    "            df.to_excel(xls, index=False); return df, log_text+log(f\"{len(frames)}개 표 발견\"), str(xls)\n",
    "    except Exception as e:\n",
    "        log_text += log(f\"Tabula 실패: {e}\")\n",
    "    return None, log_text+log(\"표를 추출하지 못했습니다.\"), None\n",
    "\n",
    "# =============================\n",
    "# 3) 🧠 RAG 챗봇 (무거운 모델만 사용)\n",
    "# =============================\n",
    "def handle_rag_indexing(pdf_files, chunk_size: int, chunk_overlap: int, progress=gr.Progress()):\n",
    "    if not pdf_files: return \"색인할 PDF 파일을 업로드하세요.\", gr.update(interactive=False)\n",
    "    log_text = \"\"\n",
    "    try:\n",
    "        progress(0, desc=\"임베딩 모델 로딩 중...\")\n",
    "        STATE.rag_embeddings = OllamaEmbeddings(model=EMB_MODEL)\n",
    "        log_text += log(\"임베딩 모델 로드 완료.\")\n",
    "        all_docs = []\n",
    "        for f in progress.tqdm(pdf_files, desc=\"PDF 파일 처리 중...\"):\n",
    "            loader = PyPDFLoader(f.name)\n",
    "            docs = loader.load()\n",
    "            splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "            all_docs.extend(splitter.split_documents(docs))\n",
    "        log_text += log(f\"총 {len(pdf_files)}개 PDF에서 {len(all_docs)}개 청크 생성.\")\n",
    "        progress(0.85, desc=\"ChromaDB 저장 중...\")\n",
    "        STATE.rag_vectorstore = Chroma.from_documents(\n",
    "            documents=all_docs,\n",
    "            embedding=STATE.rag_embeddings,\n",
    "            persist_directory=DEFAULT_PERSIST_DIR\n",
    "        )\n",
    "        log_text += log(\"벡터 DB 색인 완료. 질문 가능.\")\n",
    "        return log_text, gr.update(interactive=True)\n",
    "    except Exception as e:\n",
    "        return f\"색인 중 오류 발생: {e}\", gr.update(interactive=False)\n",
    "\n",
    "def handle_rag_chat_stream(message: str, history: list, temperature: float):\n",
    "    if not STATE.rag_vectorstore:\n",
    "        yield \"먼저 PDF 파일을 색인해주세요.\"; return\n",
    "    llm = STATE.get_llm(model_name=HEAVY_MODEL, temperature=temperature, num_predict=512)\n",
    "    retriever = STATE.rag_vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "    history.append({\"role\":\"user\",\"content\":message})\n",
    "    docs = retriever.invoke(message)\n",
    "    STATE.last_rag_sources = [\n",
    "        {\"source\": d.metadata.get('source','N/A'), \"page\": d.metadata.get('page','N/A'), \"content\": d.page_content}\n",
    "        for d in docs\n",
    "    ]\n",
    "    system_prompt = \"제공된 컨텍스트에 기반해 정확히 답하세요. 모르면 모른다고 답하기.\"\n",
    "    prompt_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"system\", \"컨텍스트:\\n{context}\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ])\n",
    "    chain = create_stuff_documents_chain(llm, prompt_template)\n",
    "    history.append({\"role\":\"assistant\",\"content\":\"\"})\n",
    "    resp = \"\"\n",
    "    for ch in chain.stream({\"input\": message, \"context\": docs}):\n",
    "        resp += ch; history[-1][\"content\"] = resp; yield history\n",
    "\n",
    "def update_rag_sources_display():\n",
    "    if not STATE.last_rag_sources: return \"검색된 문서가 없습니다.\"\n",
    "    md = \"### 📚 답변에 참조된 문서\\n\\n\"\n",
    "    for i, s in enumerate(STATE.last_rag_sources):\n",
    "        md += f\"**[출처 {i+1}]**\\n- **파일**: `{os.path.basename(s['source'])}` (페이지: {s['page']})\\n- **내용 일부**: {s['content'][:200]}...\\n\\n\"\n",
    "    return md\n",
    "\n",
    "# =============================\n",
    "# 4) 🖼️ 이미지 분석\n",
    "# =============================\n",
    "def handle_single_image_analysis(image, prompt: str, vlm_model: str, use_refine: bool, temperature: float):\n",
    "    if image is None: return \"이미지를 업로드하세요.\", \"\"\n",
    "    analysis = call_vlm([image.name], prompt, vlm_model)\n",
    "    if not use_refine: return analysis, analysis\n",
    "    llm = STATE.get_llm(model_name=FAST_MODEL, temperature=temperature, num_predict=200)\n",
    "    refined = llm.invoke(f\"다음 분석을 간결하고 정보가 풍부하게 정리:\\n{analysis}\").content\n",
    "    return refined, analysis\n",
    "\n",
    "def handle_image_comparison(image1, image2, prompt: str, vlm_model: str, use_refine: bool, temperature: float):\n",
    "    if image1 is None or image2 is None: return \"두 이미지를 모두 업로드하세요.\", \"\"\n",
    "    comparison = call_vlm([image1.name, image2.name], prompt, vlm_model)\n",
    "    if not use_refine: return comparison, comparison\n",
    "    llm = STATE.get_llm(model_name=FAST_MODEL, temperature=temperature, num_predict=220)\n",
    "    refined = llm.invoke(f\"다음 비교결과를 항목화해 정리:\\n{comparison}\").content\n",
    "    return refined, comparison\n",
    "\n",
    "# =============================\n",
    "# 5) 🛠️ 고급 생성기\n",
    "# =============================\n",
    "def handle_mermaid_generation(idea: str, diagram_type: str, use_llm: bool):\n",
    "    if not idea.strip(): return \"\", \"\", \"아이디어를 입력하세요.\"\n",
    "    if not use_llm:\n",
    "        code, status = idea, \"LLM 미사용\"\n",
    "    else:\n",
    "        llm = STATE.get_llm(model_name=FAST_MODEL, temperature=0.1, num_predict=192)\n",
    "        sys = f\"Mermaid {diagram_type} 타입의 '유효한' 코드만 코드블록으로 출력. 설명 금지.\"\n",
    "        resp = llm.invoke([SystemMessage(content=sys), HumanMessage(content=idea)]).content\n",
    "        m = re.search(r\"```mermaid\\s*([\\s\\S]*?)```\", resp)\n",
    "        if m: code, status = m.group(1).strip(), \"Mermaid 코드 생성 완료.\"\n",
    "        else:\n",
    "            code = re.sub(r\"^.*?mermaid\", \"\", resp, flags=re.S|re.I).strip()\n",
    "            status = \"코드 블록 미검출 → 본문에서 추출\"\n",
    "    return f\"```mermaid\\n{code}\\n```\", code, status\n",
    "\n",
    "def handle_sd_prompt_generation(idea: str):\n",
    "    if not idea.strip(): return \"\", \"\", \"아이디어를 입력하세요.\"\n",
    "    llm = STATE.get_llm(model_name=FAST_MODEL, temperature=0.6, num_predict=160)\n",
    "    sys = \"Stable Diffusion 프롬프트 엔지니어. JSON으로만 응답하고 keys=['prompt','negative_prompt'] 포함.\"\n",
    "    resp = llm.invoke([SystemMessage(content=sys), HumanMessage(content=f\"아이디어: {idea}\")]).content\n",
    "    data = safe_json_extract(resp)\n",
    "    if isinstance(data, dict):\n",
    "        return data.get(\"prompt\", idea), data.get(\"negative_prompt\",\"low quality, blurry\"), \"프롬프트 생성 완료.\"\n",
    "    return idea, \"\", \"JSON 파싱 실패. 원본 응답:\\n\"+resp\n",
    "\n",
    "def handle_structured_ocr(image, use_llm: bool):\n",
    "    if image is None: return None, \"\", None, \"이미지를 업로드하세요.\"\n",
    "    # 지연 임포트\n",
    "    try:\n",
    "        import easyocr\n",
    "    except Exception:\n",
    "        return None, \"EasyOCR 미설치 또는 로딩 실패.\", None, \"\"\n",
    "    reader = easyocr.Reader(['ko','en'])\n",
    "    np_img = np.array(image)\n",
    "    results = reader.readtext(np_img)\n",
    "    overlay = image.copy(); draw = ImageDraw.Draw(overlay)\n",
    "    full_text = \"\\n\".join([r[1] for r in results])\n",
    "    for (bbox, text, prob) in results:\n",
    "        draw.polygon([tuple(p) for p in bbox], outline=\"cyan\", width=2)\n",
    "    if not use_llm: return overlay, full_text, None, \"LLM 미사용\"\n",
    "    llm = STATE.get_llm(model_name=FAST_MODEL, temperature=0.1, num_predict=220)\n",
    "    resp = llm.invoke([\n",
    "        SystemMessage(content=\"OCR 텍스트를 구조화 JSON으로 변환. 표면 구조 인식 시 rows/columns or items 사용.\"),\n",
    "        HumanMessage(content=full_text)\n",
    "    ]).content\n",
    "    data = safe_json_extract(resp); df = None\n",
    "    if isinstance(data, dict):\n",
    "        if 'items' in data and isinstance(data['items'], list):\n",
    "            df = pd.DataFrame(data['items'])\n",
    "        elif 'rows' in data and 'columns' in data:\n",
    "            df = pd.DataFrame(data['rows'], columns=data['columns'])\n",
    "    return overlay, full_text, df, (json.dumps(data, ensure_ascii=False, indent=2) if data else \"\")\n",
    "\n",
    "# =============================\n",
    "# 6) 🌐 웹 & 도구\n",
    "# =============================\n",
    "def handle_sequential_thinking(query: str, temperature: float, progress=gr.Progress()):\n",
    "    if not query.strip(): return \"\", \"\", \"\", \"질의를 입력하세요.\"\n",
    "    llm = STATE.get_llm(model_name=FAST_MODEL, temperature=temperature, num_predict=256)\n",
    "    progress(0.1, desc=\"[1/3] 분석 중...\")\n",
    "    analysis = llm.invoke([SystemMessage(content=\"핵심 쟁점을 3~5줄로.\"), HumanMessage(content=query)]).content\n",
    "    progress(0.5, desc=\"[2/3] 최종 답변 생성 중...\")\n",
    "    answer = llm.invoke([SystemMessage(content=\"간결·정확·구조화된 답변 8~12줄.\"), HumanMessage(content=f\"{query}\\n\\n분석:{analysis}\")]).content\n",
    "    progress(0.8, desc=\"[3/3] 검증 중...\")\n",
    "    reflection = llm.invoke([SystemMessage(content=\"모호성/누락/과장 점검 후 3줄 제안.\"), HumanMessage(content=f\"답변:{answer}\")]).content\n",
    "    return analysis, answer, reflection, \"모든 단계 완료.\"\n",
    "\n",
    "def handle_web_rag(query: str, temperature: float, progress=gr.Progress()):\n",
    "    llm = STATE.get_llm(model_name=FAST_MODEL, temperature=temperature, num_predict=256)\n",
    "    progress(0, desc=\"웹 검색 중...\")\n",
    "    search = DuckDuckGoSearchResults()\n",
    "    search_results = search.run(query)\n",
    "    links = re.findall(r'https?://[^\\s,```<>\"]+', search_results)\n",
    "    if not links: return \"관련 웹 페이지를 찾지 못했습니다.\", search_results, \"\"\n",
    "    links = links[:2]  # 속도 위해 상위 2개만\n",
    "    progress(0.35, desc=f\"{len(links)}개 링크 수집 중...\")\n",
    "    try:\n",
    "        # bs_kwargs는 버전별 차이가 있어 제거(안정성 우선)\n",
    "        loader = WebBaseLoader(web_paths=links)\n",
    "        docs = loader.load()\n",
    "        ctx = \"\\n\\n---\\n\\n\".join([d.page_content for d in docs])\n",
    "    except Exception as e:\n",
    "        ctx = f\"콘텐츠 수집 실패: {e}\"\n",
    "    progress(0.7, desc=\"답변 생성 중...\")\n",
    "    resp = llm.invoke([SystemMessage(content=\"웹 결과 기반으로 정확·간결히 답변.\"), HumanMessage(content=f\"검색 결과:\\n{ctx}\\n\\n질문: {query}\")]).content\n",
    "    return resp, search_results, \"\\n\".join(links)\n",
    "\n",
    "# =============================\n",
    "# 7) Gradio UI\n",
    "# =============================\n",
    "def build_ui():\n",
    "    with gr.Blocks(theme=gr.themes.Soft(), title=\"DeepSeek-R1 AI Toolkit\") as demo:\n",
    "        gr.Markdown(\"# 🚀 DeepSeek-R1 AI Toolkit (All-in-One)\")\n",
    "        gr.Markdown(\"38개 예제를 통합한 로컬 AI 애플리케이션입니다. (속도 최적화)\")\n",
    "\n",
    "        # 챗봇\n",
    "        with gr.Tab(\"💬 챗봇\"):\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=1):\n",
    "                    persona = gr.Dropdown(list(PERSONAS.keys()), value=\"기본 상담사\", label=\"페르소나 선택\")\n",
    "                    temp = gr.Slider(0.0, 1.5, value=0.7, step=0.1, label=\"Temperature\")\n",
    "                with gr.Column(scale=3):\n",
    "                    win = gr.Chatbot(height=500, label=\"대화창\", type=\"messages\")\n",
    "                    with gr.Row():\n",
    "                        msg = gr.Textbox(label=\"메시지 입력\", placeholder=\"무엇이든 물어보세요...\", scale=4)\n",
    "                        btn = gr.Button(\"전송\", variant=\"primary\", scale=1)\n",
    "                        clr = gr.Button(\"초기화\")\n",
    "\n",
    "            def on_submit(m, h, p, t):\n",
    "                for nh in handle_chatbot_stream(m, h, p, t):\n",
    "                    yield nh, \"\"\n",
    "            msg.submit(on_submit, [msg, win, persona, temp], [win, msg])\n",
    "            btn.click(on_submit, [msg, win, persona, temp], [win, msg])\n",
    "            clr.click(lambda: [], None, win)\n",
    "\n",
    "        # PDF & 문서\n",
    "        with gr.Tab(\"📄 PDF & 문서\"):\n",
    "            with gr.Tabs():\n",
    "                with gr.TabItem(\"PDF 요약\"):\n",
    "                    with gr.Row():\n",
    "                        f_in = gr.File(label=\"PDF 파일\", file_types=[\".pdf\"])\n",
    "                        with gr.Column():\n",
    "                            chunk = gr.Slider(1000, 8000, value=4000, step=500, label=\"청크 크기\")\n",
    "                            t = gr.Slider(0.0, 1.0, value=0.2, step=0.1, label=\"Temperature\")\n",
    "                            run = gr.Button(\"요약 실행\", variant=\"primary\")\n",
    "                    out = gr.Textbox(label=\"요약 결과\", lines=15)\n",
    "                    with gr.Accordion(\"전체 텍스트\", open=False):\n",
    "                        raw = gr.Textbox(label=\"원본 텍스트\", lines=10)\n",
    "                    run.click(handle_pdf_summary, [f_in, chunk, t], [out, raw])\n",
    "\n",
    "                with gr.TabItem(\"텍스트/이미지 추출\"):\n",
    "                    with gr.Row():\n",
    "                        f_in2 = gr.File(label=\"PDF 파일\", file_types=[\".pdf\"])\n",
    "                        with gr.Column():\n",
    "                            head = gr.Slider(0, 200, value=60, label=\"헤더 높이 (px)\")\n",
    "                            foot = gr.Slider(0, 200, value=60, label=\"푸터 높이 (px)\")\n",
    "                            run2 = gr.Button(\"추출 실행\", variant=\"primary\")\n",
    "                    with gr.Row():\n",
    "                        tx = gr.Textbox(label=\"추출된 텍스트\", lines=15, scale=2)\n",
    "                        with gr.Column(scale=1):\n",
    "                            gal = gr.Gallery(label=\"추출된 이미지\", height=400)\n",
    "                            pth = gr.Textbox(label=\"저장 경로\")\n",
    "                    run2.click(handle_pdf_extraction, [f_in2, head, foot], [tx, gal, pth])\n",
    "\n",
    "                with gr.TabItem(\"표 추출\"):\n",
    "                    f_tbl = gr.File(label=\"PDF 파일\", file_types=[\".pdf\"])\n",
    "                    run3 = gr.Button(\"표 추출 실행\", variant=\"primary\")\n",
    "                    df = gr.Dataframe(label=\"추출된 표\")\n",
    "                    logbx = gr.Textbox(label=\"처리 로그\", lines=5)\n",
    "                    xfile = gr.File(label=\"엑셀 다운로드\")\n",
    "                    run3.click(handle_pdf_table_extraction, [f_tbl], [df, logbx, xfile])\n",
    "\n",
    "        # RAG\n",
    "        with gr.Tab(\"🧠 RAG 챗봇\"):\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=1):\n",
    "                    gr.Markdown(\"##### 1. 문서 색인\")\n",
    "                    f_multi = gr.File(label=\"PDF 파일들\", file_count=\"multiple\", file_types=[\".pdf\"])\n",
    "                    cs = gr.Slider(200, 2000, value=1000, label=\"청크 크기\")\n",
    "                    co = gr.Slider(0, 500, value=100, label=\"청크 중첩\")\n",
    "                    idx = gr.Button(\"색인 시작\", variant=\"primary\")\n",
    "                    stat = gr.Textbox(label=\"색인 상태\", lines=10)\n",
    "                with gr.Column(scale=2):\n",
    "                    gr.Markdown(\"##### 2. 문서 기반 대화\")\n",
    "                    rag_chat = gr.Chatbot(height=450, label=\"대화창\", type=\"messages\")\n",
    "                    with gr.Accordion(\"참조 문서\", open=False):\n",
    "                        rag_src = gr.Markdown()\n",
    "                    rtemp = gr.Slider(0.0, 1.0, value=0.3, label=\"Temperature\")\n",
    "                    rmsg = gr.Textbox(label=\"질문\", placeholder=\"문서에 대해 질문하세요...\", interactive=False)\n",
    "                    rsend = gr.Button(\"전송\", variant=\"primary\")\n",
    "\n",
    "            idx.click(handle_rag_indexing, [f_multi, cs, co], [stat, rmsg])\n",
    "\n",
    "            def on_rag_submit(m, h, t):\n",
    "                for nh in handle_rag_chat_stream(m, h, t):\n",
    "                    yield nh, \"\"\n",
    "            rmsg.submit(on_rag_submit, [rmsg, rag_chat, rtemp], [rag_chat, rmsg]).then(update_rag_sources_display, None, rag_src)\n",
    "            rsend.click(on_rag_submit, [rmsg, rag_chat, rtemp], [rag_chat, rmsg]).then(update_rag_sources_display, None, rag_src)\n",
    "\n",
    "        # 이미지 분석\n",
    "        with gr.Tab(\"🖼️ 이미지 분석\"):\n",
    "            vlm_model = gr.Dropdown([VLM_MODEL, \"llava:latest\"], value=VLM_MODEL, label=\"VLM 모델\")\n",
    "            with gr.Tabs():\n",
    "                with gr.TabItem(\"단일 이미지\"):\n",
    "                    with gr.Row():\n",
    "                        img_single = gr.Image(type=\"pil\", label=\"이미지\")\n",
    "                        with gr.Column():\n",
    "                            iprompt = gr.Textbox(label=\"프롬프트\", value=\"이 이미지를 설명해주세요.\", lines=3)\n",
    "                            irefine = gr.Checkbox(label=\"LLM으로 정제\", value=True)\n",
    "                            itemp = gr.Slider(0.0, 1.0, value=0.7, label=\"Temperature\")\n",
    "                            ibtn = gr.Button(\"분석\", variant=\"primary\")\n",
    "                    iout = gr.Textbox(label=\"분석 결과\", lines=10)\n",
    "                    with gr.Accordion(\"VLM 원본\", open=False):\n",
    "                        iraw = gr.Textbox(label=\"원본 결과\")\n",
    "                    ibtn.click(handle_single_image_analysis, [img_single, iprompt, vlm_model, irefine, itemp], [iout, iraw])\n",
    "\n",
    "                with gr.TabItem(\"이미지 비교\"):\n",
    "                    with gr.Row():\n",
    "                        img1 = gr.Image(type=\"pil\", label=\"이미지 1\")\n",
    "                        img2 = gr.Image(type=\"pil\", label=\"이미지 2\")\n",
    "                    cprompt = gr.Textbox(label=\"프롬프트\", value=\"두 이미지를 비교해주세요.\", lines=3)\n",
    "                    crefine = gr.Checkbox(label=\"LLM으로 정제\", value=True)\n",
    "                    ctemp = gr.Slider(0.0, 1.0, value=0.7, label=\"Temperature\")\n",
    "                    cbtn = gr.Button(\"비교\", variant=\"primary\")\n",
    "                    cout = gr.Textbox(label=\"비교 결과\", lines=10)\n",
    "                    with gr.Accordion(\"VLM 원본\", open=False):\n",
    "                        craw = gr.Textbox(label=\"원본 결과\")\n",
    "                    cbtn.click(handle_image_comparison, [img1, img2, cprompt, vlm_model, crefine, ctemp], [cout, craw])\n",
    "\n",
    "        # 고급 생성기\n",
    "        with gr.Tab(\"🛠️ 고급 생성기\"):\n",
    "            with gr.Tabs():\n",
    "                with gr.TabItem(\"Mermaid 다이어그램\"):\n",
    "                    with gr.Row():\n",
    "                        with gr.Column():\n",
    "                            m_idea = gr.Textbox(label=\"아이디어\", lines=5)\n",
    "                            m_type = gr.Dropdown([\"flowchart\",\"sequenceDiagram\",\"gantt\",\"pie\",\"classDiagram\"], value=\"flowchart\", label=\"다이어그램 타입\")\n",
    "                            m_llm = gr.Checkbox(value=True, label=\"LLM 사용\")\n",
    "                            m_btn = gr.Button(\"생성\", variant=\"primary\")\n",
    "                        with gr.Column():\n",
    "                            m_out = gr.Markdown(label=\"렌더링\")\n",
    "                            m_code = gr.Code(label=\"코드\", language=\"markdown\")\n",
    "                            m_status = gr.Textbox(label=\"상태\")\n",
    "                    m_btn.click(handle_mermaid_generation, [m_idea, m_type, m_llm], [m_out, m_code, m_status])\n",
    "\n",
    "                with gr.TabItem(\"SD 프롬프트\"):\n",
    "                    sd_idea = gr.Textbox(label=\"아이디어\", lines=3)\n",
    "                    sd_btn = gr.Button(\"생성\", variant=\"primary\")\n",
    "                    sd_prompt = gr.Textbox(label=\"프롬프트\", lines=4)\n",
    "                    sd_negative = gr.Textbox(label=\"네거티브 프롬프트\", lines=2)\n",
    "                    sd_status = gr.Textbox(label=\"상태\")\n",
    "                    sd_btn.click(handle_sd_prompt_generation, [sd_idea], [sd_prompt, sd_negative, sd_status])\n",
    "\n",
    "                with gr.TabItem(\"구조화 OCR\"):\n",
    "                    ocr_img = gr.Image(type=\"pil\", label=\"이미지\")\n",
    "                    ocr_use_llm = gr.Checkbox(value=True, label=\"LLM으로 구조화\")\n",
    "                    ocr_btn = gr.Button(\"OCR 실행\", variant=\"primary\")\n",
    "                    with gr.Row():\n",
    "                        ocr_overlay = gr.Image(label=\"인식 영역\")\n",
    "                        ocr_df = gr.Dataframe(label=\"구조화 데이터\")\n",
    "                    with gr.Accordion(\"텍스트 및 JSON\", open=False):\n",
    "                        ocr_text = gr.Textbox(label=\"텍스트\")\n",
    "                        ocr_json = gr.Code(label=\"JSON\", language=\"json\")\n",
    "                    ocr_btn.click(handle_structured_ocr, [ocr_img, ocr_use_llm], [ocr_overlay, ocr_text, ocr_df, ocr_json])\n",
    "\n",
    "        # 웹 & 도구\n",
    "        with gr.Tab(\"🌐 웹 & 도구\"):\n",
    "            wq = gr.Textbox(label=\"웹 검색 질문\", lines=2)\n",
    "            wt = gr.Slider(0.0, 1.0, value=0.7, label=\"Temperature\")\n",
    "            wb = gr.Button(\"검색 및 답변\", variant=\"primary\")\n",
    "            wa = gr.Textbox(label=\"답변\", lines=10)\n",
    "            with gr.Accordion(\"검색 정보\", open=False):\n",
    "                wr = gr.Textbox(label=\"검색 결과\")\n",
    "                wl = gr.Textbox(label=\"참조 URL\")\n",
    "            wb.click(handle_web_rag, [wq, wt], [wa, wr, wl])\n",
    "\n",
    "        # 순차적 사고\n",
    "        with gr.Tab(\"🤔 순차적 사고\"):\n",
    "            sq = gr.Textbox(label=\"질문\", lines=3)\n",
    "            st = gr.Slider(0.0, 1.0, value=0.7, label=\"Temperature\")\n",
    "            sb = gr.Button(\"사고 시작\", variant=\"primary\")\n",
    "            s_status = gr.Textbox(label=\"상태\")\n",
    "            with gr.Row():\n",
    "                s_a = gr.Markdown(label=\"분석\")\n",
    "                s_ans = gr.Markdown(label=\"답변\")\n",
    "                s_ref = gr.Markdown(label=\"검증\")\n",
    "            sb.click(handle_sequential_thinking, [sq, st], [s_a, s_ans, s_ref, s_status])\n",
    "\n",
    "    return demo\n",
    "\n",
    "# =============================\n",
    "# 메인 실행\n",
    "# =============================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🚀 Gradio 앱 실행 중...\")\n",
    "    app = build_ui()\n",
    "    # ▼ Gradio 4.x: queue 인자 축소(호환)\n",
    "    app = app.queue(max_size=32)\n",
    "    # share=False 가 터널링 지연을 줄여 훨씬 빠름\n",
    "    app.launch(server_name=\"0.0.0.0\", server_port=7860, inbrowser=True, share=False, show_error=True)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
