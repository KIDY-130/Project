{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8f4332",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# =============================\n",
    "# ğŸš€ DeepSeek-R1 AI Toolkit (ë¹ ë¥¸ ì‹¤í–‰ ìµœì í™” ë²„ì „) â€” Gradio queue ìˆ˜ì •\n",
    "# =============================\n",
    "\n",
    "# ---------- í™˜ê²½/ì„¤ì¹˜ ìµœì†Œí™” & ìºì‹± ----------\n",
    "print(\"ğŸ“¦ í™˜ê²½ ì ê²€ ì¤‘...\")\n",
    "\n",
    "import os, shutil, subprocess, time, re, json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "def sh(cmd: str):\n",
    "    return subprocess.run(cmd, shell=True, check=False, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "\n",
    "# Java (tabula/camelot í•„ìš”) â€” ì—†ì„ ë•Œë§Œ ì„¤ì¹˜\n",
    "if shutil.which(\"java\") is None:\n",
    "    print(\"ğŸ‘‰ Java ì„¤ì¹˜\")\n",
    "    sh(\"apt-get update -qq && apt-get install -y openjdk-11-jdk-headless\")\n",
    "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-11-openjdk-amd64'\n",
    "\n",
    "# pip íŒ¨í‚¤ì§€ â€” í•„ìš”í•œ ê²ƒë§Œ & ì—†ì„ ë•Œë§Œ\n",
    "def ensure(mod: str, pip_name: str = None):\n",
    "    pip_name = pip_name or mod\n",
    "    try:\n",
    "        __import__(mod)\n",
    "    except Exception:\n",
    "        print(f\"ğŸ“¥ install {pip_name}\")\n",
    "        sh(f\"pip install -q {pip_name}\")\n",
    "\n",
    "print(\"ğŸ í•„ìˆ˜ íŒ¨í‚¤ì§€ í™•ì¸\")\n",
    "for mod, pipn in [\n",
    "    (\"gradio\",\"gradio\"),\n",
    "    (\"ollama\",\"ollama\"),\n",
    "    (\"fitz\",\"pymupdf\"),\n",
    "    (\"pandas\",\"pandas\"),\n",
    "    (\"PIL\",\"Pillow\"),\n",
    "    (\"numpy\",\"numpy\"),\n",
    "    (\"langchain_community\",\"langchain-community\"),\n",
    "    (\"langchain_chroma\",\"langchain-chroma\"),\n",
    "    (\"langchain_text_splitters\",\"langchain-text-splitters\"),\n",
    "    (\"beautifulsoup4\",\"beautifulsoup4\"),\n",
    "    (\"duckduckgo_search\",\"duckduckgo-search\"),\n",
    "]:\n",
    "    ensure(mod, pipn)\n",
    "\n",
    "# Ollama ì„¤ì¹˜\n",
    "print(\"ğŸ¦™ Ollama ì¤€ë¹„\")\n",
    "if shutil.which(\"ollama\") is None:\n",
    "    sh(\"curl -fsSL https://ollama.com/install.sh | sh\")\n",
    "\n",
    "# ì„œë²„ ë°±ê·¸ë¼ìš´ë“œ\n",
    "subprocess.Popen([\"ollama\",\"serve\"])\n",
    "time.sleep(3)\n",
    "\n",
    "# ëª¨ë¸ êµ¬ì„±: ë¹ ë¥¸ ëª¨ë¸ / ë¬´ê±°ìš´ ëª¨ë¸ / VLM\n",
    "FAST_MODEL  = \"qwen2.5:7b-instruct\"   # ë¹ ë¥¸ ìƒì„±/ë„êµ¬ íƒ­\n",
    "HEAVY_MODEL = \"deepseek-r1\"           # RAG ë“± ê¸´ ì¶”ë¡  í•„ìš” ì‹œ\n",
    "VLM_MODEL   = \"qwen2.5-vl\"            # ë¹„ì „-í…ìŠ¤íŠ¸\n",
    "EMB_MODEL   = \"nomic-embed-text\"      # ì„ë² ë”©\n",
    "\n",
    "print(\"ğŸ“¥ í•„ìš”í•œ ëª¨ë¸ pull\")\n",
    "for m in [FAST_MODEL, HEAVY_MODEL, VLM_MODEL, EMB_MODEL]:\n",
    "    sh(f\"ollama pull {m}\")\n",
    "\n",
    "# ê°„ë‹¨ ì›œì—…(ì²« í† í° ì§€ì—° ê°ì†Œ)\n",
    "print(\"ğŸ”¥ ëª¨ë¸ ì›œì—…\")\n",
    "sh(f'''python - <<'PY'\n",
    "import ollama\n",
    "for m in [\"{FAST_MODEL}\", \"{HEAVY_MODEL}\"]:\n",
    "    try:\n",
    "        ollama.chat(model=m, messages=[{{\"role\":\"user\",\"content\":\"hi\"}}], options={{\"num_predict\":8}})\n",
    "    except Exception as e:\n",
    "        print(\"warmup err:\", m, e)\n",
    "PY''')\n",
    "print(\"âœ… í™˜ê²½ ì¤€ë¹„ ì™„ë£Œ\")\n",
    "\n",
    "# =============================\n",
    "# ğŸ§  ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ ì½”ë“œ\n",
    "# =============================\n",
    "from typing import Optional, List, Dict\n",
    "import gradio as gr\n",
    "import ollama\n",
    "import fitz  # PyMuPDF\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader, WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "\n",
    "# ---------- ì „ì—­ ----------\n",
    "class GlobalState:\n",
    "    def __init__(self):\n",
    "        self.cache = {}  # (model, temp, opts_key) -> ChatOllama\n",
    "        self.rag_vectorstore: Optional[Chroma] = None\n",
    "        self.rag_embeddings: Optional[OllamaEmbeddings] = None\n",
    "        self.last_rag_sources: List[Dict] = []\n",
    "\n",
    "    def get_llm(self, model_name=FAST_MODEL, temperature=0.7, **model_kwargs) -> ChatOllama:\n",
    "        # ì£¼ìš” ì†ë„ì˜µì…˜: num_predict(ì‘ë‹µê¸¸ì´), num_ctx\n",
    "        if \"num_predict\" not in model_kwargs:\n",
    "            model_kwargs[\"num_predict\"] = 256\n",
    "        opts_key = tuple(sorted(model_kwargs.items()))\n",
    "        key = (model_name, float(temperature), opts_key)\n",
    "        if key not in self.cache:\n",
    "            self.cache[key] = ChatOllama(\n",
    "                model=model_name,\n",
    "                temperature=temperature,\n",
    "                model_kwargs=model_kwargs\n",
    "            )\n",
    "        return self.cache[key]\n",
    "\n",
    "STATE = GlobalState()\n",
    "DEFAULT_PERSIST_DIR = \"./chroma_db_store\"\n",
    "os.makedirs(DEFAULT_PERSIST_DIR, exist_ok=True)\n",
    "\n",
    "# ---------- í—¬í¼ ----------\n",
    "def log(message: str) -> str:\n",
    "    ts = datetime.now().strftime('%H:%M:%S')\n",
    "    return f\"[{ts}] {message}\\n\"\n",
    "\n",
    "def strip_think_tags(text: str) -> str:\n",
    "    return re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL|re.IGNORECASE).strip()\n",
    "\n",
    "def safe_json_extract(text: str):\n",
    "    \"\"\"ë¹ ë¥´ê³  ì•ˆì „í•œ JSON ì¶”ì¶œ\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return None\n",
    "    m = re.search(r\"```json\\s*(\\{[\\s\\S]*?\\}|\\[[\\s\\S]*?\\])\\s*```\", text, flags=re.I)\n",
    "    if m:\n",
    "        try: return json.loads(m.group(1))\n",
    "        except: pass\n",
    "    text_wo = re.sub(r\"```(?!json)[\\s\\S]*?```\", \"\", text, flags=re.I)\n",
    "    cands = []\n",
    "    for p in (r\"\\{[\\s\\S]*?\\}\", r\"\\[[\\s\\S]*?\\]\"):\n",
    "        for mm in re.finditer(p, text_wo):\n",
    "            cands.append(mm.group(0))\n",
    "    for chunk in sorted(cands, key=len, reverse=True):\n",
    "        try: return json.loads(chunk)\n",
    "        except: pass\n",
    "    try: return json.loads(text_wo.strip())\n",
    "    except: return None\n",
    "\n",
    "def call_vlm(image_paths: list, prompt: str, vlm_model: str) -> str:\n",
    "    try:\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt, \"images\": image_paths}]\n",
    "        res = ollama.chat(model=vlm_model, messages=messages, options={\"num_predict\":256})\n",
    "        return res.get(\"message\", {}).get(\"content\", \"VLM í˜¸ì¶œ ì‹¤íŒ¨\")\n",
    "    except Exception as e:\n",
    "        return f\"VLM ì˜¤ë¥˜: {e}\"\n",
    "\n",
    "# =============================\n",
    "# 1) ğŸ’¬ ì±—ë´‡\n",
    "# =============================\n",
    "PERSONAS = {\n",
    "    \"ê¸°ë³¸ ìƒë‹´ì‚¬\": \"ë„ˆëŠ” ì‚¬ìš©ìë¥¼ ë„ì™€ì£¼ëŠ” ì¹œì ˆí•˜ê³  ìœ ëŠ¥í•œ ìƒë‹´ì‚¬ì•¼.\",\n",
    "    \"ë°±ì„¤ê³µì£¼ ë§ˆë²•ê±°ìš¸\": \"ë„ˆëŠ” ë°±ì„¤ê³µì£¼ ì´ì•¼ê¸° ì† ë§ˆë²• ê±°ìš¸ì´ì•¼. í’ˆìœ„ ìˆê³  ìš´ìœ¨ê° ìˆê²Œ ë‹µí•´ì¤˜.\",\n",
    "    \"ìœ ì¹˜ì›ìƒ\": \"ë„ˆëŠ” 5ì‚´ ìœ ì¹˜ì›ìƒì´ì•¼. ì§§ê³  ì‰¬ìš´ ë§ë¡œ ê·€ì—½ê²Œ ëŒ€ë‹µí•´.\",\n",
    "}\n",
    "\n",
    "def handle_chatbot_stream(message: str, history: list, persona: str, temperature: float):\n",
    "    if not message.strip(): return\n",
    "    llm = STATE.get_llm(model_name=FAST_MODEL, temperature=temperature, num_predict=256)\n",
    "    system_prompt = PERSONAS.get(persona, PERSONAS[\"ê¸°ë³¸ ìƒë‹´ì‚¬\"])\n",
    "    msgs = [SystemMessage(content=system_prompt)]\n",
    "    for t in history:\n",
    "        msgs.append(HumanMessage(content=t[\"content\"]) if t[\"role\"]==\"user\" else AIMessage(content=t[\"content\"]))\n",
    "    msgs.append(HumanMessage(content=message))\n",
    "    stream = llm.stream(msgs)\n",
    "    history.append({\"role\":\"user\",\"content\":message})\n",
    "    history.append({\"role\":\"assistant\",\"content\":\"\"})\n",
    "    for ch in stream:\n",
    "        history[-1][\"content\"] += ch.content\n",
    "        yield history\n",
    "\n",
    "# =============================\n",
    "# 2) ğŸ“„ PDF & ë¬¸ì„œ\n",
    "# =============================\n",
    "def handle_pdf_summary(pdf_file, chunk_size: int, temperature: float, progress=gr.Progress()):\n",
    "    if not pdf_file: return \"PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.\", \"\"\n",
    "    llm = STATE.get_llm(model_name=FAST_MODEL, temperature=temperature, num_predict=256)\n",
    "    progress(0, desc=\"PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...\")\n",
    "    doc = fitz.open(pdf_file.name)\n",
    "    full_text = \"\\n\".join([p.get_text() for p in doc]); doc.close()\n",
    "    if not full_text.strip(): return \"PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\", full_text\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=int(chunk_size*0.1))\n",
    "    chunks = splitter.split_text(full_text)\n",
    "    system_prompt = \"ë‹¤ìŒ í…ìŠ¤íŠ¸ëŠ” ë¬¸ì„œì˜ ì¼ë¶€ì…ë‹ˆë‹¤. í•µì‹¬ì„ ì •í™•Â·ê°„ê²°íˆ ìš”ì•½í•˜ì„¸ìš”.\"\n",
    "    summaries = []\n",
    "    for c in progress.tqdm(chunks, desc=\"ê° ì²­í¬ ìš”ì•½ ì¤‘...\"):\n",
    "        s = llm.invoke([SystemMessage(content=system_prompt), HumanMessage(content=c)]).content\n",
    "        summaries.append(s)\n",
    "    if len(summaries)==1:\n",
    "        final = summaries[0]\n",
    "    else:\n",
    "        progress(0.85, desc=\"ë¶€ë¶„ ìš”ì•½ í†µí•© ì¤‘...\")\n",
    "        final = llm.invoke([\n",
    "            SystemMessage(content=\"ë¶€ë¶„ ìš”ì•½ë“¤ì„ ì¢…í•©í•´ ì „ì²´ ìš”ì•½ 10~15ì¤„.\"),\n",
    "            HumanMessage(content=\"\\n\\n---\\n\\n\".join(summaries))\n",
    "        ]).content\n",
    "    return final, full_text\n",
    "\n",
    "def handle_pdf_extraction(pdf_file, header_px: int, footer_px: int):\n",
    "    if not pdf_file: return \"PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.\", [], None\n",
    "    out = Path(\"outputs/pdf_extraction\"); out.mkdir(parents=True, exist_ok=True)\n",
    "    doc = fitz.open(pdf_file.name)\n",
    "    full_text, images = \"\", []\n",
    "    for i, page in enumerate(doc):\n",
    "        rect = page.rect\n",
    "        text = page.get_text(\"text\", clip=(0, header_px, rect.width, rect.height-footer_px))\n",
    "        full_text += f\"--- Page {i+1} ---\\n{text}\\n\\n\"\n",
    "        for idx, img in enumerate(page.get_images(full=True)):\n",
    "            xref = img[0]; base = doc.extract_image(xref)\n",
    "            path = out / f\"page{i+1}_img{idx+1}.{base['ext']}\"\n",
    "            with open(path,\"wb\") as f: f.write(base[\"image\"])\n",
    "            images.append(str(path))\n",
    "    doc.close()\n",
    "    return full_text, images, str(out)\n",
    "\n",
    "def handle_pdf_table_extraction(pdf_file):\n",
    "    if not pdf_file: return None, \"PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.\", None\n",
    "    # ì§€ì—° ì„í¬íŠ¸\n",
    "    try:\n",
    "        import camelot; import tabula\n",
    "    except Exception:\n",
    "        return None, \"í…Œì´ë¸” ì¶”ì¶œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\", None\n",
    "    os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-11-openjdk-amd64'\n",
    "    out = Path(\"outputs/table_extraction\"); out.mkdir(parents=True, exist_ok=True)\n",
    "    pdf_path = pdf_file.name; log_text = \"\"\n",
    "    log_text += log(\"Camelotìœ¼ë¡œ ì¶”ì¶œ ì‹œë„ ì¤‘...\")\n",
    "    try:\n",
    "        tables = camelot.read_pdf(pdf_path, flavor='lattice', pages='all')\n",
    "        if tables.n>0:\n",
    "            df = tables[0].df; xls = out/\"extracted_tables_camelot.xlsx\"\n",
    "            df.to_excel(xls, index=False); return df, log_text+log(f\"{tables.n}ê°œ í‘œ ë°œê²¬\"), str(xls)\n",
    "    except Exception as e:\n",
    "        log_text += log(f\"Camelot ì‹¤íŒ¨: {e}\")\n",
    "    log_text += log(\"Tabulaë¡œ ì¶”ì¶œ ì‹œë„ ì¤‘...\")\n",
    "    try:\n",
    "        frames = tabula.read_pdf(pdf_path, pages='all', multiple_tables=True)\n",
    "        if frames:\n",
    "            df = frames[0]; xls = out/\"extracted_tables_tabula.xlsx\"\n",
    "            df.to_excel(xls, index=False); return df, log_text+log(f\"{len(frames)}ê°œ í‘œ ë°œê²¬\"), str(xls)\n",
    "    except Exception as e:\n",
    "        log_text += log(f\"Tabula ì‹¤íŒ¨: {e}\")\n",
    "    return None, log_text+log(\"í‘œë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\"), None\n",
    "\n",
    "# =============================\n",
    "# 3) ğŸ§  RAG ì±—ë´‡ (ë¬´ê±°ìš´ ëª¨ë¸ë§Œ ì‚¬ìš©)\n",
    "# =============================\n",
    "def handle_rag_indexing(pdf_files, chunk_size: int, chunk_overlap: int, progress=gr.Progress()):\n",
    "    if not pdf_files: return \"ìƒ‰ì¸í•  PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.\", gr.update(interactive=False)\n",
    "    log_text = \"\"\n",
    "    try:\n",
    "        progress(0, desc=\"ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...\")\n",
    "        STATE.rag_embeddings = OllamaEmbeddings(model=EMB_MODEL)\n",
    "        log_text += log(\"ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.\")\n",
    "        all_docs = []\n",
    "        for f in progress.tqdm(pdf_files, desc=\"PDF íŒŒì¼ ì²˜ë¦¬ ì¤‘...\"):\n",
    "            loader = PyPDFLoader(f.name)\n",
    "            docs = loader.load()\n",
    "            splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "            all_docs.extend(splitter.split_documents(docs))\n",
    "        log_text += log(f\"ì´ {len(pdf_files)}ê°œ PDFì—ì„œ {len(all_docs)}ê°œ ì²­í¬ ìƒì„±.\")\n",
    "        progress(0.85, desc=\"ChromaDB ì €ì¥ ì¤‘...\")\n",
    "        STATE.rag_vectorstore = Chroma.from_documents(\n",
    "            documents=all_docs,\n",
    "            embedding=STATE.rag_embeddings,\n",
    "            persist_directory=DEFAULT_PERSIST_DIR\n",
    "        )\n",
    "        log_text += log(\"ë²¡í„° DB ìƒ‰ì¸ ì™„ë£Œ. ì§ˆë¬¸ ê°€ëŠ¥.\")\n",
    "        return log_text, gr.update(interactive=True)\n",
    "    except Exception as e:\n",
    "        return f\"ìƒ‰ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\", gr.update(interactive=False)\n",
    "\n",
    "def handle_rag_chat_stream(message: str, history: list, temperature: float):\n",
    "    if not STATE.rag_vectorstore:\n",
    "        yield \"ë¨¼ì € PDF íŒŒì¼ì„ ìƒ‰ì¸í•´ì£¼ì„¸ìš”.\"; return\n",
    "    llm = STATE.get_llm(model_name=HEAVY_MODEL, temperature=temperature, num_predict=512)\n",
    "    retriever = STATE.rag_vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "    history.append({\"role\":\"user\",\"content\":message})\n",
    "    docs = retriever.invoke(message)\n",
    "    STATE.last_rag_sources = [\n",
    "        {\"source\": d.metadata.get('source','N/A'), \"page\": d.metadata.get('page','N/A'), \"content\": d.page_content}\n",
    "        for d in docs\n",
    "    ]\n",
    "    system_prompt = \"ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì— ê¸°ë°˜í•´ ì •í™•íˆ ë‹µí•˜ì„¸ìš”. ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë‹µí•˜ê¸°.\"\n",
    "    prompt_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"system\", \"ì»¨í…ìŠ¤íŠ¸:\\n{context}\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ])\n",
    "    chain = create_stuff_documents_chain(llm, prompt_template)\n",
    "    history.append({\"role\":\"assistant\",\"content\":\"\"})\n",
    "    resp = \"\"\n",
    "    for ch in chain.stream({\"input\": message, \"context\": docs}):\n",
    "        resp += ch; history[-1][\"content\"] = resp; yield history\n",
    "\n",
    "def update_rag_sources_display():\n",
    "    if not STATE.last_rag_sources: return \"ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "    md = \"### ğŸ“š ë‹µë³€ì— ì°¸ì¡°ëœ ë¬¸ì„œ\\n\\n\"\n",
    "    for i, s in enumerate(STATE.last_rag_sources):\n",
    "        md += f\"**[ì¶œì²˜ {i+1}]**\\n- **íŒŒì¼**: `{os.path.basename(s['source'])}` (í˜ì´ì§€: {s['page']})\\n- **ë‚´ìš© ì¼ë¶€**: {s['content'][:200]}...\\n\\n\"\n",
    "    return md\n",
    "\n",
    "# =============================\n",
    "# 4) ğŸ–¼ï¸ ì´ë¯¸ì§€ ë¶„ì„\n",
    "# =============================\n",
    "def handle_single_image_analysis(image, prompt: str, vlm_model: str, use_refine: bool, temperature: float):\n",
    "    if image is None: return \"ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.\", \"\"\n",
    "    analysis = call_vlm([image.name], prompt, vlm_model)\n",
    "    if not use_refine: return analysis, analysis\n",
    "    llm = STATE.get_llm(model_name=FAST_MODEL, temperature=temperature, num_predict=200)\n",
    "    refined = llm.invoke(f\"ë‹¤ìŒ ë¶„ì„ì„ ê°„ê²°í•˜ê³  ì •ë³´ê°€ í’ë¶€í•˜ê²Œ ì •ë¦¬:\\n{analysis}\").content\n",
    "    return refined, analysis\n",
    "\n",
    "def handle_image_comparison(image1, image2, prompt: str, vlm_model: str, use_refine: bool, temperature: float):\n",
    "    if image1 is None or image2 is None: return \"ë‘ ì´ë¯¸ì§€ë¥¼ ëª¨ë‘ ì—…ë¡œë“œí•˜ì„¸ìš”.\", \"\"\n",
    "    comparison = call_vlm([image1.name, image2.name], prompt, vlm_model)\n",
    "    if not use_refine: return comparison, comparison\n",
    "    llm = STATE.get_llm(model_name=FAST_MODEL, temperature=temperature, num_predict=220)\n",
    "    refined = llm.invoke(f\"ë‹¤ìŒ ë¹„êµê²°ê³¼ë¥¼ í•­ëª©í™”í•´ ì •ë¦¬:\\n{comparison}\").content\n",
    "    return refined, comparison\n",
    "\n",
    "# =============================\n",
    "# 5) ğŸ› ï¸ ê³ ê¸‰ ìƒì„±ê¸°\n",
    "# =============================\n",
    "def handle_mermaid_generation(idea: str, diagram_type: str, use_llm: bool):\n",
    "    if not idea.strip(): return \"\", \"\", \"ì•„ì´ë””ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”.\"\n",
    "    if not use_llm:\n",
    "        code, status = idea, \"LLM ë¯¸ì‚¬ìš©\"\n",
    "    else:\n",
    "        llm = STATE.get_llm(model_name=FAST_MODEL, temperature=0.1, num_predict=192)\n",
    "        sys = f\"Mermaid {diagram_type} íƒ€ì…ì˜ 'ìœ íš¨í•œ' ì½”ë“œë§Œ ì½”ë“œë¸”ë¡ìœ¼ë¡œ ì¶œë ¥. ì„¤ëª… ê¸ˆì§€.\"\n",
    "        resp = llm.invoke([SystemMessage(content=sys), HumanMessage(content=idea)]).content\n",
    "        m = re.search(r\"```mermaid\\s*([\\s\\S]*?)```\", resp)\n",
    "        if m: code, status = m.group(1).strip(), \"Mermaid ì½”ë“œ ìƒì„± ì™„ë£Œ.\"\n",
    "        else:\n",
    "            code = re.sub(r\"^.*?mermaid\", \"\", resp, flags=re.S|re.I).strip()\n",
    "            status = \"ì½”ë“œ ë¸”ë¡ ë¯¸ê²€ì¶œ â†’ ë³¸ë¬¸ì—ì„œ ì¶”ì¶œ\"\n",
    "    return f\"```mermaid\\n{code}\\n```\", code, status\n",
    "\n",
    "def handle_sd_prompt_generation(idea: str):\n",
    "    if not idea.strip(): return \"\", \"\", \"ì•„ì´ë””ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”.\"\n",
    "    llm = STATE.get_llm(model_name=FAST_MODEL, temperature=0.6, num_predict=160)\n",
    "    sys = \"Stable Diffusion í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´. JSONìœ¼ë¡œë§Œ ì‘ë‹µí•˜ê³  keys=['prompt','negative_prompt'] í¬í•¨.\"\n",
    "    resp = llm.invoke([SystemMessage(content=sys), HumanMessage(content=f\"ì•„ì´ë””ì–´: {idea}\")]).content\n",
    "    data = safe_json_extract(resp)\n",
    "    if isinstance(data, dict):\n",
    "        return data.get(\"prompt\", idea), data.get(\"negative_prompt\",\"low quality, blurry\"), \"í”„ë¡¬í”„íŠ¸ ìƒì„± ì™„ë£Œ.\"\n",
    "    return idea, \"\", \"JSON íŒŒì‹± ì‹¤íŒ¨. ì›ë³¸ ì‘ë‹µ:\\n\"+resp\n",
    "\n",
    "def handle_structured_ocr(image, use_llm: bool):\n",
    "    if image is None: return None, \"\", None, \"ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.\"\n",
    "    # ì§€ì—° ì„í¬íŠ¸\n",
    "    try:\n",
    "        import easyocr\n",
    "    except Exception:\n",
    "        return None, \"EasyOCR ë¯¸ì„¤ì¹˜ ë˜ëŠ” ë¡œë”© ì‹¤íŒ¨.\", None, \"\"\n",
    "    reader = easyocr.Reader(['ko','en'])\n",
    "    np_img = np.array(image)\n",
    "    results = reader.readtext(np_img)\n",
    "    overlay = image.copy(); draw = ImageDraw.Draw(overlay)\n",
    "    full_text = \"\\n\".join([r[1] for r in results])\n",
    "    for (bbox, text, prob) in results:\n",
    "        draw.polygon([tuple(p) for p in bbox], outline=\"cyan\", width=2)\n",
    "    if not use_llm: return overlay, full_text, None, \"LLM ë¯¸ì‚¬ìš©\"\n",
    "    llm = STATE.get_llm(model_name=FAST_MODEL, temperature=0.1, num_predict=220)\n",
    "    resp = llm.invoke([\n",
    "        SystemMessage(content=\"OCR í…ìŠ¤íŠ¸ë¥¼ êµ¬ì¡°í™” JSONìœ¼ë¡œ ë³€í™˜. í‘œë©´ êµ¬ì¡° ì¸ì‹ ì‹œ rows/columns or items ì‚¬ìš©.\"),\n",
    "        HumanMessage(content=full_text)\n",
    "    ]).content\n",
    "    data = safe_json_extract(resp); df = None\n",
    "    if isinstance(data, dict):\n",
    "        if 'items' in data and isinstance(data['items'], list):\n",
    "            df = pd.DataFrame(data['items'])\n",
    "        elif 'rows' in data and 'columns' in data:\n",
    "            df = pd.DataFrame(data['rows'], columns=data['columns'])\n",
    "    return overlay, full_text, df, (json.dumps(data, ensure_ascii=False, indent=2) if data else \"\")\n",
    "\n",
    "# =============================\n",
    "# 6) ğŸŒ ì›¹ & ë„êµ¬\n",
    "# =============================\n",
    "def handle_sequential_thinking(query: str, temperature: float, progress=gr.Progress()):\n",
    "    if not query.strip(): return \"\", \"\", \"\", \"ì§ˆì˜ë¥¼ ì…ë ¥í•˜ì„¸ìš”.\"\n",
    "    llm = STATE.get_llm(model_name=FAST_MODEL, temperature=temperature, num_predict=256)\n",
    "    progress(0.1, desc=\"[1/3] ë¶„ì„ ì¤‘...\")\n",
    "    analysis = llm.invoke([SystemMessage(content=\"í•µì‹¬ ìŸì ì„ 3~5ì¤„ë¡œ.\"), HumanMessage(content=query)]).content\n",
    "    progress(0.5, desc=\"[2/3] ìµœì¢… ë‹µë³€ ìƒì„± ì¤‘...\")\n",
    "    answer = llm.invoke([SystemMessage(content=\"ê°„ê²°Â·ì •í™•Â·êµ¬ì¡°í™”ëœ ë‹µë³€ 8~12ì¤„.\"), HumanMessage(content=f\"{query}\\n\\në¶„ì„:{analysis}\")]).content\n",
    "    progress(0.8, desc=\"[3/3] ê²€ì¦ ì¤‘...\")\n",
    "    reflection = llm.invoke([SystemMessage(content=\"ëª¨í˜¸ì„±/ëˆ„ë½/ê³¼ì¥ ì ê²€ í›„ 3ì¤„ ì œì•ˆ.\"), HumanMessage(content=f\"ë‹µë³€:{answer}\")]).content\n",
    "    return analysis, answer, reflection, \"ëª¨ë“  ë‹¨ê³„ ì™„ë£Œ.\"\n",
    "\n",
    "def handle_web_rag(query: str, temperature: float, progress=gr.Progress()):\n",
    "    llm = STATE.get_llm(model_name=FAST_MODEL, temperature=temperature, num_predict=256)\n",
    "    progress(0, desc=\"ì›¹ ê²€ìƒ‰ ì¤‘...\")\n",
    "    search = DuckDuckGoSearchResults()\n",
    "    search_results = search.run(query)\n",
    "    links = re.findall(r'https?://[^\\s,```<>\"]+', search_results)\n",
    "    if not links: return \"ê´€ë ¨ ì›¹ í˜ì´ì§€ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\", search_results, \"\"\n",
    "    links = links[:2]  # ì†ë„ ìœ„í•´ ìƒìœ„ 2ê°œë§Œ\n",
    "    progress(0.35, desc=f\"{len(links)}ê°œ ë§í¬ ìˆ˜ì§‘ ì¤‘...\")\n",
    "    try:\n",
    "        # bs_kwargsëŠ” ë²„ì „ë³„ ì°¨ì´ê°€ ìˆì–´ ì œê±°(ì•ˆì •ì„± ìš°ì„ )\n",
    "        loader = WebBaseLoader(web_paths=links)\n",
    "        docs = loader.load()\n",
    "        ctx = \"\\n\\n---\\n\\n\".join([d.page_content for d in docs])\n",
    "    except Exception as e:\n",
    "        ctx = f\"ì½˜í…ì¸  ìˆ˜ì§‘ ì‹¤íŒ¨: {e}\"\n",
    "    progress(0.7, desc=\"ë‹µë³€ ìƒì„± ì¤‘...\")\n",
    "    resp = llm.invoke([SystemMessage(content=\"ì›¹ ê²°ê³¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•Â·ê°„ê²°íˆ ë‹µë³€.\"), HumanMessage(content=f\"ê²€ìƒ‰ ê²°ê³¼:\\n{ctx}\\n\\nì§ˆë¬¸: {query}\")]).content\n",
    "    return resp, search_results, \"\\n\".join(links)\n",
    "\n",
    "# =============================\n",
    "# 7) Gradio UI\n",
    "# =============================\n",
    "def build_ui():\n",
    "    with gr.Blocks(theme=gr.themes.Soft(), title=\"DeepSeek-R1 AI Toolkit\") as demo:\n",
    "        gr.Markdown(\"# ğŸš€ DeepSeek-R1 AI Toolkit (All-in-One)\")\n",
    "        gr.Markdown(\"38ê°œ ì˜ˆì œë¥¼ í†µí•©í•œ ë¡œì»¬ AI ì• í”Œë¦¬ì¼€ì´ì…˜ì…ë‹ˆë‹¤. (ì†ë„ ìµœì í™”)\")\n",
    "\n",
    "        # ì±—ë´‡\n",
    "        with gr.Tab(\"ğŸ’¬ ì±—ë´‡\"):\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=1):\n",
    "                    persona = gr.Dropdown(list(PERSONAS.keys()), value=\"ê¸°ë³¸ ìƒë‹´ì‚¬\", label=\"í˜ë¥´ì†Œë‚˜ ì„ íƒ\")\n",
    "                    temp = gr.Slider(0.0, 1.5, value=0.7, step=0.1, label=\"Temperature\")\n",
    "                with gr.Column(scale=3):\n",
    "                    win = gr.Chatbot(height=500, label=\"ëŒ€í™”ì°½\", type=\"messages\")\n",
    "                    with gr.Row():\n",
    "                        msg = gr.Textbox(label=\"ë©”ì‹œì§€ ì…ë ¥\", placeholder=\"ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”...\", scale=4)\n",
    "                        btn = gr.Button(\"ì „ì†¡\", variant=\"primary\", scale=1)\n",
    "                        clr = gr.Button(\"ì´ˆê¸°í™”\")\n",
    "\n",
    "            def on_submit(m, h, p, t):\n",
    "                for nh in handle_chatbot_stream(m, h, p, t):\n",
    "                    yield nh, \"\"\n",
    "            msg.submit(on_submit, [msg, win, persona, temp], [win, msg])\n",
    "            btn.click(on_submit, [msg, win, persona, temp], [win, msg])\n",
    "            clr.click(lambda: [], None, win)\n",
    "\n",
    "        # PDF & ë¬¸ì„œ\n",
    "        with gr.Tab(\"ğŸ“„ PDF & ë¬¸ì„œ\"):\n",
    "            with gr.Tabs():\n",
    "                with gr.TabItem(\"PDF ìš”ì•½\"):\n",
    "                    with gr.Row():\n",
    "                        f_in = gr.File(label=\"PDF íŒŒì¼\", file_types=[\".pdf\"])\n",
    "                        with gr.Column():\n",
    "                            chunk = gr.Slider(1000, 8000, value=4000, step=500, label=\"ì²­í¬ í¬ê¸°\")\n",
    "                            t = gr.Slider(0.0, 1.0, value=0.2, step=0.1, label=\"Temperature\")\n",
    "                            run = gr.Button(\"ìš”ì•½ ì‹¤í–‰\", variant=\"primary\")\n",
    "                    out = gr.Textbox(label=\"ìš”ì•½ ê²°ê³¼\", lines=15)\n",
    "                    with gr.Accordion(\"ì „ì²´ í…ìŠ¤íŠ¸\", open=False):\n",
    "                        raw = gr.Textbox(label=\"ì›ë³¸ í…ìŠ¤íŠ¸\", lines=10)\n",
    "                    run.click(handle_pdf_summary, [f_in, chunk, t], [out, raw])\n",
    "\n",
    "                with gr.TabItem(\"í…ìŠ¤íŠ¸/ì´ë¯¸ì§€ ì¶”ì¶œ\"):\n",
    "                    with gr.Row():\n",
    "                        f_in2 = gr.File(label=\"PDF íŒŒì¼\", file_types=[\".pdf\"])\n",
    "                        with gr.Column():\n",
    "                            head = gr.Slider(0, 200, value=60, label=\"í—¤ë” ë†’ì´ (px)\")\n",
    "                            foot = gr.Slider(0, 200, value=60, label=\"í‘¸í„° ë†’ì´ (px)\")\n",
    "                            run2 = gr.Button(\"ì¶”ì¶œ ì‹¤í–‰\", variant=\"primary\")\n",
    "                    with gr.Row():\n",
    "                        tx = gr.Textbox(label=\"ì¶”ì¶œëœ í…ìŠ¤íŠ¸\", lines=15, scale=2)\n",
    "                        with gr.Column(scale=1):\n",
    "                            gal = gr.Gallery(label=\"ì¶”ì¶œëœ ì´ë¯¸ì§€\", height=400)\n",
    "                            pth = gr.Textbox(label=\"ì €ì¥ ê²½ë¡œ\")\n",
    "                    run2.click(handle_pdf_extraction, [f_in2, head, foot], [tx, gal, pth])\n",
    "\n",
    "                with gr.TabItem(\"í‘œ ì¶”ì¶œ\"):\n",
    "                    f_tbl = gr.File(label=\"PDF íŒŒì¼\", file_types=[\".pdf\"])\n",
    "                    run3 = gr.Button(\"í‘œ ì¶”ì¶œ ì‹¤í–‰\", variant=\"primary\")\n",
    "                    df = gr.Dataframe(label=\"ì¶”ì¶œëœ í‘œ\")\n",
    "                    logbx = gr.Textbox(label=\"ì²˜ë¦¬ ë¡œê·¸\", lines=5)\n",
    "                    xfile = gr.File(label=\"ì—‘ì…€ ë‹¤ìš´ë¡œë“œ\")\n",
    "                    run3.click(handle_pdf_table_extraction, [f_tbl], [df, logbx, xfile])\n",
    "\n",
    "        # RAG\n",
    "        with gr.Tab(\"ğŸ§  RAG ì±—ë´‡\"):\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=1):\n",
    "                    gr.Markdown(\"##### 1. ë¬¸ì„œ ìƒ‰ì¸\")\n",
    "                    f_multi = gr.File(label=\"PDF íŒŒì¼ë“¤\", file_count=\"multiple\", file_types=[\".pdf\"])\n",
    "                    cs = gr.Slider(200, 2000, value=1000, label=\"ì²­í¬ í¬ê¸°\")\n",
    "                    co = gr.Slider(0, 500, value=100, label=\"ì²­í¬ ì¤‘ì²©\")\n",
    "                    idx = gr.Button(\"ìƒ‰ì¸ ì‹œì‘\", variant=\"primary\")\n",
    "                    stat = gr.Textbox(label=\"ìƒ‰ì¸ ìƒíƒœ\", lines=10)\n",
    "                with gr.Column(scale=2):\n",
    "                    gr.Markdown(\"##### 2. ë¬¸ì„œ ê¸°ë°˜ ëŒ€í™”\")\n",
    "                    rag_chat = gr.Chatbot(height=450, label=\"ëŒ€í™”ì°½\", type=\"messages\")\n",
    "                    with gr.Accordion(\"ì°¸ì¡° ë¬¸ì„œ\", open=False):\n",
    "                        rag_src = gr.Markdown()\n",
    "                    rtemp = gr.Slider(0.0, 1.0, value=0.3, label=\"Temperature\")\n",
    "                    rmsg = gr.Textbox(label=\"ì§ˆë¬¸\", placeholder=\"ë¬¸ì„œì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”...\", interactive=False)\n",
    "                    rsend = gr.Button(\"ì „ì†¡\", variant=\"primary\")\n",
    "\n",
    "            idx.click(handle_rag_indexing, [f_multi, cs, co], [stat, rmsg])\n",
    "\n",
    "            def on_rag_submit(m, h, t):\n",
    "                for nh in handle_rag_chat_stream(m, h, t):\n",
    "                    yield nh, \"\"\n",
    "            rmsg.submit(on_rag_submit, [rmsg, rag_chat, rtemp], [rag_chat, rmsg]).then(update_rag_sources_display, None, rag_src)\n",
    "            rsend.click(on_rag_submit, [rmsg, rag_chat, rtemp], [rag_chat, rmsg]).then(update_rag_sources_display, None, rag_src)\n",
    "\n",
    "        # ì´ë¯¸ì§€ ë¶„ì„\n",
    "        with gr.Tab(\"ğŸ–¼ï¸ ì´ë¯¸ì§€ ë¶„ì„\"):\n",
    "            vlm_model = gr.Dropdown([VLM_MODEL, \"llava:latest\"], value=VLM_MODEL, label=\"VLM ëª¨ë¸\")\n",
    "            with gr.Tabs():\n",
    "                with gr.TabItem(\"ë‹¨ì¼ ì´ë¯¸ì§€\"):\n",
    "                    with gr.Row():\n",
    "                        img_single = gr.Image(type=\"pil\", label=\"ì´ë¯¸ì§€\")\n",
    "                        with gr.Column():\n",
    "                            iprompt = gr.Textbox(label=\"í”„ë¡¬í”„íŠ¸\", value=\"ì´ ì´ë¯¸ì§€ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”.\", lines=3)\n",
    "                            irefine = gr.Checkbox(label=\"LLMìœ¼ë¡œ ì •ì œ\", value=True)\n",
    "                            itemp = gr.Slider(0.0, 1.0, value=0.7, label=\"Temperature\")\n",
    "                            ibtn = gr.Button(\"ë¶„ì„\", variant=\"primary\")\n",
    "                    iout = gr.Textbox(label=\"ë¶„ì„ ê²°ê³¼\", lines=10)\n",
    "                    with gr.Accordion(\"VLM ì›ë³¸\", open=False):\n",
    "                        iraw = gr.Textbox(label=\"ì›ë³¸ ê²°ê³¼\")\n",
    "                    ibtn.click(handle_single_image_analysis, [img_single, iprompt, vlm_model, irefine, itemp], [iout, iraw])\n",
    "\n",
    "                with gr.TabItem(\"ì´ë¯¸ì§€ ë¹„êµ\"):\n",
    "                    with gr.Row():\n",
    "                        img1 = gr.Image(type=\"pil\", label=\"ì´ë¯¸ì§€ 1\")\n",
    "                        img2 = gr.Image(type=\"pil\", label=\"ì´ë¯¸ì§€ 2\")\n",
    "                    cprompt = gr.Textbox(label=\"í”„ë¡¬í”„íŠ¸\", value=\"ë‘ ì´ë¯¸ì§€ë¥¼ ë¹„êµí•´ì£¼ì„¸ìš”.\", lines=3)\n",
    "                    crefine = gr.Checkbox(label=\"LLMìœ¼ë¡œ ì •ì œ\", value=True)\n",
    "                    ctemp = gr.Slider(0.0, 1.0, value=0.7, label=\"Temperature\")\n",
    "                    cbtn = gr.Button(\"ë¹„êµ\", variant=\"primary\")\n",
    "                    cout = gr.Textbox(label=\"ë¹„êµ ê²°ê³¼\", lines=10)\n",
    "                    with gr.Accordion(\"VLM ì›ë³¸\", open=False):\n",
    "                        craw = gr.Textbox(label=\"ì›ë³¸ ê²°ê³¼\")\n",
    "                    cbtn.click(handle_image_comparison, [img1, img2, cprompt, vlm_model, crefine, ctemp], [cout, craw])\n",
    "\n",
    "        # ê³ ê¸‰ ìƒì„±ê¸°\n",
    "        with gr.Tab(\"ğŸ› ï¸ ê³ ê¸‰ ìƒì„±ê¸°\"):\n",
    "            with gr.Tabs():\n",
    "                with gr.TabItem(\"Mermaid ë‹¤ì´ì–´ê·¸ë¨\"):\n",
    "                    with gr.Row():\n",
    "                        with gr.Column():\n",
    "                            m_idea = gr.Textbox(label=\"ì•„ì´ë””ì–´\", lines=5)\n",
    "                            m_type = gr.Dropdown([\"flowchart\",\"sequenceDiagram\",\"gantt\",\"pie\",\"classDiagram\"], value=\"flowchart\", label=\"ë‹¤ì´ì–´ê·¸ë¨ íƒ€ì…\")\n",
    "                            m_llm = gr.Checkbox(value=True, label=\"LLM ì‚¬ìš©\")\n",
    "                            m_btn = gr.Button(\"ìƒì„±\", variant=\"primary\")\n",
    "                        with gr.Column():\n",
    "                            m_out = gr.Markdown(label=\"ë Œë”ë§\")\n",
    "                            m_code = gr.Code(label=\"ì½”ë“œ\", language=\"markdown\")\n",
    "                            m_status = gr.Textbox(label=\"ìƒíƒœ\")\n",
    "                    m_btn.click(handle_mermaid_generation, [m_idea, m_type, m_llm], [m_out, m_code, m_status])\n",
    "\n",
    "                with gr.TabItem(\"SD í”„ë¡¬í”„íŠ¸\"):\n",
    "                    sd_idea = gr.Textbox(label=\"ì•„ì´ë””ì–´\", lines=3)\n",
    "                    sd_btn = gr.Button(\"ìƒì„±\", variant=\"primary\")\n",
    "                    sd_prompt = gr.Textbox(label=\"í”„ë¡¬í”„íŠ¸\", lines=4)\n",
    "                    sd_negative = gr.Textbox(label=\"ë„¤ê±°í‹°ë¸Œ í”„ë¡¬í”„íŠ¸\", lines=2)\n",
    "                    sd_status = gr.Textbox(label=\"ìƒíƒœ\")\n",
    "                    sd_btn.click(handle_sd_prompt_generation, [sd_idea], [sd_prompt, sd_negative, sd_status])\n",
    "\n",
    "                with gr.TabItem(\"êµ¬ì¡°í™” OCR\"):\n",
    "                    ocr_img = gr.Image(type=\"pil\", label=\"ì´ë¯¸ì§€\")\n",
    "                    ocr_use_llm = gr.Checkbox(value=True, label=\"LLMìœ¼ë¡œ êµ¬ì¡°í™”\")\n",
    "                    ocr_btn = gr.Button(\"OCR ì‹¤í–‰\", variant=\"primary\")\n",
    "                    with gr.Row():\n",
    "                        ocr_overlay = gr.Image(label=\"ì¸ì‹ ì˜ì—­\")\n",
    "                        ocr_df = gr.Dataframe(label=\"êµ¬ì¡°í™” ë°ì´í„°\")\n",
    "                    with gr.Accordion(\"í…ìŠ¤íŠ¸ ë° JSON\", open=False):\n",
    "                        ocr_text = gr.Textbox(label=\"í…ìŠ¤íŠ¸\")\n",
    "                        ocr_json = gr.Code(label=\"JSON\", language=\"json\")\n",
    "                    ocr_btn.click(handle_structured_ocr, [ocr_img, ocr_use_llm], [ocr_overlay, ocr_text, ocr_df, ocr_json])\n",
    "\n",
    "        # ì›¹ & ë„êµ¬\n",
    "        with gr.Tab(\"ğŸŒ ì›¹ & ë„êµ¬\"):\n",
    "            wq = gr.Textbox(label=\"ì›¹ ê²€ìƒ‰ ì§ˆë¬¸\", lines=2)\n",
    "            wt = gr.Slider(0.0, 1.0, value=0.7, label=\"Temperature\")\n",
    "            wb = gr.Button(\"ê²€ìƒ‰ ë° ë‹µë³€\", variant=\"primary\")\n",
    "            wa = gr.Textbox(label=\"ë‹µë³€\", lines=10)\n",
    "            with gr.Accordion(\"ê²€ìƒ‰ ì •ë³´\", open=False):\n",
    "                wr = gr.Textbox(label=\"ê²€ìƒ‰ ê²°ê³¼\")\n",
    "                wl = gr.Textbox(label=\"ì°¸ì¡° URL\")\n",
    "            wb.click(handle_web_rag, [wq, wt], [wa, wr, wl])\n",
    "\n",
    "        # ìˆœì°¨ì  ì‚¬ê³ \n",
    "        with gr.Tab(\"ğŸ¤” ìˆœì°¨ì  ì‚¬ê³ \"):\n",
    "            sq = gr.Textbox(label=\"ì§ˆë¬¸\", lines=3)\n",
    "            st = gr.Slider(0.0, 1.0, value=0.7, label=\"Temperature\")\n",
    "            sb = gr.Button(\"ì‚¬ê³  ì‹œì‘\", variant=\"primary\")\n",
    "            s_status = gr.Textbox(label=\"ìƒíƒœ\")\n",
    "            with gr.Row():\n",
    "                s_a = gr.Markdown(label=\"ë¶„ì„\")\n",
    "                s_ans = gr.Markdown(label=\"ë‹µë³€\")\n",
    "                s_ref = gr.Markdown(label=\"ê²€ì¦\")\n",
    "            sb.click(handle_sequential_thinking, [sq, st], [s_a, s_ans, s_ref, s_status])\n",
    "\n",
    "    return demo\n",
    "\n",
    "# =============================\n",
    "# ë©”ì¸ ì‹¤í–‰\n",
    "# =============================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ğŸš€ Gradio ì•± ì‹¤í–‰ ì¤‘...\")\n",
    "    app = build_ui()\n",
    "    # â–¼ Gradio 4.x: queue ì¸ì ì¶•ì†Œ(í˜¸í™˜)\n",
    "    app = app.queue(max_size=32)\n",
    "    # share=False ê°€ í„°ë„ë§ ì§€ì—°ì„ ì¤„ì—¬ í›¨ì”¬ ë¹ ë¦„\n",
    "    app.launch(server_name=\"0.0.0.0\", server_port=7860, inbrowser=True, share=False, show_error=True)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
