{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8f4332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ ÌôòÍ≤Ω Ï†êÍ≤Ä Ï§ë...\n",
      "üêç ÌïÑÏàò Ìå®ÌÇ§ÏßÄ ÌôïÏù∏\n",
      "üì• install beautifulsoup4\n",
      "üì• install duckduckgo-search\n",
      "ü¶ô Ollama Ï§ÄÎπÑ\n",
      "üì• ÌïÑÏöîÌïú Î™®Îç∏ pull\n",
      "üî• Î™®Îç∏ ÏõúÏóÖ\n",
      "‚úÖ ÌôòÍ≤Ω Ï§ÄÎπÑ ÏôÑÎ£å\n",
      "üöÄ Gradio Ïï± Ïã§Ìñâ Ï§ë...\n",
      "* Running on local URL:  http://0.0.0.0:7869\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\uvicorn\\protocols\\http\\httptools_impl.py\", line 409, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.scope, self.receive, self.send\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\uvicorn\\middleware\\proxy_headers.py\", line 60, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\fastapi\\applications.py\", line 1133, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\applications.py\", line 113, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\middleware\\errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\middleware\\errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\gradio\\brotli_middleware.py\", line 74, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\gradio\\route_utils.py\", line 882, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\middleware\\exceptions.py\", line 63, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\fastapi\\middleware\\asyncexitstack.py\", line 18, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\routing.py\", line 716, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\routing.py\", line 736, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\routing.py\", line 290, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\fastapi\\routing.py\", line 123, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\fastapi\\routing.py\", line 109, in app\n",
      "    response = await f(request)\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\fastapi\\routing.py\", line 387, in app\n",
      "    raw_response = await run_endpoint_function(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<3 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\fastapi\\routing.py\", line 288, in run_endpoint_function\n",
      "    return await dependant.call(**values)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\gradio\\routes.py\", line 1671, in get_upload_progress\n",
      "    await asyncio.wait_for(\n",
      "        file_upload_statuses.is_tracked(upload_id), timeout=3\n",
      "    )\n",
      "  File \"c:\\Python313\\Lib\\asyncio\\tasks.py\", line 507, in wait_for\n",
      "    return await fut\n",
      "           ^^^^^^^^^\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\gradio\\route_utils.py\", line 528, in is_tracked\n",
      "    return await self._signals[upload_id].wait()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python313\\Lib\\asyncio\\locks.py\", line 210, in wait\n",
      "    fut = self._get_loop().create_future()\n",
      "          ~~~~~~~~~~~~~~^^\n",
      "  File \"c:\\Python313\\Lib\\asyncio\\mixins.py\", line 20, in _get_loop\n",
      "    raise RuntimeError(f'{self!r} is bound to a different event loop')\n",
      "RuntimeError: <asyncio.locks.Event object at 0x0000018C55E1CC50 [unset]> is bound to a different event loop\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\PIL\\ImagePalette.py\", line 165, in getcolor\n",
      "    return self.colors[color]\n",
      "           ~~~~~~~~~~~^^^^^^^\n",
      "KeyError: (0, 255, 255)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\gradio\\queueing.py\", line 759, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\gradio\\route_utils.py\", line 354, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<11 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\gradio\\blocks.py\", line 2116, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<8 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\gradio\\blocks.py\", line 1623, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        fn, *processed_input, limiter=self.limiter\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        func, args, abandon_on_cancel=abandon_on_cancel, limiter=limiter\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2476, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\gradio\\utils.py\", line 915, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"C:\\Users\\kimru\\AppData\\Local\\Temp\\ipykernel_32272\\3483671045.py\", line 393, in handle_structured_ocr\n",
      "    draw.polygon([tuple(p) for p in bbox], outline=\"cyan\", width=2)\n",
      "    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\PIL\\ImageDraw.py\", line 357, in polygon\n",
      "    ink, fill_ink = self._getink(outline, fill)\n",
      "                    ~~~~~~~~~~~~^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\PIL\\ImageDraw.py\", line 160, in _getink\n",
      "    ink = self.palette.getcolor(ink, self._image)\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\PIL\\ImagePalette.py\", line 168, in getcolor\n",
      "    index = self._new_color_index(image, e)\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\PIL\\ImagePalette.py\", line 139, in _new_color_index\n",
      "    raise ValueError(msg) from e\n",
      "ValueError: cannot allocate more than 256 colors\n",
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\uvicorn\\protocols\\http\\httptools_impl.py\", line 409, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.scope, self.receive, self.send\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\uvicorn\\middleware\\proxy_headers.py\", line 60, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\fastapi\\applications.py\", line 1133, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\applications.py\", line 113, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\middleware\\errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\middleware\\errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\gradio\\brotli_middleware.py\", line 74, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\gradio\\route_utils.py\", line 882, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\middleware\\exceptions.py\", line 63, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\fastapi\\middleware\\asyncexitstack.py\", line 18, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\routing.py\", line 716, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\routing.py\", line 736, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\routing.py\", line 290, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\fastapi\\routing.py\", line 123, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\fastapi\\routing.py\", line 109, in app\n",
      "    response = await f(request)\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\fastapi\\routing.py\", line 387, in app\n",
      "    raw_response = await run_endpoint_function(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<3 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\fastapi\\routing.py\", line 288, in run_endpoint_function\n",
      "    return await dependant.call(**values)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\gradio\\routes.py\", line 1671, in get_upload_progress\n",
      "    await asyncio.wait_for(\n",
      "        file_upload_statuses.is_tracked(upload_id), timeout=3\n",
      "    )\n",
      "  File \"c:\\Python313\\Lib\\asyncio\\tasks.py\", line 507, in wait_for\n",
      "    return await fut\n",
      "           ^^^^^^^^^\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\gradio\\route_utils.py\", line 528, in is_tracked\n",
      "    return await self._signals[upload_id].wait()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python313\\Lib\\asyncio\\locks.py\", line 210, in wait\n",
      "    fut = self._get_loop().create_future()\n",
      "          ~~~~~~~~~~~~~~^^\n",
      "  File \"c:\\Python313\\Lib\\asyncio\\mixins.py\", line 20, in _get_loop\n",
      "    raise RuntimeError(f'{self!r} is bound to a different event loop')\n",
      "RuntimeError: <asyncio.locks.Event object at 0x0000018C55E1CC50 [unset]> is bound to a different event loop\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\uvicorn\\protocols\\http\\httptools_impl.py\", line 409, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.scope, self.receive, self.send\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\uvicorn\\middleware\\proxy_headers.py\", line 60, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\fastapi\\applications.py\", line 1133, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\applications.py\", line 113, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\middleware\\errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\middleware\\errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\gradio\\brotli_middleware.py\", line 74, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\gradio\\route_utils.py\", line 882, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\middleware\\exceptions.py\", line 63, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\fastapi\\middleware\\asyncexitstack.py\", line 18, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\routing.py\", line 716, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\routing.py\", line 736, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\routing.py\", line 290, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\fastapi\\routing.py\", line 123, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\fastapi\\routing.py\", line 109, in app\n",
      "    response = await f(request)\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\fastapi\\routing.py\", line 387, in app\n",
      "    raw_response = await run_endpoint_function(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<3 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\fastapi\\routing.py\", line 288, in run_endpoint_function\n",
      "    return await dependant.call(**values)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\gradio\\routes.py\", line 1671, in get_upload_progress\n",
      "    await asyncio.wait_for(\n",
      "        file_upload_statuses.is_tracked(upload_id), timeout=3\n",
      "    )\n",
      "  File \"c:\\Python313\\Lib\\asyncio\\tasks.py\", line 507, in wait_for\n",
      "    return await fut\n",
      "           ^^^^^^^^^\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\gradio\\route_utils.py\", line 528, in is_tracked\n",
      "    return await self._signals[upload_id].wait()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python313\\Lib\\asyncio\\locks.py\", line 210, in wait\n",
      "    fut = self._get_loop().create_future()\n",
      "          ~~~~~~~~~~~~~~^^\n",
      "  File \"c:\\Python313\\Lib\\asyncio\\mixins.py\", line 20, in _get_loop\n",
      "    raise RuntimeError(f'{self!r} is bound to a different event loop')\n",
      "RuntimeError: <asyncio.locks.Event object at 0x0000018C55E1CC50 [unset]> is bound to a different event loop\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\uvicorn\\protocols\\http\\httptools_impl.py\", line 409, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.scope, self.receive, self.send\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\uvicorn\\middleware\\proxy_headers.py\", line 60, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\fastapi\\applications.py\", line 1133, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\applications.py\", line 113, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\middleware\\errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\middleware\\errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\gradio\\brotli_middleware.py\", line 74, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\gradio\\route_utils.py\", line 882, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\middleware\\exceptions.py\", line 63, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\fastapi\\middleware\\asyncexitstack.py\", line 18, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\routing.py\", line 716, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\routing.py\", line 736, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\routing.py\", line 290, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\fastapi\\routing.py\", line 123, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\fastapi\\routing.py\", line 109, in app\n",
      "    response = await f(request)\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\fastapi\\routing.py\", line 387, in app\n",
      "    raw_response = await run_endpoint_function(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<3 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\fastapi\\routing.py\", line 288, in run_endpoint_function\n",
      "    return await dependant.call(**values)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\gradio\\routes.py\", line 1671, in get_upload_progress\n",
      "    await asyncio.wait_for(\n",
      "        file_upload_statuses.is_tracked(upload_id), timeout=3\n",
      "    )\n",
      "  File \"c:\\Python313\\Lib\\asyncio\\tasks.py\", line 507, in wait_for\n",
      "    return await fut\n",
      "           ^^^^^^^^^\n",
      "  File \"C:\\Users\\kimru\\AppData\\Roaming\\Python\\Python313\\site-packages\\gradio\\route_utils.py\", line 528, in is_tracked\n",
      "    return await self._signals[upload_id].wait()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python313\\Lib\\asyncio\\locks.py\", line 210, in wait\n",
      "    fut = self._get_loop().create_future()\n",
      "          ~~~~~~~~~~~~~~^^\n",
      "  File \"c:\\Python313\\Lib\\asyncio\\mixins.py\", line 20, in _get_loop\n",
      "    raise RuntimeError(f'{self!r} is bound to a different event loop')\n",
      "RuntimeError: <asyncio.locks.Event object at 0x0000018C55E1CC50 [unset]> is bound to a different event loop\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# üöÄ DeepSeek-R1 AI Toolkit (Îπ†Î•∏ Ïã§Ìñâ ÏµúÏ†ÅÌôî Î≤ÑÏ†Ñ) ‚Äî Gradio queue ÏàòÏ†ï\n",
    "# =============================\n",
    "\n",
    "# ---------- ÌôòÍ≤Ω/ÏÑ§Ïπò ÏµúÏÜåÌôî & Ï∫êÏã± ----------\n",
    "print(\"üì¶ ÌôòÍ≤Ω Ï†êÍ≤Ä Ï§ë...\")\n",
    "\n",
    "import os, shutil, subprocess, time, re, json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "def sh(cmd: str):\n",
    "    return subprocess.run(cmd, shell=True, check=False, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "\n",
    "# Java (tabula/camelot ÌïÑÏöî) ‚Äî ÏóÜÏùÑ ÎïåÎßå ÏÑ§Ïπò\n",
    "if shutil.which(\"java\") is None:\n",
    "    print(\"üëâ Java ÏÑ§Ïπò\")\n",
    "    sh(\"apt-get update -qq && apt-get install -y openjdk-11-jdk-headless\")\n",
    "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-11-openjdk-amd64'\n",
    "\n",
    "# pip Ìå®ÌÇ§ÏßÄ ‚Äî ÌïÑÏöîÌïú Í≤ÉÎßå & ÏóÜÏùÑ ÎïåÎßå\n",
    "def ensure(mod: str, pip_name: str = None):\n",
    "    pip_name = pip_name or mod\n",
    "    try:\n",
    "        __import__(mod)\n",
    "    except Exception:\n",
    "        print(f\"üì• install {pip_name}\")\n",
    "        sh(f\"pip install -q {pip_name}\")\n",
    "\n",
    "print(\"üêç ÌïÑÏàò Ìå®ÌÇ§ÏßÄ ÌôïÏù∏\")\n",
    "for mod, pipn in [\n",
    "    (\"gradio\",\"gradio\"),\n",
    "    (\"ollama\",\"ollama\"),\n",
    "    (\"fitz\",\"pymupdf\"),\n",
    "    (\"pandas\",\"pandas\"),\n",
    "    (\"PIL\",\"Pillow\"),\n",
    "    (\"numpy\",\"numpy\"),\n",
    "    (\"langchain_community\",\"langchain-community\"),\n",
    "    (\"langchain_chroma\",\"langchain-chroma\"),\n",
    "    (\"langchain_text_splitters\",\"langchain-text-splitters\"),\n",
    "    (\"beautifulsoup4\",\"beautifulsoup4\"),\n",
    "    (\"duckduckgo_search\",\"duckduckgo-search\"),\n",
    "]:\n",
    "    ensure(mod, pipn)\n",
    "\n",
    "# Ollama ÏÑ§Ïπò\n",
    "print(\"ü¶ô Ollama Ï§ÄÎπÑ\")\n",
    "if shutil.which(\"ollama\") is None:\n",
    "    sh(\"curl -fsSL https://ollama.com/install.sh | sh\")\n",
    "\n",
    "# ÏÑúÎ≤Ñ Î∞±Í∑∏ÎùºÏö¥Îìú\n",
    "subprocess.Popen([\"ollama\",\"serve\"])\n",
    "time.sleep(3)\n",
    "\n",
    "# Î™®Îç∏ Íµ¨ÏÑ±: Îπ†Î•∏ Î™®Îç∏ / Î¨¥Í±∞Ïö¥ Î™®Îç∏ / VLM\n",
    "FAST_MODEL  = \"qwen2.5:7b-instruct\"   # Îπ†Î•∏ ÏÉùÏÑ±/ÎèÑÍµ¨ ÌÉ≠\n",
    "HEAVY_MODEL = \"deepseek-r1\"           # RAG Îì± Í∏¥ Ï∂îÎ°† ÌïÑÏöî Ïãú\n",
    "VLM_MODEL   = \"qwen2.5-vl\"            # ÎπÑÏ†Ñ-ÌÖçÏä§Ìä∏\n",
    "EMB_MODEL   = \"nomic-embed-text\"      # ÏûÑÎ≤†Îî©\n",
    "\n",
    "print(\"üì• ÌïÑÏöîÌïú Î™®Îç∏ pull\")\n",
    "for m in [FAST_MODEL, HEAVY_MODEL, VLM_MODEL, EMB_MODEL]:\n",
    "    sh(f\"ollama pull {m}\")\n",
    "\n",
    "# Í∞ÑÎã® ÏõúÏóÖ(Ï≤´ ÌÜ†ÌÅ∞ ÏßÄÏó∞ Í∞êÏÜå)\n",
    "print(\"üî• Î™®Îç∏ ÏõúÏóÖ\")\n",
    "sh(f'''python - <<'PY'\n",
    "import ollama\n",
    "for m in [\"{FAST_MODEL}\", \"{HEAVY_MODEL}\"]:\n",
    "    try:\n",
    "        ollama.chat(model=m, messages=[{{\"role\":\"user\",\"content\":\"hi\"}}], options={{\"num_predict\":8}})\n",
    "    except Exception as e:\n",
    "        print(\"warmup err:\", m, e)\n",
    "PY''')\n",
    "print(\"‚úÖ ÌôòÍ≤Ω Ï§ÄÎπÑ ÏôÑÎ£å\")\n",
    "\n",
    "# =============================\n",
    "# üß† Î©îÏù∏ Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖò ÏΩîÎìú\n",
    "# =============================\n",
    "from typing import Optional, List, Dict\n",
    "import gradio as gr\n",
    "import ollama\n",
    "import fitz  # PyMuPDF\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader, WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "\n",
    "# ---------- Ï†ÑÏó≠ ----------\n",
    "class GlobalState:\n",
    "    def __init__(self):\n",
    "        self.cache = {}  # (model, temp, opts_key) -> ChatOllama\n",
    "        self.rag_vectorstore: Optional[Chroma] = None\n",
    "        self.rag_embeddings: Optional[OllamaEmbeddings] = None\n",
    "        self.last_rag_sources: List[Dict] = []\n",
    "\n",
    "    def get_llm(self, model_name=FAST_MODEL, temperature=0.7, **model_kwargs) -> ChatOllama:\n",
    "        # Ï£ºÏöî ÏÜçÎèÑÏòµÏÖò: num_predict(ÏùëÎãµÍ∏∏Ïù¥), num_ctx\n",
    "        if \"num_predict\" not in model_kwargs:\n",
    "            model_kwargs[\"num_predict\"] = 256\n",
    "        opts_key = tuple(sorted(model_kwargs.items()))\n",
    "        key = (model_name, float(temperature), opts_key)\n",
    "        if key not in self.cache:\n",
    "            self.cache[key] = ChatOllama(\n",
    "                model=model_name,\n",
    "                temperature=temperature,\n",
    "                model_kwargs=model_kwargs\n",
    "            )\n",
    "        return self.cache[key]\n",
    "\n",
    "STATE = GlobalState()\n",
    "DEFAULT_PERSIST_DIR = \"./chroma_db_store\"\n",
    "os.makedirs(DEFAULT_PERSIST_DIR, exist_ok=True)\n",
    "\n",
    "# ---------- Ìó¨Ìçº ----------\n",
    "def log(message: str) -> str:\n",
    "    ts = datetime.now().strftime('%H:%M:%S')\n",
    "    return f\"[{ts}] {message}\\n\"\n",
    "\n",
    "def strip_think_tags(text: str) -> str:\n",
    "    return re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL|re.IGNORECASE).strip()\n",
    "\n",
    "def safe_json_extract(text: str):\n",
    "    \"\"\"Îπ†Î•¥Í≥† ÏïàÏ†ÑÌïú JSON Ï∂îÏ∂ú\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return None\n",
    "        \n",
    "    # 1. ```json ... ``` ÏΩîÎìú Î∏îÎ°ù Î®ºÏ†Ä Ï∞æÍ∏∞\n",
    "    m = re.search(r\"```json\\s*(\\{[\\s\\S]*?\\}|\\[[\\s\\S]*?\\])\\s*```\", text, flags=re.DOTALL|re.IGNORECASE)\n",
    "    if m:\n",
    "        try: return json.loads(m.group(1))\n",
    "        except: pass # ÌååÏã± Ïã§Ìå® Ïãú Îã§Ïùå Îã®Í≥ÑÎ°ú ÎÑòÏñ¥Í∞ê\n",
    "\n",
    "    # 2. ``` (Ïñ∏Ïñ¥ Î™ÖÏãú ÏóÜÎäî) ÏΩîÎìú Î∏îÎ°ùÏù¥ÎÇò ÏùºÎ∞ò Í¥ÑÌò∏ {} [] Ï∞æÍ∏∞ (Í∏∞Ï°¥ Î°úÏßÅ Ïú†ÏßÄ)\n",
    "    text_wo = re.sub(r\"```(?!json)[\\s\\S]*?```\", \"\", text, flags=re.DOTALL|re.IGNORECASE)\n",
    "    cands = []\n",
    "    for p in (r\"\\{[\\s\\S]*?\\}\", r\"\\[[\\s\\S]*?\\]\"):\n",
    "        for mm in re.finditer(p, text_wo, flags=re.DOTALL):\n",
    "            cands.append(mm.group(0))\n",
    "            \n",
    "    for chunk in sorted(cands, key=len, reverse=True):\n",
    "        try: return json.loads(chunk)\n",
    "        except: pass\n",
    "        \n",
    "    try: return json.loads(text_wo.strip())\n",
    "    except: return None\n",
    "\n",
    "def call_vlm(image_paths: list, prompt: str, vlm_model: str) -> str:\n",
    "    try:\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt, \"images\": image_paths}]\n",
    "        res = ollama.chat(model=vlm_model, messages=messages, options={\"num_predict\":256})\n",
    "        return res.get(\"message\", {}).get(\"content\", \"VLM Ìò∏Ï∂ú Ïã§Ìå®\")\n",
    "    except Exception as e:\n",
    "        return f\"VLM Ïò§Î•ò: {e}\"\n",
    "\n",
    "# =============================\n",
    "# 1) üí¨ Ï±óÎ¥á\n",
    "# =============================\n",
    "PERSONAS = {\n",
    "    \"Í∏∞Î≥∏ ÏÉÅÎã¥ÏÇ¨\": \"ÎÑàÎäî ÏÇ¨Ïö©ÏûêÎ•º ÎèÑÏôÄÏ£ºÎäî ÏπúÏ†àÌïòÍ≥† Ïú†Îä•Ìïú ÏÉÅÎã¥ÏÇ¨Ïïº.\",\n",
    "    \"Î∞±ÏÑ§Í≥µÏ£º ÎßàÎ≤ïÍ±∞Ïö∏\": \"ÎÑàÎäî Î∞±ÏÑ§Í≥µÏ£º Ïù¥ÏïºÍ∏∞ ÏÜç ÎßàÎ≤ï Í±∞Ïö∏Ïù¥Ïïº. ÌíàÏúÑ ÏûàÍ≥† Ïö¥Ïú®Í∞ê ÏûàÍ≤å ÎãµÌï¥Ï§ò.\",\n",
    "    \"Ïú†ÏπòÏõêÏÉù\": \"ÎÑàÎäî 5ÏÇ¥ Ïú†ÏπòÏõêÏÉùÏù¥Ïïº. ÏßßÍ≥† Ïâ¨Ïö¥ ÎßêÎ°ú Í∑ÄÏóΩÍ≤å ÎåÄÎãµÌï¥.\",\n",
    "}\n",
    "\n",
    "def handle_chatbot_stream(message: str, history: list, persona: str, temperature: float):\n",
    "    if not message.strip(): return\n",
    "    llm = STATE.get_llm(model_name=FAST_MODEL, temperature=temperature, num_predict=256)\n",
    "    system_prompt = PERSONAS.get(persona, PERSONAS[\"Í∏∞Î≥∏ ÏÉÅÎã¥ÏÇ¨\"])\n",
    "    system_prompt += \" **ÎÑàÎäî ÏÇ¨Ïö©ÏûêÏùò Î™®Îì† ÏßàÎ¨∏Ïóê Î∞òÎìúÏãú ÌïúÍµ≠Ïñ¥Î°úÎßå ÎãµÎ≥ÄÌï¥Ïïº Ìï¥. Îã§Î•∏ Ïñ∏Ïñ¥(ÌäπÌûà Ï§ëÍµ≠Ïñ¥)Î•º ÏÇ¨Ïö©Ìï¥ÏÑúÎäî Ïïà Îèº.**\"\n",
    "    msgs = [SystemMessage(content=system_prompt)]\n",
    "    for t in history:\n",
    "        msgs.append(HumanMessage(content=t[\"content\"]) if t[\"role\"]==\"user\" else AIMessage(content=t[\"content\"]))\n",
    "    msgs.append(HumanMessage(content=message))\n",
    "    stream = llm.stream(msgs)\n",
    "    history.append({\"role\":\"user\",\"content\":message})\n",
    "    history.append({\"role\":\"assistant\",\"content\":\"\"})\n",
    "    for ch in stream:\n",
    "        history[-1][\"content\"] += ch.content\n",
    "        yield history\n",
    "\n",
    "# =============================\n",
    "# 2) üìÑ PDF & Î¨∏ÏÑú\n",
    "# =============================\n",
    "def handle_pdf_summary(pdf_file, chunk_size: int, temperature: float, progress=gr.Progress()):\n",
    "    if not pdf_file: return \"PDF ÌååÏùºÏùÑ ÏóÖÎ°úÎìúÌïòÏÑ∏Ïöî.\", \"\"\n",
    "    llm = STATE.get_llm(model_name=FAST_MODEL, temperature=temperature, num_predict=256)\n",
    "    progress(0, desc=\"PDFÏóêÏÑú ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú Ï§ë...\")\n",
    "    doc = fitz.open(pdf_file.name)\n",
    "    full_text = \"\\n\".join([p.get_text() for p in doc]); doc.close()\n",
    "    if not full_text.strip(): return \"PDFÏóêÏÑú ÌÖçÏä§Ìä∏Î•º Ï∂îÏ∂úÌï† Ïàò ÏóÜÏäµÎãàÎã§.\", full_text\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=int(chunk_size*0.1))\n",
    "    chunks = splitter.split_text(full_text)\n",
    "    system_prompt = \"Îã§Ïùå ÌÖçÏä§Ìä∏Îäî Î¨∏ÏÑúÏùò ÏùºÎ∂ÄÏûÖÎãàÎã§. ÌïµÏã¨ÏùÑ Ï†ïÌôï¬∑Í∞ÑÍ≤∞Ìûà ÏöîÏïΩÌïòÏÑ∏Ïöî.\"\n",
    "    summaries = []\n",
    "    for c in progress.tqdm(chunks, desc=\"Í∞Å Ï≤≠ÌÅ¨ ÏöîÏïΩ Ï§ë...\"):\n",
    "        s = llm.invoke([SystemMessage(content=system_prompt), HumanMessage(content=c)]).content\n",
    "        summaries.append(s)\n",
    "    if len(summaries)==1:\n",
    "        final = summaries[0]\n",
    "    else:\n",
    "        progress(0.85, desc=\"Î∂ÄÎ∂Ñ ÏöîÏïΩ ÌÜµÌï© Ï§ë...\")\n",
    "        final = llm.invoke([\n",
    "            SystemMessage(content=\"Î∂ÄÎ∂Ñ ÏöîÏïΩÎì§ÏùÑ Ï¢ÖÌï©Ìï¥ Ï†ÑÏ≤¥ ÏöîÏïΩ 10~15Ï§Ñ.\"),\n",
    "            HumanMessage(content=\"\\n\\n---\\n\\n\".join(summaries))\n",
    "        ]).content\n",
    "    return final, full_text\n",
    "\n",
    "def handle_pdf_extraction(pdf_file, header_px: int, footer_px: int):\n",
    "    if not pdf_file: return \"PDF ÌååÏùºÏùÑ ÏóÖÎ°úÎìúÌïòÏÑ∏Ïöî.\", [], None\n",
    "    out = Path(\"outputs/pdf_extraction\"); out.mkdir(parents=True, exist_ok=True)\n",
    "    doc = fitz.open(pdf_file.name)\n",
    "    full_text, images = \"\", []\n",
    "    for i, page in enumerate(doc):\n",
    "        rect = page.rect\n",
    "        text = page.get_text(\"text\", clip=(0, header_px, rect.width, rect.height-footer_px))\n",
    "        full_text += f\"--- Page {i+1} ---\\n{text}\\n\\n\"\n",
    "        for idx, img in enumerate(page.get_images(full=True)):\n",
    "            xref = img[0]; base = doc.extract_image(xref)\n",
    "            path = out / f\"page{i+1}_img{idx+1}.{base['ext']}\"\n",
    "            with open(path,\"wb\") as f: f.write(base[\"image\"])\n",
    "            images.append(str(path))\n",
    "    doc.close()\n",
    "    return full_text, images, str(out)\n",
    "\n",
    "def handle_pdf_table_extraction(pdf_file):\n",
    "    if not pdf_file: return None, \"PDF ÌååÏùºÏùÑ ÏóÖÎ°úÎìúÌïòÏÑ∏Ïöî.\", None\n",
    "    # ÏßÄÏó∞ ÏûÑÌè¨Ìä∏\n",
    "    try:\n",
    "        import camelot; import tabula\n",
    "    except Exception:\n",
    "        return None, \"ÌÖåÏù¥Î∏î Ï∂îÏ∂ú ÎùºÏù¥Î∏åÎü¨Î¶¨Í∞Ä ÏÑ§ÏπòÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.\", None\n",
    "    os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-11-openjdk-amd64'\n",
    "    out = Path(\"outputs/table_extraction\"); out.mkdir(parents=True, exist_ok=True)\n",
    "    pdf_path = pdf_file.name; log_text = \"\"\n",
    "    log_text += log(\"CamelotÏúºÎ°ú Ï∂îÏ∂ú ÏãúÎèÑ Ï§ë...\")\n",
    "    try:\n",
    "        tables = camelot.read_pdf(pdf_path, flavor='lattice', pages='all')\n",
    "        if tables.n>0:\n",
    "            df = tables[0].df; xls = out/\"extracted_tables_camelot.xlsx\"\n",
    "            df.to_excel(xls, index=False); return df, log_text+log(f\"{tables.n}Í∞ú Ìëú Î∞úÍ≤¨\"), str(xls)\n",
    "    except Exception as e:\n",
    "        log_text += log(f\"Camelot Ïã§Ìå®: {e}\")\n",
    "    log_text += log(\"TabulaÎ°ú Ï∂îÏ∂ú ÏãúÎèÑ Ï§ë...\")\n",
    "    try:\n",
    "        frames = tabula.read_pdf(pdf_path, pages='all', multiple_tables=True)\n",
    "        if frames:\n",
    "            df = frames[0]; xls = out/\"extracted_tables_tabula.xlsx\"\n",
    "            df.to_excel(xls, index=False); return df, log_text+log(f\"{len(frames)}Í∞ú Ìëú Î∞úÍ≤¨\"), str(xls)\n",
    "    except Exception as e:\n",
    "        log_text += log(f\"Tabula Ïã§Ìå®: {e}\")\n",
    "    return None, log_text+log(\"ÌëúÎ•º Ï∂îÏ∂úÌïòÏßÄ Î™ªÌñàÏäµÎãàÎã§.\"), None\n",
    "\n",
    "# =============================\n",
    "# 3) üß† RAG Ï±óÎ¥á (Î¨¥Í±∞Ïö¥ Î™®Îç∏Îßå ÏÇ¨Ïö©)\n",
    "# =============================\n",
    "def handle_rag_indexing(pdf_files, chunk_size: int, chunk_overlap: int, progress=gr.Progress()):\n",
    "    if not pdf_files: return \"ÏÉâÏù∏Ìï† PDF ÌååÏùºÏùÑ ÏóÖÎ°úÎìúÌïòÏÑ∏Ïöî.\", gr.update(interactive=False)\n",
    "    log_text = \"\"\n",
    "    try:\n",
    "        progress(0, desc=\"ÏûÑÎ≤†Îî© Î™®Îç∏ Î°úÎî© Ï§ë...\")\n",
    "        STATE.rag_embeddings = OllamaEmbeddings(model=EMB_MODEL)\n",
    "        log_text += log(\"ÏûÑÎ≤†Îî© Î™®Îç∏ Î°úÎìú ÏôÑÎ£å.\")\n",
    "        all_docs = []\n",
    "        for f in progress.tqdm(pdf_files, desc=\"PDF ÌååÏùº Ï≤òÎ¶¨ Ï§ë...\"):\n",
    "            loader = PyPDFLoader(f.name)\n",
    "            docs = loader.load()\n",
    "            splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "            all_docs.extend(splitter.split_documents(docs))\n",
    "        log_text += log(f\"Ï¥ù {len(pdf_files)}Í∞ú PDFÏóêÏÑú {len(all_docs)}Í∞ú Ï≤≠ÌÅ¨ ÏÉùÏÑ±.\")\n",
    "        progress(0.85, desc=\"ChromaDB Ï†ÄÏû• Ï§ë...\")\n",
    "        STATE.rag_vectorstore = Chroma.from_documents(\n",
    "            documents=all_docs,\n",
    "            embedding=STATE.rag_embeddings,\n",
    "            persist_directory=DEFAULT_PERSIST_DIR\n",
    "        )\n",
    "        log_text += log(\"Î≤°ÌÑ∞ DB ÏÉâÏù∏ ÏôÑÎ£å. ÏßàÎ¨∏ Í∞ÄÎä•.\")\n",
    "        return log_text, gr.update(interactive=True)\n",
    "    except Exception as e:\n",
    "        return f\"ÏÉâÏù∏ Ï§ë Ïò§Î•ò Î∞úÏÉù: {e}\", gr.update(interactive=False)\n",
    "\n",
    "def handle_rag_chat_stream(message: str, history: list, temperature: float):\n",
    "    if not STATE.rag_vectorstore:\n",
    "        yield \"Î®ºÏ†Ä PDF ÌååÏùºÏùÑ ÏÉâÏù∏Ìï¥Ï£ºÏÑ∏Ïöî.\"; return\n",
    "    \n",
    "    # ‚¨áÔ∏è 1. ÏÇ¨Ïö©Ïûê ÏßàÎ¨∏ÏùÑ historyÏóê Ï∂îÍ∞ÄÌïòÍ≥† Ï¶âÏãú UIÏóê Î∞òÏòÅ\n",
    "    history.append({\"role\":\"user\",\"content\":message})\n",
    "    \n",
    "    # ‚¨áÔ∏è 2. \"ÏÉùÏÑ± Ï§ë\" ÏïàÎÇ¥ Î©îÏãúÏßÄÎ•º AI ÏùëÎãµÏúºÎ°ú Î®ºÏ†Ä ÌëúÏãú\n",
    "    # Îπà contentÎ•º yieldÌïòÏßÄ ÏïäÍ≥†, ÏïàÎÇ¥ Î©îÏãúÏßÄÎ•º Î®ºÏ†Ä yieldÌïòÏó¨ GradioÍ∞Ä ÎßêÌíçÏÑ†ÏùÑ Ï§ÄÎπÑÌïòÍ≤å Ìï©ÎãàÎã§.\n",
    "    # Ïù¥ Î©îÏãúÏßÄÎäî Í≥ßÎ∞îÎ°ú Ïã§Ï†ú ÎãµÎ≥ÄÏúºÎ°ú ÎåÄÏ≤¥Îê† Í≤ÉÏûÖÎãàÎã§.\n",
    "    history.append({\"role\":\"assistant\",\"content\":\"Î¨∏ÏÑú Í∏∞Î∞òÏúºÎ°ú ÎãµÎ≥ÄÏùÑ Ï∂îÎ°†ÌïòÎäî Ï§ëÏûÖÎãàÎã§... (DeepSeek-R1) \\n\\nÏû†ÏãúÎßå Í∏∞Îã§Î†§Ï£ºÏÑ∏Ïöî.\"})\n",
    "    yield history # Ï§ëÍ∞Ñ ÏÉÅÌÉúÎ•º UIÏóê ÌëúÏãú\n",
    "    \n",
    "    # ‚¨áÔ∏è 3. LLM Î∞è RAG Í≤ÄÏÉâ ÏãúÏûë (ÏãúÍ∞ÑÏù¥ Í±∏Î¶¨Îäî ÏûëÏóÖ)\n",
    "    llm = STATE.get_llm(model_name=HEAVY_MODEL, temperature=temperature, num_predict=512)\n",
    "    retriever = STATE.rag_vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "    \n",
    "    # Ï±óÎ¥á historyÏóêÏÑú Î∞©Í∏à Ï∂îÍ∞ÄÌïú ÏïàÎÇ¥ Î©îÏãúÏßÄÎ•º Ï†úÍ±∞Ìï©ÎãàÎã§ (Ïã§Ï†ú ÎãµÎ≥ÄÏùÑ Î∞õÏùÑ Ï§ÄÎπÑ)\n",
    "    history.pop() \n",
    "    \n",
    "    # ‚¨áÔ∏è 4. RAG Í≤ÄÏÉâ (docs)\n",
    "    docs = retriever.invoke(message)\n",
    "    STATE.last_rag_sources = [\n",
    "        {\"source\": d.metadata.get('source','N/A'), \"page\": d.metadata.get('page','N/A'), \"content\": d.page_content}\n",
    "        for d in docs\n",
    "    ]\n",
    "    \n",
    "    # ‚¨áÔ∏è 5. ÌîÑÎ°¨ÌîÑÌä∏ Î∞è Chain ÏÑ§Ï†ï (Ïù¥Ìïò Í∏∞Ï°¥ Î°úÏßÅ Ïú†ÏßÄ)\n",
    "    system_prompt = \"Ï†úÍ≥µÎêú Ïª®ÌÖçÏä§Ìä∏Ïóê Í∏∞Î∞òÌï¥ Ï†ïÌôïÌûà ÎãµÌïòÏÑ∏Ïöî. Î™®Î•¥Î©¥ Î™®Î•∏Îã§Í≥† ÎãµÌïòÍ∏∞. **ÎãµÎ≥ÄÏùÄ Î∞òÎìúÏãú ÌïúÍµ≠Ïñ¥Î°úÎßå Ìï¥Ïïº Ìï©ÎãàÎã§. Îã§Î•∏ Ïñ∏Ïñ¥(ÌäπÌûà Ï§ëÍµ≠Ïñ¥) ÏÇ¨Ïö©ÏùÑ Í∏àÏßÄÌï©ÎãàÎã§.**\"\n",
    "    prompt_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"system\", \"Ïª®ÌÖçÏä§Ìä∏:\\n{context}\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ])\n",
    "    chain = create_stuff_documents_chain(llm, prompt_template)\n",
    "    \n",
    "    # ‚¨áÔ∏è 6. Ïã§Ï†ú AI ÎãµÎ≥ÄÏùÑ Î∞õÏùÑ Îπà ÎßêÌíçÏÑ†ÏùÑ Îã§Ïãú Ï∂îÍ∞ÄÌï©ÎãàÎã§.\n",
    "    history.append({\"role\":\"assistant\",\"content\":\"\"})\n",
    "    resp = \"\"\n",
    "    for ch in chain.stream({\"input\": message, \"context\": docs}):\n",
    "        resp += ch; history[-1][\"content\"] = resp; yield history\n",
    "\n",
    "def update_rag_sources_display():\n",
    "    if not STATE.last_rag_sources: return \"Í≤ÄÏÉâÎêú Î¨∏ÏÑúÍ∞Ä ÏóÜÏäµÎãàÎã§.\"\n",
    "    md = \"### üìö ÎãµÎ≥ÄÏóê Ï∞∏Ï°∞Îêú Î¨∏ÏÑú\\n\\n\"\n",
    "    for i, s in enumerate(STATE.last_rag_sources):\n",
    "        md += f\"**[Ï∂úÏ≤ò {i+1}]**\\n- **ÌååÏùº**: `{os.path.basename(s['source'])}` (ÌéòÏù¥ÏßÄ: {s['page']})\\n- **ÎÇ¥Ïö© ÏùºÎ∂Ä**: {s['content'][:200]}...\\n\\n\"\n",
    "    return md\n",
    "\n",
    "# =============================\n",
    "# 4) üñºÔ∏è Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑù\n",
    "# =============================\n",
    "def handle_single_image_analysis(image, prompt: str, vlm_model: str, use_refine: bool, temperature: float):\n",
    "    if image is None: return \"Ïù¥ÎØ∏ÏßÄÎ•º ÏóÖÎ°úÎìúÌïòÏÑ∏Ïöî.\", \"\"\n",
    "    analysis = call_vlm([image], prompt, vlm_model)\n",
    "    if not use_refine: return analysis, analysis\n",
    "    llm = STATE.get_llm(model_name=FAST_MODEL, temperature=temperature, num_predict=200)\n",
    "    refined = llm.invoke(f\"Îã§Ïùå Î∂ÑÏÑùÏùÑ Í∞ÑÍ≤∞ÌïòÍ≥† Ï†ïÎ≥¥Í∞Ä ÌíçÎ∂ÄÌïòÍ≤å Ï†ïÎ¶¨:\\n{analysis}\").content\n",
    "    return refined, analysis\n",
    "\n",
    "def handle_image_comparison(image1, image2, prompt: str, vlm_model: str, use_refine: bool, temperature: float):\n",
    "    if image1 is None or image2 is None: return \"Îëê Ïù¥ÎØ∏ÏßÄÎ•º Î™®Îëê ÏóÖÎ°úÎìúÌïòÏÑ∏Ïöî.\", \"\"\n",
    "    comparison = call_vlm([image1, image2], prompt, vlm_model)\n",
    "    if not use_refine: return comparison, comparison\n",
    "    llm = STATE.get_llm(model_name=FAST_MODEL, temperature=temperature, num_predict=220)\n",
    "    refined = llm.invoke(f\"Îã§Ïùå ÎπÑÍµêÍ≤∞Í≥ºÎ•º Ìï≠Î™©ÌôîÌï¥ Ï†ïÎ¶¨:\\n{comparison}\").content\n",
    "    return refined, comparison\n",
    "\n",
    "# =============================\n",
    "# 5) üõ†Ô∏è Í≥†Í∏â ÏÉùÏÑ±Í∏∞\n",
    "# =============================\n",
    "def handle_mermaid_generation(idea: str, diagram_type: str, use_llm: bool):\n",
    "    if not idea.strip(): return \"\", \"\", \"ÏïÑÏù¥ÎîîÏñ¥Î•º ÏûÖÎ†•ÌïòÏÑ∏Ïöî.\"\n",
    "    if not use_llm:\n",
    "        code, status = idea, \"LLM ÎØ∏ÏÇ¨Ïö©\"\n",
    "    else:\n",
    "        llm = STATE.get_llm(model_name=FAST_MODEL, temperature=0.1, num_predict=192)\n",
    "        sys = f\"Mermaid {diagram_type} ÌÉÄÏûÖÏùò 'Ïú†Ìö®Ìïú' ÏΩîÎìúÎßå ÏΩîÎìúÎ∏îÎ°ùÏúºÎ°ú Ï∂úÎ†•. ÏÑ§Î™Ö Í∏àÏßÄ.\"\n",
    "        resp = llm.invoke([SystemMessage(content=sys), HumanMessage(content=idea)]).content\n",
    "        m = re.search(r\"```mermaid\\s*([\\s\\S]*?)```\", resp)\n",
    "        if m: code, status = m.group(1).strip(), \"Mermaid ÏΩîÎìú ÏÉùÏÑ± ÏôÑÎ£å.\"\n",
    "        else:\n",
    "            code = re.sub(r\"^.*?mermaid\", \"\", resp, flags=re.S|re.I).strip()\n",
    "            status = \"ÏΩîÎìú Î∏îÎ°ù ÎØ∏Í≤ÄÏ∂ú ‚Üí Î≥∏Î¨∏ÏóêÏÑú Ï∂îÏ∂ú\"\n",
    "    return f\"```mermaid\\n{code}\\n```\", code, status\n",
    "\n",
    "def handle_sd_prompt_generation(idea: str):\n",
    "    if not idea.strip(): return \"\", \"\", \"ÏïÑÏù¥ÎîîÏñ¥Î•º ÏûÖÎ†•ÌïòÏÑ∏Ïöî.\"\n",
    "    llm = STATE.get_llm(model_name=FAST_MODEL, temperature=0.6, num_predict=160)\n",
    "    sys = \"Stable Diffusion ÌîÑÎ°¨ÌîÑÌä∏ ÏóîÏßÄÎãàÏñ¥. ÏïÑÏù¥ÎîîÏñ¥Î•º Î∞îÌÉïÏúºÎ°ú ÏÉÅÏÑ∏Ìïú ÌîÑÎ°¨ÌîÑÌä∏Î•º ÏÉùÏÑ±ÌïòÏÑ∏Ïöî. ÏùëÎãµÏùÄ **Ïò§ÏßÅ Ïú†Ìö®Ìïú JSON ÏΩîÎìúÎ∏îÎ°ùÎßå**Ïù¥Ïñ¥Ïïº ÌïòÎ©∞, Îã§Î•∏ ÏÑ§Î™Ö ÌÖçÏä§Ìä∏Îäî **Ï†àÎåÄ** Ìè¨Ìï®ÌïòÏßÄ ÎßàÏÑ∏Ïöî. JSONÏùÄ Î∞òÎìúÏãú keys=['prompt','negative_prompt']Î•º Ìè¨Ìï®Ìï¥Ïïº Ìï©ÎãàÎã§. **ÌîÑÎ°¨ÌîÑÌä∏ÏôÄ ÎÑ§Í±∞Ìã∞Î∏å ÌîÑÎ°¨ÌîÑÌä∏ ÎÇ¥Ïö©ÏùÄ ÏòÅÏñ¥Î°ú ÏûëÏÑ±ÌïòÏÑ∏Ïöî.**\"\n",
    "    resp = llm.invoke([SystemMessage(content=sys), HumanMessage(content=f\"\"\"ÏïÑÏù¥ÎîîÏñ¥: {idea} ÏòàÏãú ÏùëÎãµ ÌòïÏãù:```json{{ \"prompt\": \"detailed, high-quality, full-body portrait of...\", \"negative_prompt\": \"low quality, deformed, blurry, ugly\"}}\"\"\")]).content\n",
    "    data = safe_json_extract(resp)\n",
    "    # ÌååÏã± Ïã§Ìå® Ïãú ÏÇ¨Ïö©Ìï† Í∏∞Î≥∏ ÎÑ§Í±∞Ìã∞Î∏å ÌîÑÎ°¨ÌîÑÌä∏\n",
    "    default_negative = \"low quality, blurry, ugly, deformed, worst quality, noise\"\n",
    "    if isinstance(data, dict):\n",
    "        prompt = data.get(\"prompt\", idea)\n",
    "        negative = data.get(\"negative_prompt\", default_negative)\n",
    "        status_text = \"ÌîÑÎ°¨ÌîÑÌä∏ ÏÉùÏÑ± Î∞è JSON ÌååÏã± ÏôÑÎ£å.\"\n",
    "        return prompt, negative, status_text\n",
    "    # ÌååÏã± Ïã§Ìå® Ïãú: ÌîÑÎ°¨ÌîÑÌä∏Îäî ÏïÑÏù¥ÎîîÏñ¥ ÏõêÎ≥∏, ÎÑ§Í±∞Ìã∞Î∏åÎäî Í∏∞Î≥∏Í∞í ÏÇ¨Ïö©\n",
    "    else:\n",
    "        # Ïã§Ìå® Ïãú, UI Î∞ïÏä§ÏóêÎäî ÏïÑÏù¥ÎîîÏñ¥ ÏõêÎ≥∏Í≥º Í∏∞Î≥∏ ÎÑ§Í±∞Ìã∞Î∏å ÌîÑÎ°¨ÌîÑÌä∏Î•º Ï∂úÎ†•\n",
    "        # ÏÉÅÌÉú Ï∞ΩÏóêÎäî ÎîîÎ≤ÑÍπÖÏùÑ ÏúÑÌï¥ AIÏùò ÏõêÎ≥∏ ÏùëÎãµÏùÑ Î™®Îëê Ï∂úÎ†•\n",
    "        status_text = f\"JSON ÌååÏã± Ïã§Ìå®. AIÍ∞Ä ÏöîÏ≤≠Îêú JSON ÌòïÏãùÏùÑ Îî∞Î•¥ÏßÄ ÏïäÏïòÏäµÎãàÎã§. **ÏïÑÎûòÏóê AIÏùò ÏõêÎ≥∏ ÏùëÎãµÏù¥ Î™®Îëê Ìè¨Ìï®ÎêòÏñ¥ ÏûàÏäµÎãàÎã§.**\"\n",
    "        \n",
    "        # UI Î∞ïÏä§Ïóê Ï∂úÎ†•Ìï† Í∞í\n",
    "        # ÏÉÅÌÉú Ï∞ΩÏóê ÏõêÎ≥∏ ÏùëÎãµÏùÑ Î≥¥Ïó¨Ï£ºÍ∏∞ ÏúÑÌï¥ ÏÉÅÌÉú ÌÖçÏä§Ìä∏Ïóê ÏõêÎ≥∏ ÏùëÎãµ Ï∂îÍ∞Ä\n",
    "        return idea, default_negative, status_text + \"\\n\\n\" + resp\n",
    "def handle_structured_ocr(image, use_llm: bool):\n",
    "    if image is None: return None, \"\", None, \"Ïù¥ÎØ∏ÏßÄÎ•º ÏóÖÎ°úÎìúÌïòÏÑ∏Ïöî.\"\n",
    "    # ÏßÄÏó∞ ÏûÑÌè¨Ìä∏\n",
    "    try:\n",
    "        import easyocr\n",
    "    except Exception:\n",
    "        return None, \"EasyOCR ÎØ∏ÏÑ§Ïπò ÎòêÎäî Î°úÎî© Ïã§Ìå®.\", None, \"\"\n",
    "    reader = easyocr.Reader(['ko','en'])\n",
    "    np_img = np.array(image)\n",
    "    results = reader.readtext(np_img)\n",
    "    overlay = image.copy(); draw = ImageDraw.Draw(overlay)\n",
    "    full_text = \"\\n\".join([r[1] for r in results])\n",
    "    for (bbox, text, prob) in results:\n",
    "        draw.polygon([tuple(p) for p in bbox], outline=\"cyan\", width=2)\n",
    "    if not use_llm: return overlay, full_text, None, \"LLM ÎØ∏ÏÇ¨Ïö©\"\n",
    "    llm = STATE.get_llm(model_name=FAST_MODEL, temperature=0.1, num_predict=220)\n",
    "    resp = llm.invoke([\n",
    "        SystemMessage(content=\"OCR ÌÖçÏä§Ìä∏Î•º Íµ¨Ï°∞Ìôî JSONÏúºÎ°ú Î≥ÄÌôò. ÌëúÎ©¥ Íµ¨Ï°∞ Ïù∏Ïãù Ïãú rows/columns or items ÏÇ¨Ïö©.\"),\n",
    "        HumanMessage(content=full_text)\n",
    "    ]).content\n",
    "    data = safe_json_extract(resp); df = None\n",
    "    if isinstance(data, dict):\n",
    "        if 'items' in data and isinstance(data['items'], list):\n",
    "            df = pd.DataFrame(data['items'])\n",
    "        elif 'rows' in data and 'columns' in data:\n",
    "            df = pd.DataFrame(data['rows'], columns=data['columns'])\n",
    "    return overlay, full_text, df, (json.dumps(data, ensure_ascii=False, indent=2) if data else \"\")\n",
    "\n",
    "# =============================\n",
    "# 6) üåê Ïõπ & ÎèÑÍµ¨\n",
    "# =============================\n",
    "\n",
    "def handle_web_rag(query: str, temperature: float, progress=gr.Progress()):\n",
    "    llm = STATE.get_llm(model_name=FAST_MODEL, temperature=temperature, num_predict=256)\n",
    "    progress(0, desc=\"Ïõπ Í≤ÄÏÉâ Ï§ë...\")\n",
    "    search = DuckDuckGoSearchResults()\n",
    "    search_results = search.run(query)\n",
    "    links = re.findall(r'https?://[^\\s,```<>\"]+', search_results)\n",
    "    if not links: return \"Í¥ÄÎ†® Ïõπ ÌéòÏù¥ÏßÄÎ•º Ï∞æÏßÄ Î™ªÌñàÏäµÎãàÎã§.\", search_results, \"\"\n",
    "    links = links[:2]  # ÏÜçÎèÑ ÏúÑÌï¥ ÏÉÅÏúÑ 2Í∞úÎßå\n",
    "    progress(0.35, desc=f\"{len(links)}Í∞ú ÎßÅÌÅ¨ ÏàòÏßë Ï§ë...\")\n",
    "    try:\n",
    "        # bs_kwargsÎäî Î≤ÑÏ†ÑÎ≥Ñ Ï∞®Ïù¥Í∞Ä ÏûàÏñ¥ Ï†úÍ±∞(ÏïàÏ†ïÏÑ± Ïö∞ÏÑ†)\n",
    "        loader = WebBaseLoader(web_paths=links)\n",
    "        docs = loader.load()\n",
    "        ctx = \"\\n\\n---\\n\\n\".join([d.page_content for d in docs])\n",
    "    except Exception as e:\n",
    "        ctx = f\"ÏΩòÌÖêÏ∏† ÏàòÏßë Ïã§Ìå®: {e}\"\n",
    "    progress(0.7, desc=\"ÎãµÎ≥Ä ÏÉùÏÑ± Ï§ë...\")\n",
    "    resp = llm.invoke([SystemMessage(content=\"**Î∞òÎìúÏãú ÌïúÍµ≠Ïñ¥Î°ú**, Ïõπ Í≤∞Í≥º Í∏∞Î∞òÏúºÎ°ú Ï†ïÌôï¬∑Í∞ÑÍ≤∞Ìûà ÎãµÎ≥Ä, Îã§Î•∏ Ïñ∏Ïñ¥(ÌäπÌûà Ï§ëÍµ≠Ïñ¥) ÏÇ¨Ïö© Í∏àÏßÄ.\"), HumanMessage(content=f\"Í≤ÄÏÉâ Í≤∞Í≥º:\\n{ctx}\\n\\nÏßàÎ¨∏: {query}\")]).content\n",
    "    return resp, search_results, \"\\n\".join(links)\n",
    "\n",
    "# =============================\n",
    "# 7) ü§îÏàúÏ∞®Ï†Å ÏÇ¨Í≥†\n",
    "# =============================\n",
    "\n",
    "def handle_sequential_thinking(query: str, temperature: float, progress=gr.Progress()):\n",
    "    if not query.strip(): return \"\", \"\", \"\", \"ÏßàÏùòÎ•º ÏûÖÎ†•ÌïòÏÑ∏Ïöî.\"\n",
    "    llm = STATE.get_llm(model_name=FAST_MODEL, temperature=temperature, num_predict=256)\n",
    "    progress(0.1, desc=\"[1/3] Î∂ÑÏÑù Ï§ë...\")\n",
    "    analysis = llm.invoke([SystemMessage(content=\"**Î∞òÎìúÏãú ÌïúÍµ≠Ïñ¥Î°ú**, ÌïµÏã¨ ÏüÅÏ†êÏùÑ 3~5Ï§ÑÎ°ú, Îã§Î•∏ Ïñ∏Ïñ¥(ÌäπÌûà Ï§ëÍµ≠Ïñ¥) ÏÇ¨Ïö© Í∏àÏßÄ.\"), HumanMessage(content=query)]).content\n",
    "    progress(0.5, desc=\"[2/3] ÏµúÏ¢Ö ÎãµÎ≥Ä ÏÉùÏÑ± Ï§ë...\")\n",
    "    answer = llm.invoke([SystemMessage(content=\"**Î∞òÎìúÏãú ÌïúÍµ≠Ïñ¥Î°ú**, Í∞ÑÍ≤∞¬∑Ï†ïÌôï¬∑Íµ¨Ï°∞ÌôîÎêú ÎãµÎ≥Ä 8~12Ï§Ñ, Îã§Î•∏ Ïñ∏Ïñ¥(ÌäπÌûà Ï§ëÍµ≠Ïñ¥) ÏÇ¨Ïö© Í∏àÏßÄ.\"), HumanMessage(content=f\"{query}\\n\\nÎ∂ÑÏÑù:{analysis}\")]).content\n",
    "    progress(0.8, desc=\"[3/3] Í≤ÄÏ¶ù Ï§ë...\")\n",
    "    reflection = llm.invoke([SystemMessage(content=\"**Î∞òÎìúÏãú ÌïúÍµ≠Ïñ¥Î°ú**, Î™®Ìò∏ÏÑ±/ÎàÑÎùΩ/Í≥ºÏû• Ï†êÍ≤Ä ÌõÑ 3Ï§Ñ Ï†úÏïà, Îã§Î•∏ Ïñ∏Ïñ¥(ÌäπÌûà Ï§ëÍµ≠Ïñ¥) ÏÇ¨Ïö© Í∏àÏßÄ.\"), HumanMessage(content=f\"ÎãµÎ≥Ä:{answer}\")]).content\n",
    "    return analysis, answer, reflection, \"Î™®Îì† Îã®Í≥Ñ ÏôÑÎ£å.\"\n",
    "\n",
    "# =============================\n",
    "# 8) Gradio UI\n",
    "# =============================\n",
    "def build_ui():\n",
    "    with gr.Blocks(theme=gr.themes.Soft(), title=\"DeepSeek-R1 AI Toolkit\") as demo:\n",
    "        gr.Markdown(\"# üöÄ DeepSeek-R1 AI Toolkit (All-in-One)\")\n",
    "        gr.Markdown(\"38Í∞ú ÏòàÏ†úÎ•º ÌÜµÌï©Ìïú Î°úÏª¨ AI Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖòÏûÖÎãàÎã§. (ÏÜçÎèÑ ÏµúÏ†ÅÌôî)\")\n",
    "\n",
    "        # Ï±óÎ¥á\n",
    "        with gr.Tab(\"üí¨ Ï±óÎ¥á\"):\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=1):\n",
    "                    persona = gr.Dropdown(list(PERSONAS.keys()), value=\"Í∏∞Î≥∏ ÏÉÅÎã¥ÏÇ¨\", label=\"ÌéòÎ•¥ÏÜåÎÇò ÏÑ†ÌÉù\")\n",
    "                    temp = gr.Slider(0.0, 1.5, value=0.7, step=0.1, label=\"Temperature\")\n",
    "                with gr.Column(scale=3):\n",
    "                    win = gr.Chatbot(height=500, label=\"ÎåÄÌôîÏ∞Ω\", type=\"messages\")\n",
    "                    with gr.Row():\n",
    "                        msg = gr.Textbox(label=\"Î©îÏãúÏßÄ ÏûÖÎ†•\", placeholder=\"Î¨¥ÏóáÏù¥Îì† Î¨ºÏñ¥Î≥¥ÏÑ∏Ïöî...\", scale=4)\n",
    "                        btn = gr.Button(\"Ï†ÑÏÜ°\", variant=\"primary\", scale=1)\n",
    "                        clr = gr.Button(\"Ï¥àÍ∏∞Ìôî\")\n",
    "\n",
    "            def on_submit(m, h, p, t):\n",
    "                for nh in handle_chatbot_stream(m, h, p, t):\n",
    "                    yield nh, \"\"\n",
    "            msg.submit(on_submit, [msg, win, persona, temp], [win, msg])\n",
    "            btn.click(on_submit, [msg, win, persona, temp], [win, msg])\n",
    "            clr.click(lambda: [], None, win)\n",
    "\n",
    "        # PDF & Î¨∏ÏÑú\n",
    "        with gr.Tab(\"üìÑ PDF & Î¨∏ÏÑú\"):\n",
    "            with gr.Tabs():\n",
    "                with gr.TabItem(\"PDF ÏöîÏïΩ\"):\n",
    "                    with gr.Row():\n",
    "                        f_in = gr.File(label=\"PDF ÌååÏùº\", file_types=[\".pdf\"])\n",
    "                        with gr.Column():\n",
    "                            chunk = gr.Slider(1000, 8000, value=4000, step=500, label=\"Ï≤≠ÌÅ¨ ÌÅ¨Í∏∞\")\n",
    "                            t = gr.Slider(0.0, 1.0, value=0.2, step=0.1, label=\"Temperature\")\n",
    "                            run = gr.Button(\"ÏöîÏïΩ Ïã§Ìñâ\", variant=\"primary\")\n",
    "                    out = gr.Textbox(label=\"ÏöîÏïΩ Í≤∞Í≥º\", lines=15)\n",
    "                    with gr.Accordion(\"Ï†ÑÏ≤¥ ÌÖçÏä§Ìä∏\", open=False):\n",
    "                        raw = gr.Textbox(label=\"ÏõêÎ≥∏ ÌÖçÏä§Ìä∏\", lines=10)\n",
    "                    run.click(handle_pdf_summary, [f_in, chunk, t], [out, raw])\n",
    "\n",
    "                with gr.TabItem(\"ÌÖçÏä§Ìä∏/Ïù¥ÎØ∏ÏßÄ Ï∂îÏ∂ú\"):\n",
    "                    with gr.Row():\n",
    "                        f_in2 = gr.File(label=\"PDF ÌååÏùº\", file_types=[\".pdf\"])\n",
    "                        with gr.Column():\n",
    "                            head = gr.Slider(0, 200, value=60, label=\"Ìó§Îçî ÎÜíÏù¥ (px)\")\n",
    "                            foot = gr.Slider(0, 200, value=60, label=\"Ìë∏ÌÑ∞ ÎÜíÏù¥ (px)\")\n",
    "                            run2 = gr.Button(\"Ï∂îÏ∂ú Ïã§Ìñâ\", variant=\"primary\")\n",
    "                    with gr.Row():\n",
    "                        tx = gr.Textbox(label=\"Ï∂îÏ∂úÎêú ÌÖçÏä§Ìä∏\", lines=15, scale=2)\n",
    "                        with gr.Column(scale=1):\n",
    "                            gal = gr.Gallery(label=\"Ï∂îÏ∂úÎêú Ïù¥ÎØ∏ÏßÄ\", height=400)\n",
    "                            pth = gr.Textbox(label=\"Ï†ÄÏû• Í≤ΩÎ°ú\")\n",
    "                    run2.click(handle_pdf_extraction, [f_in2, head, foot], [tx, gal, pth])\n",
    "\n",
    "                with gr.TabItem(\"Ìëú Ï∂îÏ∂ú\"):\n",
    "                    f_tbl = gr.File(label=\"PDF ÌååÏùº\", file_types=[\".pdf\"])\n",
    "                    run3 = gr.Button(\"Ìëú Ï∂îÏ∂ú Ïã§Ìñâ\", variant=\"primary\")\n",
    "                    df = gr.Dataframe(label=\"Ï∂îÏ∂úÎêú Ìëú\")\n",
    "                    logbx = gr.Textbox(label=\"Ï≤òÎ¶¨ Î°úÍ∑∏\", lines=5)\n",
    "                    xfile = gr.File(label=\"ÏóëÏÖÄ Îã§Ïö¥Î°úÎìú\")\n",
    "                    run3.click(handle_pdf_table_extraction, [f_tbl], [df, logbx, xfile])\n",
    "\n",
    "        # RAG\n",
    "        with gr.Tab(\"üß† RAG Ï±óÎ¥á\"):\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=1):\n",
    "                    gr.Markdown(\"##### 1. Î¨∏ÏÑú ÏÉâÏù∏\")\n",
    "                    f_multi = gr.File(label=\"PDF ÌååÏùºÎì§\", file_count=\"multiple\", file_types=[\".pdf\"])\n",
    "                    cs = gr.Slider(200, 2000, value=1000, label=\"Ï≤≠ÌÅ¨ ÌÅ¨Í∏∞\")\n",
    "                    co = gr.Slider(0, 500, value=100, label=\"Ï≤≠ÌÅ¨ Ï§ëÏ≤©\")\n",
    "                    idx = gr.Button(\"ÏÉâÏù∏ ÏãúÏûë\", variant=\"primary\")\n",
    "                    stat = gr.Textbox(label=\"ÏÉâÏù∏ ÏÉÅÌÉú\", lines=10)\n",
    "                with gr.Column(scale=2):\n",
    "                    gr.Markdown(\"##### 2. Î¨∏ÏÑú Í∏∞Î∞ò ÎåÄÌôî\")\n",
    "                    rag_chat = gr.Chatbot(height=450, label=\"ÎåÄÌôîÏ∞Ω\", type=\"messages\")\n",
    "                    with gr.Accordion(\"Ï∞∏Ï°∞ Î¨∏ÏÑú\", open=False):\n",
    "                        rag_src = gr.Markdown()\n",
    "                    rtemp = gr.Slider(0.0, 1.0, value=0.3, label=\"Temperature\")\n",
    "                    rmsg = gr.Textbox(label=\"ÏßàÎ¨∏\", placeholder=\"Î¨∏ÏÑúÏóê ÎåÄÌï¥ ÏßàÎ¨∏ÌïòÏÑ∏Ïöî...\", interactive=False)\n",
    "                    rsend = gr.Button(\"Ï†ÑÏÜ°\", variant=\"primary\")\n",
    "\n",
    "            idx.click(handle_rag_indexing, [f_multi, cs, co], [stat, rmsg])\n",
    "\n",
    "            def on_rag_submit(m, h, t):\n",
    "                for nh in handle_rag_chat_stream(m, h, t):\n",
    "                    yield nh, \"\"\n",
    "            rmsg.submit(on_rag_submit, [rmsg, rag_chat, rtemp], [rag_chat, rmsg]).then(update_rag_sources_display, None, rag_src)\n",
    "            rsend.click(on_rag_submit, [rmsg, rag_chat, rtemp], [rag_chat, rmsg]).then(update_rag_sources_display, None, rag_src)\n",
    "\n",
    "        # Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑù\n",
    "        with gr.Tab(\"üñºÔ∏è Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑù\"):\n",
    "            vlm_model = gr.Dropdown([VLM_MODEL, \"llava:latest\"], value=VLM_MODEL, label=\"VLM Î™®Îç∏\")\n",
    "            with gr.Tabs():\n",
    "                with gr.TabItem(\"Îã®Ïùº Ïù¥ÎØ∏ÏßÄ\"):\n",
    "                    with gr.Row():\n",
    "                        img_single = gr.Image(type=\"filepath\", label=\"Ïù¥ÎØ∏ÏßÄ\")\n",
    "                        with gr.Column():\n",
    "                            iprompt = gr.Textbox(label=\"ÌîÑÎ°¨ÌîÑÌä∏\", value=\"Ïù¥ Ïù¥ÎØ∏ÏßÄÎ•º ÏÑ§Î™ÖÌï¥Ï£ºÏÑ∏Ïöî.\", lines=3)\n",
    "                            irefine = gr.Checkbox(label=\"LLMÏúºÎ°ú Ï†ïÏ†ú\", value=True)\n",
    "                            itemp = gr.Slider(0.0, 1.0, value=0.7, label=\"Temperature\")\n",
    "                            ibtn = gr.Button(\"Î∂ÑÏÑù\", variant=\"primary\")\n",
    "                    iout = gr.Textbox(label=\"Î∂ÑÏÑù Í≤∞Í≥º\", lines=10)\n",
    "                    with gr.Accordion(\"VLM ÏõêÎ≥∏\", open=False):\n",
    "                        iraw = gr.Textbox(label=\"ÏõêÎ≥∏ Í≤∞Í≥º\")\n",
    "                    ibtn.click(handle_single_image_analysis, [img_single, iprompt, vlm_model, irefine, itemp], [iout, iraw])\n",
    "\n",
    "                with gr.TabItem(\"Ïù¥ÎØ∏ÏßÄ ÎπÑÍµê\"):\n",
    "                    with gr.Row():\n",
    "                        img1 = gr.Image(type=\"filepath\", label=\"Ïù¥ÎØ∏ÏßÄ 1\")\n",
    "                        img2 = gr.Image(type=\"filepath\", label=\"Ïù¥ÎØ∏ÏßÄ 2\")\n",
    "                    cprompt = gr.Textbox(label=\"ÌîÑÎ°¨ÌîÑÌä∏\", value=\"Îëê Ïù¥ÎØ∏ÏßÄÎ•º ÎπÑÍµêÌï¥Ï£ºÏÑ∏Ïöî.\", lines=3)\n",
    "                    crefine = gr.Checkbox(label=\"LLMÏúºÎ°ú Ï†ïÏ†ú\", value=True)\n",
    "                    ctemp = gr.Slider(0.0, 1.0, value=0.7, label=\"Temperature\")\n",
    "                    cbtn = gr.Button(\"ÎπÑÍµê\", variant=\"primary\")\n",
    "                    cout = gr.Textbox(label=\"ÎπÑÍµê Í≤∞Í≥º\", lines=10)\n",
    "                    with gr.Accordion(\"VLM ÏõêÎ≥∏\", open=False):\n",
    "                        craw = gr.Textbox(label=\"ÏõêÎ≥∏ Í≤∞Í≥º\")\n",
    "                    cbtn.click(handle_image_comparison, [img1, img2, cprompt, vlm_model, crefine, ctemp], [cout, craw])\n",
    "\n",
    "        # Í≥†Í∏â ÏÉùÏÑ±Í∏∞\n",
    "        with gr.Tab(\"üõ†Ô∏è Í≥†Í∏â ÏÉùÏÑ±Í∏∞\"):\n",
    "            with gr.Tabs():\n",
    "                with gr.TabItem(\"Mermaid Îã§Ïù¥Ïñ¥Í∑∏Îû®\"):\n",
    "                    with gr.Row():\n",
    "                        with gr.Column():\n",
    "                            m_idea = gr.Textbox(label=\"ÏïÑÏù¥ÎîîÏñ¥\", lines=5)\n",
    "                            m_type = gr.Dropdown([\"flowchart\",\"sequenceDiagram\",\"gantt\",\"pie\",\"classDiagram\"], value=\"flowchart\", label=\"Îã§Ïù¥Ïñ¥Í∑∏Îû® ÌÉÄÏûÖ\")\n",
    "                            m_llm = gr.Checkbox(value=True, label=\"LLM ÏÇ¨Ïö©\")\n",
    "                            m_btn = gr.Button(\"ÏÉùÏÑ±\", variant=\"primary\")\n",
    "                        with gr.Column():\n",
    "                            m_out = gr.Markdown(label=\"Î†åÎçîÎßÅ\")\n",
    "                            m_code = gr.Code(label=\"ÏΩîÎìú\", language=\"markdown\")\n",
    "                            m_status = gr.Textbox(label=\"ÏÉÅÌÉú\")\n",
    "                    m_btn.click(handle_mermaid_generation, [m_idea, m_type, m_llm], [m_out, m_code, m_status])\n",
    "\n",
    "                with gr.TabItem(\"SD ÌîÑÎ°¨ÌîÑÌä∏\"):\n",
    "                    sd_idea = gr.Textbox(label=\"ÏïÑÏù¥ÎîîÏñ¥\", lines=3)\n",
    "                    sd_btn = gr.Button(\"ÏÉùÏÑ±\", variant=\"primary\")\n",
    "                    sd_prompt = gr.Textbox(label=\"ÌîÑÎ°¨ÌîÑÌä∏\", lines=4)\n",
    "                    sd_negative = gr.Textbox(label=\"ÎÑ§Í±∞Ìã∞Î∏å ÌîÑÎ°¨ÌîÑÌä∏\", lines=4)\n",
    "                    sd_status = gr.Textbox(label=\"ÏÉÅÌÉú\", lines=3)\n",
    "                    sd_btn.click(handle_sd_prompt_generation, [sd_idea], [sd_prompt, sd_negative, sd_status])\n",
    "\n",
    "                with gr.TabItem(\"Íµ¨Ï°∞Ìôî OCR\"):\n",
    "                    ocr_img = gr.Image(type=\"pil\", label=\"Ïù¥ÎØ∏ÏßÄ\")\n",
    "                    ocr_use_llm = gr.Checkbox(value=True, label=\"LLMÏúºÎ°ú Íµ¨Ï°∞Ìôî\")\n",
    "                    ocr_btn = gr.Button(\"OCR Ïã§Ìñâ\", variant=\"primary\")\n",
    "                    with gr.Row():\n",
    "                        ocr_overlay = gr.Image(label=\"Ïù∏Ïãù ÏòÅÏó≠\")\n",
    "                        ocr_df = gr.Dataframe(label=\"Íµ¨Ï°∞Ìôî Îç∞Ïù¥ÌÑ∞\")\n",
    "                    with gr.Accordion(\"ÌÖçÏä§Ìä∏ Î∞è JSON\", open=False):\n",
    "                        ocr_text = gr.Textbox(label=\"ÌÖçÏä§Ìä∏\")\n",
    "                        ocr_json = gr.Code(label=\"JSON\", language=\"json\")\n",
    "                    ocr_btn.click(handle_structured_ocr, [ocr_img, ocr_use_llm], [ocr_overlay, ocr_text, ocr_df, ocr_json])\n",
    "\n",
    "        # Ïõπ & ÎèÑÍµ¨\n",
    "        with gr.Tab(\"üåê Ïõπ & ÎèÑÍµ¨\"):\n",
    "            wq = gr.Textbox(label=\"Ïõπ Í≤ÄÏÉâ ÏßàÎ¨∏\", lines=2)\n",
    "            wt = gr.Slider(0.0, 1.0, value=0.7, label=\"Temperature\")\n",
    "            wb = gr.Button(\"Í≤ÄÏÉâ Î∞è ÎãµÎ≥Ä\", variant=\"primary\")\n",
    "            wa = gr.Textbox(label=\"ÎãµÎ≥Ä\", lines=10)\n",
    "            with gr.Accordion(\"Í≤ÄÏÉâ Ï†ïÎ≥¥\", open=False):\n",
    "                wr = gr.Textbox(label=\"Í≤ÄÏÉâ Í≤∞Í≥º\")\n",
    "                wl = gr.Textbox(label=\"Ï∞∏Ï°∞ URL\")\n",
    "            wb.click(handle_web_rag, [wq, wt], [wa, wr, wl])\n",
    "\n",
    "        # ÏàúÏ∞®Ï†Å ÏÇ¨Í≥†\n",
    "        with gr.Tab(\"ü§î ÏàúÏ∞®Ï†Å ÏÇ¨Í≥†\"):\n",
    "            sq = gr.Textbox(label=\"ÏßàÎ¨∏\", lines=3)\n",
    "            st = gr.Slider(0.0, 1.0, value=0.7, label=\"Temperature\")\n",
    "            sb = gr.Button(\"ÏÇ¨Í≥† ÏãúÏûë\", variant=\"primary\")\n",
    "            s_status = gr.Textbox(label=\"ÏÉÅÌÉú\")\n",
    "            with gr.Row():\n",
    "                s_a = gr.Markdown(label=\"Î∂ÑÏÑù\")\n",
    "                s_ans = gr.Markdown(label=\"ÎãµÎ≥Ä\")\n",
    "                s_ref = gr.Markdown(label=\"Í≤ÄÏ¶ù\")\n",
    "            sb.click(handle_sequential_thinking, [sq, st], [s_a, s_ans, s_ref, s_status])\n",
    "\n",
    "    return demo\n",
    "\n",
    "# =============================\n",
    "# Î©îÏù∏ Ïã§Ìñâ\n",
    "# =============================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ Gradio Ïï± Ïã§Ìñâ Ï§ë...\")\n",
    "    app = build_ui()\n",
    "    # ‚ñº Gradio 4.x: queue Ïù∏Ïûê Ï∂ïÏÜå(Ìò∏Ìôò)\n",
    "    app = app.queue(max_size=32)\n",
    "    # share=False Í∞Ä ÌÑ∞ÎÑêÎßÅ ÏßÄÏó∞ÏùÑ Ï§ÑÏó¨ Ìõ®Ïî¨ Îπ†Î¶Ñ\n",
    "    app.launch(server_name=\"0.0.0.0\", server_port=7870, inbrowser=True, share=False, show_error=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb03a50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
