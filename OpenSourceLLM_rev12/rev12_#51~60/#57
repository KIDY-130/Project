#57
"""
OpenCode ë³€í™˜ë³¸ (Gradio + Ollama + DeepSeek-R1)

- ëª©ì : ChatGPT ë“± Closed LLM ì˜ì¡´ ì½”ë“œë¥¼, ë¡œì»¬ ì˜¤í”ˆì†ŒìŠ¤ LLM(DeepSeek-R1, Ollama)ë¡œ êµì²´
- UI: Streamlit â†’ Gradio
- LLM: langchain ì œê±°, ollama íŒ¨í‚¤ì§€ ì§ì ‘ ì‚¬ìš© (ë¡œì»¬ PCì— ëª¨ë¸ ì„¤ì¹˜ë˜ì–´ ìˆë‹¤ê³  ê°€ì •)
- ì£¼ì„: ë™ì‘ íë¦„ê³¼ ìˆ˜ì • í¬ì¸íŠ¸ë¥¼ ìì„¸íˆ ì„¤ëª…

[ì‚¬ì „ ì¤€ë¹„]
1) Ollama ì„¤ì¹˜ ë° ì‹¤í–‰: https://ollama.com
2) DeepSeek-R1 ëª¨ë¸ ë°›ê¸°(ì˜ˆ: 8B):
   $ ollama pull deepseek-r1:8b
   (ë‹¤ë¥¸ ì‚¬ì´ì¦ˆ ì‚¬ìš© ê°€ëŠ¥: deepseek-r1, deepseek-r1:32b ë“±)
3) íŒŒì´ì¬ íŒ¨í‚¤ì§€:
   $ pip install gradio ollama

[ì‹¤í–‰]
   $ python app.py
"""

from __future__ import annotations
import re
from typing import Literal
import gradio as gr
import ollama

# ------------------------------
# ì„¤ì •ê°’: ëª¨ë¸ ì´ë¦„ ë° ê¸°ë³¸ íŒŒë¼ë¯¸í„°
# ------------------------------
MODEL_NAME = "deepseek-r1:8b"  # Ollamaì— ì„¤ì¹˜ëœ ëª¨ë¸ íƒœê·¸ì™€ ë™ì¼í•´ì•¼ í•©ë‹ˆë‹¤.
DEFAULT_TEMPERATURE = 0.7
MAX_TOKENS = 1024  # deepseek-r1ì€ ë‚´ë¶€ì ìœ¼ë¡œ ì¶”ë¡  ê¸¸ì´ ì œì•½ì´ ìˆìœ¼ë‹ˆ ê³¼ë„í•˜ê²Œ í¬ê²Œ ì¡ì§€ ë§ˆì„¸ìš”.


# ------------------------------
# ìœ í‹¸: DeepSeek-R1ì˜ <think> ... </think> ì‚¬ìœ (Reasoning) ë¸”ë¡ ì œê±°
#  - R1 ê³„ì—´ì€ ì¶”ë¡  ì¤‘ê°„ ê³¼ì •ì„ <think> íƒœê·¸ë¡œ ê°ì‹¸ì„œ ë°˜í™˜í•©ë‹ˆë‹¤.
#  - ìµœì¢… ì‚¬ìš©ìì—ê²ŒëŠ” ìµœì¢… ë‹µë³€ë§Œ ë³´ì—¬ì£¼ê¸° ìœ„í•´ í•´ë‹¹ ë¸”ë¡ì„ ì œê±°í•©ë‹ˆë‹¤.
# ------------------------------
THINK_BLOCK_PATTERN = re.compile(r"<think>.*?</think>", re.DOTALL)


def strip_reasoning(text: str) -> str:
    """
    DeepSeek-R1ì´ ìƒì„±í•œ ì‘ë‹µì—ì„œ <think>...</think> êµ¬ê°„ì„ ì œê±°í•˜ì—¬
    ìµœì¢… ê²°ê³¼ë§Œ ë‚¨ê²¨ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    return THINK_BLOCK_PATTERN.sub("", text).strip()


# ------------------------------
# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
#  - ì‚¬ìš©ìê°€ ê³ ë¥¸ ì–¸ì–´(í•œêµ­ì–´/English)ì— ë§ì¶° ì´ë©”ì¼ì„ ì‘ì„±
#  - í•œë¬¸ ì œì™¸ ì¡°ê±´ì€ í•œêµ­ì–´ ì¼€ì´ìŠ¤ì—ë§Œ ë°˜ì˜ (ì› ì½”ë“œ ìš”êµ¬ì‚¬í•­ ê·¸ëŒ€ë¡œ ìœ ì§€)
# ------------------------------
def build_prompt(
    email_topic: str,
    sender: str,
    recipient: str,
    language: Literal["í•œêµ­ì–´", "English"],
) -> str:
    if language == "í•œêµ­ì–´":
        return (
            f"{email_topic} ì£¼ì œë¥¼ í¬í•¨í•œ ì´ë©”ì¼ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”.\n\n"
            f"ë³´ë‚¸ ì‚¬ëŒ: {sender}\n"
            f"ë°›ëŠ” ì‚¬ëŒ: {recipient}\n"
            f"ì „ë¶€ {language}ë¡œ ë²ˆì—­í•´ì„œ ì‘ì„±í•´ì£¼ì„¸ìš”. í•œë¬¸ì€ ë‚´ìš©ì—ì„œ ì œì™¸í•´ì£¼ì„¸ìš”.\n\n"
            f"ì´ë©”ì¼ ë‚´ìš©:\n"
        )
    else:
        return (
            f"Write an email including the topic: {email_topic}.\n\n"
            f"Sender: {sender}\n"
            f"Recipient: {recipient}\n"
            f"Please write the entire email in {language}.\n\n"
            f"Email content:\n"
        )


# ------------------------------
# LLM í˜¸ì¶œ
#  - ollama.chat()ì„ ì‚¬ìš©í•´ ë¡œì»¬ ì„œë²„(ê¸°ë³¸ 127.0.0.1:11434)ì— ìš”ì²­
#  - system í”„ë¡¬í”„íŠ¸ë¡œ ì—­í•  ë¶€ì—¬(ì •ì¤‘í•˜ê³  ê°„ê²°í•˜ê²Œ, ì´ë©”ì¼ í˜•ì‹ ìœ ì§€)
#  - ì‘ë‹µì—ì„œ <think> ì œê±°
# ------------------------------
def generate_email(
    email_topic: str,
    email_sender: str,
    email_recipient: str,
    language: Literal["í•œêµ­ì–´", "English"] = "í•œêµ­ì–´",
    temperature: float = DEFAULT_TEMPERATURE,
) -> str:
    # ê°„ë‹¨í•œ ìœ íš¨ì„± ê²€ì‚¬
    if not email_topic.strip():
        return "â— ì´ë©”ì¼ ì£¼ì œë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”."
    if not email_sender.strip():
        return "â— ë³´ë‚¸ ì‚¬ëŒ ì´ë¦„ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”."
    if not email_recipient.strip():
        return "â— ë°›ëŠ” ì‚¬ëŒ ì´ë¦„ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”."

    prompt = build_prompt(email_topic, email_sender, email_recipient, language)

    # system ë©”ì‹œì§€ë¡œ ì´ë©”ì¼ ê°€ì´ë“œë¼ì¸ ì œê³µ
    system_instruction = (
        "You are an assistant that writes well-structured, polite, and concise emails. "
        "Use clear subject lines if implied, a friendly greeting, organized paragraphs, "
        "and an appropriate closing with the sender's name."
    )

    try:
        # Ollama Chat API í˜¸ì¶œ
        res = ollama.chat(
            model=MODEL_NAME,
            messages=[
                {"role": "system", "content": system_instruction},
                {"role": "user", "content": prompt},
            ],
            options={
                "temperature": float(temperature),
                "num_predict": int(MAX_TOKENS),
            },
        )
        raw_text = res.get("message", {}).get("content", "")
        return strip_reasoning(raw_text) or "âš ï¸ ëª¨ë¸ì´ ë¹ˆ ì‘ë‹µì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸ë¥¼ ë°”ê¿” ë‹¤ì‹œ ì‹œë„í•´ ë³´ì„¸ìš”."
    except Exception as e:
        # Ollama ì„œë²„ ë¯¸ì‹¤í–‰/ëª¨ë¸ ë¯¸ì„¤ì¹˜/ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ë“±
        return (
            "ğŸš« LLM í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\n"
            f"- ì›ì¸: {e}\n\n"
            "í™•ì¸í•˜ì„¸ìš”:\n"
            "1) Ollamaê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ (ê¸°ë³¸ í¬íŠ¸ 11434)\n"
            "2) `ollama pull deepseek-r1:8b`ë¡œ ëª¨ë¸ì´ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€\n"
            "3) ë°©í™”ë²½/í”„ë¡ì‹œ ë“± ë„¤íŠ¸ì›Œí¬ ì´ìŠˆê°€ ì—†ëŠ”ì§€"
        )


# ------------------------------
# Gradio UI
#  - ì–¸ì–´ ì„ íƒ, ì£¼ì œ, ë³´ë‚¸ ì‚¬ëŒ/ë°›ëŠ” ì‚¬ëŒ ì…ë ¥
#  - ìƒì„± ë²„íŠ¼ í´ë¦­ ì‹œ generate_email í˜¸ì¶œ
#  - ì¢Œì¸¡: ì…ë ¥, ìš°ì¸¡: ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°
# ------------------------------
with gr.Blocks(title="ì´ë©”ì¼ ìƒì„±ê¸° ğŸ“® (Ollama + DeepSeek-R1)") as demo:
    gr.Markdown(
        """
        # ì´ë©”ì¼ ìƒì„±ê¸° ğŸ“®
        ë¡œì»¬ ì˜¤í”ˆì†ŒìŠ¤ LLM(DeepSeek-R1, Ollama)ì„ ì‚¬ìš©í•˜ì—¬ ì´ë©”ì¼ì„ ìƒì„±í•©ë‹ˆë‹¤.  
        **ëª¨ë¸:** `deepseek-r1:8b` (ì„¤ì •ì—ì„œ ë³€ê²½ ê°€ëŠ¥)
        """
    )

    with gr.Row():
        with gr.Column(scale=1):
            language = gr.Dropdown(
                label="ì´ë©”ì¼ ì‘ì„± ì–¸ì–´",
                choices=["í•œêµ­ì–´", "English"],
                value="í•œêµ­ì–´",
            )
            topic = gr.Textbox(label="ì´ë©”ì¼ ì£¼ì œ", placeholder="ì˜ˆ: íšŒì˜ ì¼ì • ì¡°ìœ¨ ìš”ì²­", lines=3)
            with gr.Row():
                sender = gr.Textbox(label="ë³´ë‚¸ ì‚¬ëŒ ì´ë¦„", placeholder="í™ê¸¸ë™")
                recipient = gr.Textbox(label="ë°›ëŠ” ì‚¬ëŒ ì´ë¦„", placeholder="ê¹€ì² ìˆ˜")

            temperature = gr.Slider(
                0.0, 1.5, value=DEFAULT_TEMPERATURE, step=0.05, label="ì°½ì˜ì„± (temperature)"
            )
            generate_btn = gr.Button("ìƒì„±í•˜ê¸°", variant="primary")

        with gr.Column(scale=1):
            output = gr.Markdown(label="ìƒì„±ëœ ì´ë©”ì¼")

    # ë²„íŠ¼ í´ë¦­ â†’ í•¨ìˆ˜ ì‹¤í–‰ ë§¤í•‘
    generate_btn.click(
        fn=generate_email,
        inputs=[topic, sender, recipient, language, temperature],
        outputs=[output],
    )

    gr.Markdown(
        """
        ---  
        ### íŒ
        - ë” ìƒì„¸í•œ ê²°ê³¼ë¥¼ ì›í•˜ì‹œë©´ ì£¼ì œì— **ëª©ì /ìƒí™©/í†¤**ì„ êµ¬ì²´ì ìœ¼ë¡œ ì ì–´ë³´ì„¸ìš”.  
        - í•œêµ­ì–´ ëª¨ë“œì—ì„œëŠ” *í•œë¬¸ ì œì™¸* ê·œì¹™ì´ ì ìš©ë©ë‹ˆë‹¤.  
        - ëª¨ë¸ í¬ê¸°ë¥¼ ë°”ê¾¸ë ¤ë©´ íŒŒì¼ ìƒë‹¨ì˜ `MODEL_NAME` ê°’ì„ ìˆ˜ì •í•˜ì„¸ìš”.
        """
    )

# Gradio ì•± ì‹¤í–‰
if __name__ == "__main__":
    # queue()ëŠ” ë™ì‹œ ìš”ì²­ ì²˜ë¦¬ ì•ˆì •ì„± í–¥ìƒì„ ìœ„í•œ ê¶Œì¥ ì„¤ì •
    demo.queue().launch(server_name="0.0.0.0", server_port=7860, show_api=False)




