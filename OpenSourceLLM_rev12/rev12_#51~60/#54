#54
# restaurant_review_chain_ollama.py
# ------------------------------------------------------------
# ì˜¤í”ˆì†ŒìŠ¤ LLM(DeepSeek-R1, Ollama ë¡œì»¬) + Gradio
# ê¸°ì¡´ LangChain(OpenAI) ì²´ì¸:
#   1) ë¦¬ë·° ìš”ì•½ -> 2) 0~10 ê°ì„± ì ìˆ˜(ìˆ«ìë§Œ) -> 3) ê³µì†í•œ ë‹µë³€
# ì„ ë™ì¼ ë‹¨ê³„ë¡œ ì¬í˜„í•©ë‹ˆë‹¤.
# ------------------------------------------------------------

import re
import requests
import gradio as gr
from typing import Optional

OLLAMA_HOST = "http://localhost:11434"
DEFAULT_MODEL = "deepseek-r1"   # í•„ìš” ì‹œ: `ollama pull deepseek-r1`

# -----------------------------
# DeepSeek ê³„ì—´ ì¶œë ¥ ì •ë¦¬ ìœ í‹¸
# -----------------------------
def strip_think(text: str) -> str:
    """DeepSeek-R1ì´ ì¶œë ¥í•˜ëŠ” <think>...</think> ë‚´ë¶€ ì‚¬ê³  ê³¼ì •ì„ ì œê±°."""
    return re.sub(r"<think>.*?</think>", "", text, flags=re.DOTALL).strip()

# -----------------------------
# Ollama í˜¸ì¶œ
# -----------------------------
def ollama_generate(prompt: str, model: str = DEFAULT_MODEL, temperature: float = 0.7, host: str = OLLAMA_HOST) -> str:
    """
    Ollama /api/generate í˜¸ì¶œ (non-stream).
    """
    resp = requests.post(
        f"{host}/api/generate",
        json={"model": model, "prompt": prompt, "temperature": temperature, "stream": False},
        timeout=60,
    )
    if resp.status_code != 200:
        raise RuntimeError(f"Ollama í˜¸ì¶œ ì‹¤íŒ¨(status={resp.status_code}): {resp.text[:300]}")
    data = resp.json()
    return data.get("response", "")

# -----------------------------
# ì²´ì¸ 1: ìš”ì•½
# -----------------------------
def summarize_review(review: str, model: str, temperature: float) -> str:
    """
    ì…ë ¥ ë¦¬ë·°ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½.
    """
    prompt = (
        "ë‹¤ìŒ ì‹ë‹¹ ë¦¬ë·°ë¥¼ í•œêµ­ì–´ë¡œ í•œ ë¬¸ì¥ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ìš”ì•½í•˜ì„¸ìš”.\n\n"
        f"ë¦¬ë·°:\n{review}\n\n"
        "ìš”ì•½ì€ 1ë¬¸ì¥ë§Œ ì¶œë ¥í•˜ì„¸ìš”."
    )
    raw = ollama_generate(prompt, model=model, temperature=temperature)
    return strip_think(raw)

# -----------------------------
# ì²´ì¸ 2: ê°ì„± ì ìˆ˜(0~10)
# -----------------------------
def score_sentiment(review: str, model: str, temperature: float) -> str:
    """
    ë¦¬ë·°ë¥¼ ì½ê³  0~10 ì‚¬ì´ 'ì •ìˆ˜ í•œ ê°œ'ë§Œ ì¶œë ¥í•˜ë„ë¡ ê°•ì œ.
    """
    prompt = (
        "ë‹¤ìŒ ì‹ë‹¹ ë¦¬ë·°ì˜ ì „ë°˜ì  ë§Œì¡±ë„ë¥¼ 0~10 ì‚¬ì´ **ì •ìˆ˜ í•œ ê°œ**ë¡œ í‰ê°€í•˜ì„¸ìš”.\n"
        "- 0ì€ ë§¤ìš° ë¶€ì •ì , 10ì€ ë§¤ìš° ê¸ì •ì ì…ë‹ˆë‹¤.\n"
        "- ë°˜ë“œì‹œ ìˆ«ìë§Œ ì¶œë ¥í•˜ì„¸ìš”(ì˜ˆ: 7).\n\n"
        f"ë¦¬ë·°:\n{review}"
    )
    raw = ollama_generate(prompt, model=model, temperature=temperature)
    cleaned = strip_think(raw)
    # ì²« ë²ˆì§¸ ì •ìˆ˜ ì¶”ì¶œ
    m = re.search(r"\b([0-9]|10)\b", cleaned)
    if not m:
        # ìˆ«ìë¥¼ ëª» ì°¾ì€ ê²½ìš°, ì•ˆì „í•œ ê¸°ë³¸ê°’ 5 ë°˜í™˜
        return "5"
    return m.group(1)

# -----------------------------
# ì²´ì¸ 3: ê³µì†í•œ ë‹µë³€
# -----------------------------
def polite_reply(summary: str, model: str, temperature: float) -> str:
    """
    ìš”ì•½ì„ ë°”íƒ•ìœ¼ë¡œ ê³µì†í•˜ê³  ê°„ê²°í•œ ë‹µë³€ ì‘ì„±.
    """
    prompt = (
        "ì•„ë˜ ì‹ë‹¹ ë¦¬ë·° ìš”ì•½ì— ëŒ€í•´, ì„œë¹„ìŠ¤ ì œê³µì(ì‹ë‹¹) ì…ì¥ì—ì„œ ê³µì†í•˜ê³  ê°„ê²°í•œ í•œêµ­ì–´ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.\n"
        "- 2~4ë¬¸ì¥\n- ê³¼ë„í•œ ì‚¬ê³¼/í™ë³´ëŠ” í”¼í•˜ê³ , ê°œì„  ì˜ì§€ì™€ ê°ì‚¬ í‘œí˜„ì„ ê· í˜• ìˆê²Œ í¬í•¨\n\n"
        f"ë¦¬ë·° ìš”ì•½:\n{summary}"
    )
    raw = ollama_generate(prompt, model=model, temperature=temperature)
    return strip_think(raw)
# -----------------------------
# ì „ì²´ íŒŒì´í”„ë¼ì¸
# -----------------------------
def run_chain(review: str, model: str, temperature: float):
    """
    1) ìš”ì•½ -> 2) ì ìˆ˜ -> 3) ë‹µë³€
    """
    review = (review or "").strip()
    if not review:
        return "ë¦¬ë·°ë¥¼ ì…ë ¥í•˜ì„¸ìš”.", "", ""

    try:
        summary = summarize_review(review, model, temperature)
    except Exception as e:
        return f"[ìš”ì•½ ì˜¤ë¥˜] {e}", "", ""

    try:
        score = score_sentiment(review, model, temperature)
    except Exception as e:
        return summary, f"[ì ìˆ˜ ì˜¤ë¥˜] {e}", ""

    try:
        reply = polite_reply(summary, model, temperature)
    except Exception as e:
        return summary, score, f"[ë‹µë³€ ì˜¤ë¥˜] {e}"

    return summary, score, reply

# -----------------------------
# Gradio UI
# -----------------------------
EX_REVIEW = """ì´ ì‹ë‹¹ì€ ë§›ë„ ì¢‹ê³  ë¶„ìœ„ê¸°ë„ ì¢‹ì•˜ìŠµë‹ˆë‹¤. ê°€ê²© ëŒ€ë¹„ ë§Œì¡±ë„ê°€ ë†’ì•„ìš”.
í•˜ì§€ë§Œ, ì„œë¹„ìŠ¤ ì†ë„ê°€ ë„ˆë¬´ ëŠë ¤ì„œ ì¡°ê¸ˆ ì‹¤ë§ìŠ¤ëŸ¬ì› ìŠµë‹ˆë‹¤.
ì „ë°˜ì ìœ¼ë¡œëŠ” ë‹¤ì‹œ ë°©ë¬¸í•  ì˜ì‚¬ê°€ ìˆìŠµë‹ˆë‹¤.
"""

with gr.Blocks(title="OpenCode - ì‹ë‹¹ ë¦¬ë·° ì²´ì¸ (Ollama + DeepSeek-R1)") as demo:
    gr.Markdown("## ğŸ§© ì‹ë‹¹ ë¦¬ë·° ì²´ì¸\në¡œì»¬ **Ollama + DeepSeek-R1**ë¡œ ë™ì‘í•©ë‹ˆë‹¤. API Keyê°€ í•„ìš” ì—†ìŠµë‹ˆë‹¤.")

    with gr.Row():
        review_tb = gr.Textbox(label="ì‹ë‹¹ ë¦¬ë·° ì…ë ¥", value=EX_REVIEW, lines=8, placeholder="ë¦¬ë·°ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    with gr.Row():
        model_tb = gr.Textbox(label="ëª¨ë¸ ì´ë¦„", value=DEFAULT_MODEL, placeholder="ì˜ˆ: deepseek-r1 / qwen2.5:14b ë“±")
        temp_sl = gr.Slider(0.0, 1.5, value=0.7, step=0.1, label="Temperature")

    run_btn = gr.Button("ì²´ì¸ ì‹¤í–‰")

    with gr.Row():
        out_summary = gr.Textbox(label="ìš”ì•½ (ì²´ì¸1)", lines=3)
    with gr.Row():
        out_score = gr.Textbox(label="ê°ì„± ì ìˆ˜ 0~10 (ì²´ì¸2)", lines=1)
    with gr.Row():
        out_reply = gr.Textbox(label="ê³µì†í•œ ë‹µë³€ (ì²´ì¸3)", lines=6)

    run_btn.click(
        fn=run_chain,
        inputs=[review_tb, model_tb, temp_sl],
        outputs=[out_summary, out_score, out_reply]
    )

if __name__ == "__main__":
    # ì²« ì‹¤í–‰ ì „ ëª¨ë¸ ì„¤ì¹˜ í•„ìš”:
    #   $ ollama pull deepseek-r1
    demo.launch()
