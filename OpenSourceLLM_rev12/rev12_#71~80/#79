#79
"""
Gemini ë©€í‹°ëª¨ë‹¬(ì´ë¯¸ì§€/ì˜¤ë””ì˜¤/ë¹„ë””ì˜¤) ì˜ˆì œë¥¼
ë¡œì»¬ ì˜¤í”ˆì†ŒìŠ¤ ìŠ¤íƒ(Ollama + DeepSeek-R1, LLaVA/Qwen-VL, Faster-Whisper)ê³¼ Gradio GUIë¡œ ë³€í™˜í•œ ìŠ¤í¬ë¦½íŠ¸.

â–  ì „ì œ
- ë¡œì»¬ PCì— Ollama ì„¤ì¹˜ ë° ì‹¤í–‰ ì¤‘ (ê¸°ë³¸: http://localhost:11434)
- í…ìŠ¤íŠ¸ ì¶”ë¡ : deepseek-r1 (ì˜ˆ: `ollama pull deepseek-r1`)
- ë¹„ì „ ëª¨ë¸(ì´ë¯¸ì§€/ë¹„ë””ì˜¤ í”„ë ˆì„ ì„¤ëª…): LLaVA, Llama 3.2 Vision, Qwen2.5-VL ë“± ì¤‘ í•˜ë‚˜ ì„¤ì¹˜
  (ì˜ˆ: `ollama pull llava:latest` ë˜ëŠ” `ollama pull llama3.2-vision` ë˜ëŠ” `ollama pull qwen2.5-vl`)
- ì˜¤ë””ì˜¤(ASR): Faster-Whisper ë¡œì»¬ ì¶”ë¡  (ìµœì´ˆ ì‹¤í–‰ ì‹œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ)
- ì™¸ë¶€ APIí‚¤ ë¯¸ì‚¬ìš©, 100% ë¡œì»¬ ë™ì‘

â–  ì œê³µ ê¸°ëŠ¥ (ì›ë³¸ ê¸°ëŠ¥ê³¼ ë§¤í•‘)
1) ì´ë¯¸ì§€ ì§ˆì˜: ì´ë¯¸ì§€ + ì§ˆë¬¸ -> ë¹„ì „ ëª¨ë¸ì´ ì„¤ëª…/ë¶„ë¥˜(ì›ë¬¸ì˜ generate_content([image, prompt]))
2) ì˜¤ë””ì˜¤ ìš”ì•½/ì „ì‚¬: ìŒì„± ë‚´ìš© ê°„ë‹¨ ìœ ì¶” + ì›ë¬¸ ì „ì‚¬(ì›ë¬¸ì˜ audio ì²˜ë¦¬)
3) ë¹„ë””ì˜¤ ì„¤ëª…: ë™ì˜ìƒì„ ì—…ë¡œë“œí•˜ë©´ í‚¤í”„ë ˆì„ ì¶”ì¶œ í›„ ë¹„ì „ ëª¨ë¸ë¡œ ìš”ì•½(ì›ë¬¸ì˜ video ì„¤ëª…)
4) í…ìŠ¤íŠ¸ í›„ì²˜ë¦¬/ìš”ì•½: DeepSeek-R1ë¡œ í•œêµ­ì–´ ì •ë¦¬/ìš”ì•½(ì„ íƒ)
5) Gradio GUI íƒ­: Image / Audio / Video

ì‘ì„±: OpenCode
"""

# ========================
# ì˜ì¡´ íŒ¨í‚¤ì§€ ì„¤ì¹˜(í•„ìš” ì‹œ ìë™ ì„¤ì¹˜)
# ========================
try:
    import ollama
except Exception:
    import sys, subprocess
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "ollama"])
    import ollama

try:
    import gradio as gr
except Exception:
    import sys, subprocess
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "gradio>=4.44.0"])  
    import gradio as gr

try:
    import cv2
except Exception:
    import sys, subprocess
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "opencv-python"])
    import cv2

try:
    from faster_whisper import WhisperModel
except Exception:
    import sys, subprocess
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "faster-whisper"])
    from faster_whisper import WhisperModel

from dataclasses import dataclass
from typing import List, Optional, Tuple
import os, json, tempfile, math

OLLAMA_HOST = os.environ.get("OLLAMA_HOST", "http://localhost:11434")

# ========================
# Ollama í—¬í¼
# ========================

def ollama_chat(model: str, prompt: str, images: Optional[List[str]] = None, 
                system: str = "", options: Optional[dict] = None) -> str:
    """Ollama Chat API ë˜í¼. images: íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸(ë¹„ì „ ëª¨ë¸ì—ì„œ ì‚¬ìš©)."""
    client = ollama.Client(host=OLLAMA_HOST)
    msgs = []
    if system:
        msgs.append({"role": "system", "content": system})
    msg = {"role": "user", "content": prompt}
    if images:
        # OllamaëŠ” message-levelì— images í•„ë“œ í—ˆìš©
        msg["images"] = images
    msgs.append(msg)

    res = client.chat(model=model, messages=msgs, options=options or {})
    return res.get("message", {}).get("content", "")

# ========================
# Faster-Whisper(ASR)
# ========================
@dataclass
class ASRConfig:
    model_size: str = "small"  # tiny/base/small/medium/large-v3 ë“±
    device: str = "auto"       # "auto", "cpu", "cuda"
    compute_type: str = "auto" # "int8", "int8_float16", "float16", "int8" ë“±

class ASR:
    def __init__(self, cfg: ASRConfig):
        self.cfg = cfg
        self.model = WhisperModel(cfg.model_size, device=cfg.device, compute_type=cfg.compute_type)

    def transcribe(self, audio_path: str, language: Optional[str] = None) -> str:
        """ì˜¤ë””ì˜¤ ì „ì‚¬. language=Noneì´ë©´ ìë™ ê°ì§€."""
        segments, info = self.model.transcribe(audio_path, language=language)
        text = " ".join(s.text.strip() for s in segments)
        return text.strip()

# ========================
# ë¹„ë””ì˜¤ ìœ í‹¸: í‚¤í”„ë ˆì„(ë˜ëŠ” ë“±ê°„ê²© í”„ë ˆì„) ì¶”ì¶œ
# ========================

def extract_frames(video_path: str, every_n_seconds: float = 2.0, max_frames: int = 12) -> List[str]:
    """ë™ì˜ìƒì—ì„œ ì¼ì • ê°„ê²©ìœ¼ë¡œ í”„ë ˆì„ ì¶”ì¶œ í›„ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜."""
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise RuntimeError("ë¹„ë””ì˜¤ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: " + video_path)

    fps = cap.get(cv2.CAP_PROP_FPS) or 25.0
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    duration = total_frames / max(fps, 1e-5)

    interval = max(every_n_seconds, 0.5)
    frame_ids = []
    for t in [i*interval for i in range(0, int(math.ceil(duration/interval)) + 1)]:
        f = int(t * fps)
        if f < total_frames:
            frame_ids.append(f)
    # í”„ë ˆì„ ìˆ˜ ì œí•œ
    frame_ids = frame_ids[:max_frames]

    out_paths = []
    with tempfile.TemporaryDirectory() as td:
        # ì„ì‹œ ë””ë ‰í† ë¦¬ ë‚´ì— ì €ì¥ í›„ ë³µì‚¬(Gradio ë°˜í™˜ìš©) -> ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ /tmp ê²½ë¡œ ì¬ì‚¬ìš©
        pass
    # ì„ì‹œ ë””ë ‰í† ë¦¬ëŠ” í•¨ìˆ˜ ì¢…ë£Œ ì‹œ ì‚¬ë¼ì§€ë¯€ë¡œ, ì‹¤ì œ ì €ì¥ì€ ê³ ì • temp dirì— í•˜ì
    persist = tempfile.mkdtemp(prefix="frames_")

    for idx, fid in enumerate(frame_ids):
        cap.set(cv2.CAP_PROP_POS_FRAMES, fid)
        ok, frame = cap.read()
        if not ok:
            continue
        outp = os.path.join(persist, f"frame_{idx:03d}.jpg")
        cv2.imwrite(outp, frame)
        out_paths.append(outp)

    cap.release()
    return out_paths

# ========================
# ê³ ìˆ˜ì¤€ íƒœìŠ¤í¬
# ========================

def describe_image(vision_model: str, image_path: str, question_ko: str = "ì´ê²ƒì€ ë¬´ìŠ¨ ì´ë¯¸ì§€ì…ë‹ˆê¹Œ?") -> str:
    prompt = (
        "ë‹¹ì‹ ì€ ì‹œê° ë¬˜ì‚¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ë‹µí•˜ì„¸ìš”.\n" 
        f"ì§ˆë¬¸: {question_ko}\n"
        "ê°€ëŠ¥í•˜ë©´ í•µì‹¬ í‚¤ì›Œë“œë„ 3~5ê°œ ì œì‹œí•˜ì„¸ìš”."
    )
    return ollama_chat(vision_model, prompt, images=[image_path])


def infer_audio(audio_path: str, asr: ASR, summarizer_model: Optional[str] = None) -> Tuple[str, Optional[str]]:
    """ì˜¤ë””ì˜¤ ì „ì‚¬ + (ì„ íƒ) ìš”ì•½/ì£¼ì œ ìœ ì¶”."""
    transcript = asr.transcribe(audio_path)
    summary = None
    if summarizer_model:
        prompt = (
            "ë‹¤ìŒ ì „ì‚¬ í…ìŠ¤íŠ¸ì˜ ì£¼ì œì™€ ìš”ì ì„ í•œêµ­ì–´ë¡œ 3~5ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”.\n"
            "- í•µì‹¬ í‚¤ì›Œë“œ 5ê°œ\n- ì²­ì¤‘ ëŒ€ìƒ(ìˆë‹¤ë©´)\n- í†¤/ìŠ¤íƒ€ì¼\n\nì „ì‚¬:\n" + transcript
        )
        summary = ollama_chat(summarizer_model, prompt)
    return transcript, summary


def describe_video(vision_model: str, video_path: str, seconds_interval: float = 2.0, max_frames: int = 12) -> str:
    frames = extract_frames(video_path, seconds_interval, max_frames)
    if not frames:
        return "í”„ë ˆì„ì„ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ë™ì˜ìƒì„ ì‹œë„í•˜ê±°ë‚˜ ê°„ê²©ì„ ëŠ˜ë ¤ë³´ì„¸ìš”."

    prompt = (
        "ë‹¹ì‹ ì€ ë¹„ë””ì˜¤ ì„¤ëª…ê°€ì…ë‹ˆë‹¤. ì²¨ë¶€ëœ í”„ë ˆì„ë“¤ì„ ì‹œê°„ ìˆœì„œë¡œ ë³´ê³ , í•˜ë‚˜ì˜ ì˜ìƒì´ë¼ê³  ê°€ì •í•˜ê³  í•œêµ­ì–´ë¡œ ë¬˜ì‚¬í•˜ì„¸ìš”.\n"
        "- ì „ì²´ ì¤„ê±°ë¦¬/í–‰ë™\n- ì¥ë©´ ì „í™˜/ë³€í™” í¬ì¸íŠ¸\n- ë“±ì¥ ì¸ë¬¼/ì‚¬ë¬¼\n- ë¶„ìœ„ê¸°/ì¥ë¥´\n- 5ê°œ ë‚´ì™¸ í•µì‹¬ í‚¤ì›Œë“œ\n"
    )
    # ì¼ë¶€ ë¹„ì „ ëª¨ë¸ì€ ì´ë¯¸ì§€ ì—¬ëŸ¬ ì¥ì„ í—ˆìš©. OllamaëŠ” images=[...]
    return ollama_chat(vision_model, prompt, images=frames)

# ========================
# Gradio UI
# ========================
DEFAULT_TEXT_MODEL = os.environ.get("OC_TEXT_MODEL", "deepseek-r1")
DEFAULT_VISION_MODEL = os.environ.get("OC_VISION_MODEL", "llava:latest")

asr_engine: Optional[ASR] = None

with gr.Blocks(title="OpenCode â€¢ ë©€í‹°ëª¨ë‹¬(ì´ë¯¸ì§€/ì˜¤ë””ì˜¤/ë¹„ë””ì˜¤) - ë¡œì»¬") as demo:
    gr.Markdown("""
    # ğŸ§© OpenCode â€¢ ë©€í‹°ëª¨ë‹¬ (ë¡œì»¬ ì „ìš©)
    - í…ìŠ¤íŠ¸ ì¶”ë¡ : **DeepSeek-R1** (Ollama)
    - ë¹„ì „: **LLaVA / Llama3.2-Vision / Qwen2.5-VL** ì¤‘ ì„ íƒ (Ollama)
    - ìŒì„±ì¸ì‹: **Faster-Whisper** (ë¡œì»¬)
    - ì™¸ë¶€ API/í‚¤ **ë¶ˆí•„ìš”**
    """)

    with gr.Row():
        txt_model = gr.Textbox(value=DEFAULT_TEXT_MODEL, label="í…ìŠ¤íŠ¸ ëª¨ë¸(ìš”ì•½/í›„ì²˜ë¦¬)")
        vis_model = gr.Textbox(value=DEFAULT_VISION_MODEL, label="ë¹„ì „ ëª¨ë¸(ì´ë¯¸ì§€/ë¹„ë””ì˜¤)")
        with gr.Column():
            asr_size = gr.Dropdown(["tiny", "base", "small", "medium", "large-v3"], value="small", label="ASR ëª¨ë¸ í¬ê¸° (Faster-Whisper)")
            asr_dev = gr.Dropdown(["auto", "cpu", "cuda"], value="auto", label="ASR ë””ë°”ì´ìŠ¤")
            asr_ctype = gr.Dropdown(["auto", "int8", "int8_float16", "float16"], value="auto", label="ASR ì—°ì‚°ì •ë°€ë„")
            init_asr_btn = gr.Button("ASR ì´ˆê¸°í™”", variant="secondary")
            asr_status = gr.Markdown("ASR ì—”ì§„: ì´ˆê¸°í™” í•„ìš”")

    def init_asr(size, dev, ctype):
        global asr_engine
        asr_engine = ASR(ASRConfig(model_size=size, device=dev, compute_type=ctype))
        return "âœ… ASR ì—”ì§„ ì¤€ë¹„ ì™„ë£Œ"

    init_asr_btn.click(init_asr, [asr_size, asr_dev, asr_ctype], asr_status)

    with gr.Tabs():
        # ---------- ì´ë¯¸ì§€
        with gr.Tab("Image Q&A"):
            img = gr.Image(type="filepath", label="ì´ë¯¸ì§€ ì—…ë¡œë“œ")
            question = gr.Textbox(value="ì´ê²ƒì€ ë¬´ìŠ¨ ì´ë¯¸ì§€ì…ë‹ˆê¹Œ?", label="ì§ˆë¬¸")
            run_img = gr.Button("ì´ë¯¸ì§€ ì§ˆì˜ ì‹¤í–‰", variant="primary")
            out_img = gr.Markdown()

            def on_image(vmodel, image_path, q):
                if not image_path:
                    return "ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”."
                return describe_image(vmodel, image_path, q)

            run_img.click(on_image, [vis_model, img, question], out_img)

        # ---------- ì˜¤ë””ì˜¤
        with gr.Tab("Audio: ìš”ì•½ & ì „ì‚¬"):
            au = gr.Audio(type="filepath", label="ì˜¤ë””ì˜¤ ì—…ë¡œë“œ (wav/mp3 ë“±)")
            with gr.Row():
                do_summary = gr.Checkbox(value=True, label="ìš”ì•½/ì£¼ì œ ìœ ì¶” ìˆ˜í–‰(DeepSeek-R1)")
                run_au = gr.Button("ì˜¤ë””ì˜¤ ì²˜ë¦¬", variant="primary")
            out_summary = gr.Markdown(label="ìš”ì•½")
            out_trans = gr.Textbox(label="ì „ì‚¬", lines=10)

            def on_audio(tmodel, audio_path, want_summary):
                if asr_engine is None:
                    return "ASR ì—”ì§„ì„ ë¨¼ì € ì´ˆê¸°í™”í•˜ì„¸ìš”.", ""
                if not audio_path:
                    return "ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.", ""
                transcript, summary = infer_audio(audio_path, asr_engine, tmodel if want_summary else None)
                return (summary or "ìš”ì•½(ë¹„í™œì„±í™”)"), transcript

            run_au.click(on_audio, [txt_model, au, do_summary], [out_summary, out_trans])

        # ---------- ë¹„ë””ì˜¤
        with gr.Tab("Video ì„¤ëª…"):
            vd = gr.Video(label="ë¹„ë””ì˜¤ ì—…ë¡œë“œ")
            with gr.Row():
                sec = gr.Slider(0.5, 5.0, value=2.0, step=0.5, label="í”„ë ˆì„ ê°„ê²©(ì´ˆ)")
                mxf = gr.Slider(4, 24, value=12, step=1, label="ìµœëŒ€ í”„ë ˆì„ ìˆ˜")
            run_v = gr.Button("ë¹„ë””ì˜¤ ì„¤ëª… ìƒì„±", variant="primary")
            out_v = gr.Markdown()

            def on_video(vmodel, video_path, s, mf):
                if not video_path:
                    return "ë¹„ë””ì˜¤ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”."
                return describe_video(vmodel, video_path, s, int(mf))

            run_v.click(on_video, [vis_model, vd, sec, mxf], out_v)

    gr.Markdown("""
    ---
    ### ì‚¬ìš© íŒ
    - ë¹„ì „ ëª¨ë¸ ì˜ˆì‹œ: `llava:latest`, `llama3.2-vision`, `qwen2.5-vl` (ë¡œì»¬ì— pull í•„ìš”)
    - DeepSeek-R1ì€ í…ìŠ¤íŠ¸ ì •ë¦¬/ìš”ì•½ì— ì‚¬ìš©(ë¹„ì „ ìì²´ëŠ” ìœ„ ëª¨ë¸ë“¤ì´ ìˆ˜í–‰)
    - Faster-WhisperëŠ” ì²« ì‹¤í–‰ ì‹œ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ë¯€ë¡œ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    - ê¸´ ë™ì˜ìƒì€ í”„ë ˆì„ ê°„ê²©ì„ ëŠ˜ë¦¬ê±°ë‚˜ ìµœëŒ€ í”„ë ˆì„ ìˆ˜ë¥¼ ì¤„ì´ì„¸ìš”.
    """)

if __name__ == "__main__":
    # ë¡œì»¬ ì„œë¹„ìŠ¤ ì‹œì‘
    demo.launch(server_name="0.0.0.0", server_port=7861, share=False)

# ollama pull deepseek-r1
# ollama pull llava:latest 