#80
"""
Gemini ì„ë² ë”© ì˜ˆì œ(Google Generative AI) âœ ì˜¤í”ˆì†ŒìŠ¤ ë¡œì»¬ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
- ì™¸ë¶€ API í‚¤ ì—†ì´ **ë¡œì»¬**ì—ì„œ ë™ì‘
- ë°±ì—”ë“œ 2ì¢… ì§€ì›:
  1) **Ollama Embeddings** (ì˜ˆ: nomic-embed-text, mxbai-embed-large, jina-embeddings ë“±)
  2) **FlagEmbedding â€¢ BGE-M3** (HF ë¡œì»¬ ê°€ì¤‘ì¹˜)
- **FAISS** ë¡œ ìµœê·¼ì ‘ íƒìƒ‰(NN) ìˆ˜í–‰
- **Gradio** GUI ì œê³µ(ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ / ì¸ë±ìŠ¤ ë¹Œë“œ)

ì›ë³¸ ê¸°ëŠ¥ ëŒ€ì‘í‘œ
- ëª¨ë¸ ë‚˜ì—´(Gemini): Ollamaì— ì„¤ì¹˜ëœ ì„ë² ë”© ëª¨ë¸ ì´ë¦„ ì…ë ¥(ë˜ëŠ” bge-m3 ë¡œì»¬)
- embed_content: âœ `ollama.embeddings()` ë˜ëŠ” `BGEM3FlagModel.encode()`
- FAISS ê²€ìƒ‰: ë™ì¼í•˜ê²Œ IndexFlat(L2/IP)ë¡œ êµ¬í˜„

ì‘ì„±: OpenCode
"""

# ========================
# ì˜ì¡´ íŒ¨í‚¤ì§€ ì„¤ì¹˜(í•„ìš” ì‹œ)
# ========================
try:
    import ollama
except Exception:
    import sys, subprocess
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "ollama"])
    import ollama

try:
    from FlagEmbedding import BGEM3FlagModel
except Exception:
    import sys, subprocess
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "FlagEmbedding", "peft"])
    from FlagEmbedding import BGEM3FlagModel

try:
    import faiss
except Exception:
    import sys, subprocess
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "faiss-cpu"])
    import faiss

try:
    import gradio as gr
except Exception:
    import sys, subprocess
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "gradio>=4.44.0"])  
    import gradio as gr

import numpy as np
from typing import List, Tuple, Literal, Optional
import json, os

OLLAMA_HOST = os.environ.get("OLLAMA_HOST", "http://localhost:11434")

# ========================
# 1) ì„ë² ë”© ë°±ì—”ë“œ: Ollama
# ========================

def embed_ollama(texts: List[str], model: str = "nomic-embed-text") -> np.ndarray:
    """
    Ollama ì„ë² ë”© ëª¨ë¸ í˜¸ì¶œ.
    - ì‚¬ì „ ì¤€ë¹„: `ollama pull nomic-embed-text` (ë˜ëŠ” mxbai-embed-large, jina-embeddings ë“±)
    - ë°˜í™˜: shape=(N, D) float32 numpy ë°°ì—´
    """
    client = ollama.Client(host=OLLAMA_HOST)
    vecs: List[List[float]] = []
    for t in texts:
        res = client.embeddings(model=model, prompt=t)
        v = res.get("embedding") or res.get("data", [{}])[0].get("embedding")
        if v is None:
            raise RuntimeError("ì„ë² ë”© ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        vecs.append(v)
    arr = np.array(vecs, dtype="float32")
    return arr

# ========================
# 2) ì„ë² ë”© ë°±ì—”ë“œ: BGE-M3 (FlagEmbedding)
# ========================
_bge_model: Optional[BGEM3FlagModel] = None

def get_bge() -> BGEM3FlagModel:
    global _bge_model
    if _bge_model is None:
        # ìµœì´ˆ ë¡œë“œ ì‹œ ë¡œì»¬ ìºì‹œì— ëª¨ë¸ ë‹¤ìš´ë¡œë“œë¨
        _bge_model = BGEM3FlagModel("BAAI/bge-m3", use_fp16=True)
    return _bge_model


def embed_bge_m3(texts: List[str]) -> np.ndarray:
    """BGE-M3 ë¡œì»¬ ì„ë² ë”©(dense) ë°˜í™˜."""
    model = get_bge()
    out = model.encode(texts)
    vecs = out["dense_vecs"]  # (N, D)
    return np.array(vecs, dtype="float32")

# ========================
# 3) ìœ í‹¸: ì •ê·œí™” ë° FAISS ê²€ìƒ‰
# ========================

def l2_normalize(x: np.ndarray, eps: float = 1e-12) -> np.ndarray:
    nrm = np.linalg.norm(x, axis=1, keepdims=True)
    return x / np.maximum(nrm, eps)


def build_faiss_index(vectors: np.ndarray, metric: Literal["cos", "l2"] = "cos") -> faiss.Index:
    """
    metric = "cos": ì½”ì‚¬ì¸ ìœ ì‚¬ë„ â†’ ë²¡í„° ì •ê·œí™” í›„ Inner Product(IndexFlatIP)
    metric = "l2" : ìœ í´ë¦¬ë“œ ê±°ë¦¬ â†’ IndexFlatL2
    """
    d = vectors.shape[1]
    if metric == "cos":
        xb = l2_normalize(vectors.copy())
        index = faiss.IndexFlatIP(d)
        index.add(xb)
        # ì •ê·œí™”ëœ ì¸ë±ìŠ¤ ê°ì²´ì™€ ì›ë³¸ ì‚¬ì´ì¦ˆ ë³´ê´€ì„ ìœ„í•´ attribute ë¶€ì—¬
        index.is_cosine = True  # type: ignore
        return index
    else:
        index = faiss.IndexFlatL2(d)
        index.add(vectors)
        index.is_cosine = False  # type: ignore
        return index


def search_faiss(index: faiss.Index, query_vecs: np.ndarray, topk: int = 1) -> Tuple[np.ndarray, np.ndarray]:
    if getattr(index, "is_cosine", False):  # type: ignore
        q = l2_normalize(query_vecs.copy())
        sims, ids = index.search(q, topk)
        return sims, ids
    else:
        dists, ids = index.search(query_vecs, topk)
        return dists, ids

# ========================
# 4) ë°ëª¨: ì›ë³¸ ì˜ˆì œì™€ ë™ì¼/ìœ ì‚¬ íë¦„
# ========================
EN_INPUT = ["I'm glad it didn't rain today"]
EN_TARGETS = [
    "What is your favorite food?",
    "Where do you live?",
    "Morning trains are crowded.",
    "It's nice weather today.",
    "The economy is bad lately.",
]

KO_INPUT = ["ì˜¤ëŠ˜ì€ ë¹„ê°€ ì•ˆì™€ì„œ ë‹¤í–‰ì…ë‹ˆë‹¤."]
KO_TARGETS = [
    "ì¢‹ì•„í•˜ëŠ” ìŒì‹ì€ ë¬´ì—‡ì¸ê°€ìš”?",
    "ì–´ë””ì— ê±°ì£¼í•˜ì‹œë‚˜ìš”?",
    "ì¶œê·¼ì‹œê°„ì— ì§€í•˜ì² ì€ ë§¤ìš° ë¶ë¹•ë‹ˆë‹¤.",
    "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì°¸ ì¢‹ë„¤ìš”.",
    "ìµœê·¼ ê²½ê¸°ê°€ ì¢‹ì§€ ì•ŠìŠµë‹ˆë‹¤.",
]

# ========================
# 5) Gradio GUI
# ========================
with gr.Blocks(title="OpenCode â€¢ ë¡œì»¬ ì„ë² ë”© + FAISS", theme=gr.themes.Soft()) as demo:
    gr.Markdown("""
    # ğŸ” OpenCode â€¢ ë¡œì»¬ ì„ë² ë”© + FAISS (Ollama / BGE-M3)
    - ë°±ì—”ë“œ: **Ollama ì„ë² ë”©** ë˜ëŠ” **BGE-M3(FlagEmbedding)**
    - ì§€í‘œ: ì½”ì‚¬ì¸/ìœ í´ë¦¬ë“œ ì„ íƒ
    - ì‘ì€ ì˜ˆì œë¡œ ìµœê·¼ì ‘ ë¬¸ì¥ ì°¾ê¸°
    """)

    with gr.Tabs():
        with gr.Tab("Quick Test"):
            with gr.Row():
                backend = gr.Radio(["ollama", "bge-m3"], value="ollama", label="ë°±ì—”ë“œ")
                ollama_model = gr.Textbox(value="nomic-embed-text", label="Ollama ì„ë² ë”© ëª¨ë¸ëª…")
                metric = gr.Radio(["cos", "l2"], value="cos", label="ìœ ì‚¬ë„/ê±°ë¦¬")
                topk = gr.Slider(1, 5, value=1, step=1, label="Top-K")
            gr.Markdown("**ì˜ë¬¸ ì˜ˆì œ** (ì›ë³¸ ì½”ë“œ ëŒ€ì‘)")
            en_in = gr.Textbox(value="\n".join(EN_INPUT), label="ì…ë ¥ í…ìŠ¤íŠ¸(ì—¬ëŸ¬ ì¤„)")
            en_tar = gr.Textbox(value="\n".join(EN_TARGETS), label="íƒ€ê²Ÿ í…ìŠ¤íŠ¸(í•œ ì¤„=í•œ ë¬¸ì¥)")
            run_en = gr.Button("ì˜ë¬¸ ì˜ˆì œ ì‹¤í–‰")
            out_en = gr.JSON(label="ê²°ê³¼")

            def run_example(backend_v, model_v, metric_v, topk_v, q_text, t_text):
                q_list = [s for s in q_text.splitlines() if s.strip()]
                t_list = [s for s in t_text.splitlines() if s.strip()]
                if not t_list:
                    return {"error": "íƒ€ê²Ÿ í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."}
                # ì„ë² ë”©
                if backend_v == "ollama":
                    q_vec = embed_ollama(q_list, model_v)
                    t_vec = embed_ollama(t_list, model_v)
                else:
                    q_vec = embed_bge_m3(q_list)
                    t_vec = embed_bge_m3(t_list)
                # ì¸ë±ìŠ¤ + ê²€ìƒ‰
                index = build_faiss_index(t_vec, metric_v)
                scores, ids = search_faiss(index, q_vec, int(topk_v))
                # ê²°ê³¼ ë¬¶ê¸°
                results = []
                for qi, q in enumerate(q_list):
                    hits = []
                    for rank in range(int(topk_v)):
                        idx = int(ids[qi, rank])
                        score = float(scores[qi, rank])
                        hits.append({"rank": rank+1, "text": t_list[idx], "score": score, "index": idx})
                    results.append({"query": q, "hits": hits})
                return {"backend": backend_v, "metric": metric_v, "model": (model_v if backend_v=="ollama" else "BAAI/bge-m3"), "results": results}

            run_en.click(run_example, [backend, ollama_model, metric, topk, en_in, en_tar], out_en)

            gr.Markdown("**í•œê¸€ ì˜ˆì œ** (ì›ë³¸ ì½”ë“œ ëŒ€ì‘)")
            ko_in = gr.Textbox(value="\n".join(KO_INPUT), label="ì…ë ¥ í…ìŠ¤íŠ¸(ì—¬ëŸ¬ ì¤„)")
            ko_tar = gr.Textbox(value="\n".join(KO_TARGETS), label="íƒ€ê²Ÿ í…ìŠ¤íŠ¸(í•œ ì¤„=í•œ ë¬¸ì¥)")
            run_ko = gr.Button("í•œê¸€ ì˜ˆì œ ì‹¤í–‰")
            out_ko = gr.JSON(label="ê²°ê³¼")

            run_ko.click(run_example, [backend, ollama_model, metric, topk, ko_in, ko_tar], out_ko)

        with gr.Tab("Build Index"):
            gr.Markdown("ì¼ê´„ ë¬¸ì„œë¡œ ì¸ë±ìŠ¤ë¥¼ ë§Œë“¤ê³  ì¿¼ë¦¬ë¡œ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
            backend2 = gr.Radio(["ollama", "bge-m3"], value="ollama", label="ë°±ì—”ë“œ")
            ollama_model2 = gr.Textbox(value="nomic-embed-text", label="Ollama ì„ë² ë”© ëª¨ë¸ëª…")
            metric2 = gr.Radio(["cos", "l2"], value="cos", label="ìœ ì‚¬ë„/ê±°ë¦¬")
            docs = gr.Textbox(value="ì²« ë²ˆì§¸ ë¬¸ì„œ\në‘ ë²ˆì§¸ ë¬¸ì„œ\nì„¸ ë²ˆì§¸ ë¬¸ì„œ", lines=6, label="ë¬¸ì„œ ì§‘í•©(í•œ ì¤„=í•œ ë¬¸ì„œ)")
            build_btn = gr.Button("ì¸ë±ìŠ¤ ìƒì„±")
            status = gr.Markdown("ìƒíƒœ: ëŒ€ê¸°ì¤‘")

            query = gr.Textbox(value="ë‘ ë²ˆì§¸ì™€ ê´€ë ¨ëœ ë¬¸ì¥", label="ì¿¼ë¦¬")
            topk2 = gr.Slider(1, 10, value=3, step=1, label="Top-K")
            search_btn = gr.Button("ê²€ìƒ‰")
            out_search = gr.JSON(label="ê²€ìƒ‰ ê²°ê³¼")

            # ì„¸ì…˜ ìƒíƒœ
            state_index = gr.State(None)
            state_docs = gr.State([])

            def do_build(backend_v, model_v, metric_v, docs_text):
                dlist = [s for s in docs_text.splitlines() if s.strip()]
                if not dlist:
                    return None, [], "âŒ ë¬¸ì„œê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."
                if backend_v == "ollama":
                    vecs = embed_ollama(dlist, model_v)
                else:
                    vecs = embed_bge_m3(dlist)
                index = build_faiss_index(vecs, metric_v)
                return index, dlist, f"âœ… ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ â€¢ ë¬¸ì„œìˆ˜: {len(dlist)} â€¢ ì°¨ì›: {vecs.shape[1]}"

            def do_search(index, docs_list, q, backend_v, model_v, topk_v):
                if index is None:
                    return {"error": "ë¨¼ì € ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•˜ì„¸ìš”."}
                if backend_v == "ollama":
                    qv = embed_ollama([q], model_v)
                else:
                    qv = embed_bge_m3([q])
                scores, ids = search_faiss(index, qv, int(topk_v))
                hits = []
                for rank in range(int(topk_v)):
                    idx = int(ids[0, rank])
                    score = float(scores[0, rank])
                    if 0 <= idx < len(docs_list):
                        hits.append({"rank": rank+1, "doc": docs_list[idx], "score": score, "index": idx})
                return {"query": q, "hits": hits}

            build_btn.click(do_build, [backend2, ollama_model2, metric2, docs], [state_index, state_docs, status])
            search_btn.click(do_search, [state_index, state_docs, query, backend2, ollama_model2, topk2], out_search)

    gr.Markdown("""
    ---
    ### ì‚¬ìš© íŒ
    - Ollama: ë¨¼ì € ëª¨ë¸ì„ ë°›ì•„ë‘ì„¸ìš”. ì˜ˆ) `ollama pull nomic-embed-text` ë˜ëŠ” `mxbai-embed-large`, `jina-embeddings`
    - BGE-M3: ì²« ë¡œë”© ì‹œ HFì—ì„œ ê°€ì¤‘ì¹˜ê°€ ë‹¤ìš´ë¡œë“œë©ë‹ˆë‹¤.
    - ì½”ì‚¬ì¸ ìœ ì‚¬ë„ëŠ” ì„ë² ë”©ì„ L2 ì •ê·œí™”í•˜ì—¬ Inner Productë¡œ ê³„ì‚°í•©ë‹ˆë‹¤.
    - ì–¸ì–´ê°€ ì„ì—¬ ìˆì–´ë„ ë©€í‹°ë§êµ¬ì–¼ ì„ë² ë”©(ì˜ˆ: BGE-M3, mxbai-embed-large) ì‚¬ìš© ì‹œ ì˜ë¯¸ ë§¤í•‘ì´ ìˆ˜ì›”í•©ë‹ˆë‹¤.
    """)

if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7862, share=False)
