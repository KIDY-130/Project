#78 (ì œë¯¸ë‚˜ì´)
"""
Gemini(google-generativeai) ì˜ˆì œë¥¼
ì˜¤í”ˆì†ŒìŠ¤ LLM(DeepSeek-R1, Ollama ë¡œì»¬ ì„œë²„) + Gradio GUIë¡œ ë³€í™˜í•œ íŒŒì´ì¬ ìŠ¤í¬ë¦½íŠ¸.

â–  ì „ì œ
- ë¡œì»¬ PCì— Ollamaê°€ ì„¤ì¹˜ë˜ì–´ ìˆê³  deepseek-r1(ë˜ëŠ” deepseek-r1:32b ë“±) ëª¨ë¸ì´ pull ë˜ì–´ ìˆìŒ
  ì˜ˆ) í„°ë¯¸ë„ì—ì„œ:  ollama pull deepseek-r1
- ë„¤íŠ¸ì›Œí¬ í‚¤(API key) ì‚¬ìš©í•˜ì§€ ì•ŠìŒ. (ë¡œì»¬ http://localhost:11434 ë¡œ í†µì‹ )

â–  í¬í•¨ ê¸°ëŠ¥ (ì›ë³¸ Gemini ì½”ë“œì™€ ê¸°ëŠ¥ ë§¤í•‘)
1) ëª¨ë¸ ëª©ë¡ í‘œì‹œ               -> Ollama ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ ì¶œë ¥
2) ë‹¨ë°œì„± ì¶”ë¡ (ë¹„ìŠ¤íŠ¸ë¦¬ë°/ìŠ¤íŠ¸ë¦¬ë°) -> generate(stream=False/True)
3) ì±„íŒ… ì„¸ì…˜ ìœ ì§€                -> chat(messages ëˆ„ì , stream ê°€ëŠ¥)
4) ëŒ€í™” ì´ë ¥ í™•ì¸                -> ë‚´ë¶€ history ì¶œë ¥
5) ìƒì„± íŒŒë¼ë¯¸í„°                 -> temperature, num_predict(max tokens), stop(sequences)
6) ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸               -> system role ë©”ì‹œì§€
7) JSON ëª¨ë“œ ì¶œë ¥                -> Ollama ì˜µì…˜ format="json" í™œìš© + json.loads íŒŒì‹±
8) í† í° ìˆ˜ ì„¸ê¸°                  -> Ollama /api/tokenize ì—”ë“œí¬ì¸íŠ¸ í˜¸ì¶œë¡œ í† í° ê¸¸ì´ ì¸¡ì •
9) (ì˜µì…˜) ê°„ë‹¨ ì•ˆì „ì„¤ì • í‰ë‚´      -> ë§¤ìš° ë‹¨ìˆœí•œ ê¸ˆì¹™ì–´ í•„í„° ì˜ˆì‹œ(ë¡œì»¬ ê°€ë“œë ˆì¼)
10) Gradio GUI                  -> ì‹±ê¸€ í”„ë¡¬í”„íŠ¸ íƒ­ + ì±„íŒ… íƒ­ ì œê³µ

ì‘ì„±: OpenCode
"""

# ========================
# í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜(ìµœì´ˆ 1íšŒ)
# ========================
try:
    import ollama  # Ollama Python SDK
except Exception:
    import sys, subprocess
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "ollama"])
    import ollama

try:
    import gradio as gr
except Exception:
    import sys, subprocess
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "gradio>=4.44.0", "requests"])
    import gradio as gr

import json
import time
import requests
from typing import List, Dict, Any

# =====================================
# ì„¤ì •: ë¡œì»¬ Ollama ì„œë²„ ì •ë³´
# =====================================
OLLAMA_HOST = "http://localhost:11434"  # ê¸°ë³¸ê°’

# =====================================
# ìœ í‹¸: ëª¨ë¸ ëª©ë¡ ì¡°íšŒ
# =====================================
def list_ollama_models() -> List[str]:
    """ë¡œì»¬ Ollamaì— ì„¤ì¹˜ëœ ëª¨ë¸ ì´ë¦„ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•œë‹¤."""
    try:
        client = ollama.Client(host=OLLAMA_HOST)
        res = client.list()
        models = [m["model"] for m in res.get("models", [])]
        return models
    except Exception as e:
        print("[ê²½ê³ ] ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ ì¡°íšŒ ì‹¤íŒ¨:", e)
        return []

# =====================================
# ìœ í‹¸: í† í° ìˆ˜ ì„¸ê¸° (Ollama /api/tokenize)
# =====================================
def count_tokens(model: str, text: str) -> int:
    """ì£¼ì–´ì§„ ëª¨ë¸ì˜ í† í¬ë‚˜ì´ì €ë¡œ ë¬¸ìì—´ì„ í† í¬ë‚˜ì´ì¦ˆí•˜ì—¬ í† í° ìˆ˜ë¥¼ ë°˜í™˜í•œë‹¤."""
    try:
        url = f"{OLLAMA_HOST}/api/tokenize"
        resp = requests.post(url, json={"model": model, "prompt": text}, timeout=30)
        resp.raise_for_status()
        data = resp.json()
        return len(data.get("tokens", []))
    except Exception as e:
        print("[ê²½ê³ ] í† í° ì¹´ìš´íŠ¸ ì‹¤íŒ¨:", e)
        return -1

# =====================================
# ìœ í‹¸: ê°„ë‹¨ ê°€ë“œë ˆì¼(ì˜µì…˜)
# =====================================
BLOCKLIST = {"ë©ì²­", "ë°”ë³´", "ì–´ë¦¬ì„"}  # ë§¤ìš° ë‹¨ìˆœí•œ ì˜ˆì‹œ(ì‹¤ì„œë¹„ìŠ¤ì—ëŠ” ì í•©í•˜ì§€ ì•ŠìŒ)

def guardrail_check(text: str) -> bool:
    """ê¸ˆì¹™ì–´ê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ True(ì°¨ë‹¨) ë°˜í™˜."""
    t = text.replace(" ", "")
    return any(b in t for b in BLOCKLIST)

# =====================================
# 1) ë‹¨ë°œì„± ì¶”ë¡  (generate)
# =====================================

def generate_once(model: str, prompt: str, temperature: float = 1.0, max_tokens: int = 512, stops: str = "", system_prompt: str = "") -> str:
    """ìŠ¤íŠ¸ë¦¬ë° ì—†ì´ í•œ ë²ˆì— í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•œë‹¤."""
    client = ollama.Client(host=OLLAMA_HOST)
    options = {
        "temperature": float(temperature),
        "num_predict": int(max_tokens),
    }
    if stops.strip():
        options["stop"] = [s for s in stops.split("||") if s]

    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    messages.append({"role": "user", "content": prompt})

    # ê°„ë‹¨ ê°€ë“œë ˆì¼(ì˜µì…˜): ì…ë ¥ ì°¨ë‹¨
    if guardrail_check(prompt):
        return "[ê°€ë“œë ˆì¼] ì…ë ¥ì— ê¸ˆì¹™ì–´ê°€ í¬í•¨ë˜ì–´ ìƒì„±ì´ ì°¨ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤."

    try:
        # DeepSeek-R1ëŠ” "reasoning" ìŠ¤íƒ€ì¼ ì¶œë ¥ì„ í•  ìˆ˜ ìˆìŒ. ì¼ë°˜ chat ì‚¬ìš©.
        res = client.chat(model=model, messages=messages, options=options)
        return res.get("message", {}).get("content", "")
    except Exception as e:
        return f"[ì—ëŸ¬] ìƒì„± ì‹¤íŒ¨: {e}"

# =====================================
# 2) ìŠ¤íŠ¸ë¦¬ë° ë‹¨ë°œì„± ì¶”ë¡ 
# =====================================

def generate_stream(model: str, prompt: str, temperature: float = 1.0, max_tokens: int = 512, stops: str = "", system_prompt: str = ""):
    """ì œë„ˆë ˆì´í„°ë¥¼ ë°˜í™˜í•˜ì—¬ í† í° ë‹¨ìœ„ë¡œ ìŠ¤íŠ¸ë¦¬ë°í•œë‹¤."""
    client = ollama.Client(host=OLLAMA_HOST)
    options = {
        "temperature": float(temperature),
        "num_predict": int(max_tokens),
    }
    if stops.strip():
        options["stop"] = [s for s in stops.split("||") if s]

    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    messages.append({"role": "user", "content": prompt})

    # ê°„ë‹¨ ê°€ë“œë ˆì¼(ì˜µì…˜): ì…ë ¥ ì°¨ë‹¨
    if guardrail_check(prompt):
        yield "[ê°€ë“œë ˆì¼] ì…ë ¥ì— ê¸ˆì¹™ì–´ê°€ í¬í•¨ë˜ì–´ ìƒì„±ì´ ì°¨ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤."
        return

    try:
        stream = client.chat(model=model, messages=messages, options=options, stream=True)
        for chunk in stream:
            yield chunk.get("message", {}).get("content", "")
    except Exception as e:
        yield f"[ì—ëŸ¬] ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨: {e}"

# =====================================
# 3) ì±„íŒ… ì„¸ì…˜ ê´€ë¦¬
# =====================================
class ChatSession:
    def __init__(self, model: str, system_prompt: str = ""):
        self.client = ollama.Client(host=OLLAMA_HOST)
        self.model = model
        self.history: List[Dict[str, str]] = []
        if system_prompt:
            self.history.append({"role": "system", "content": system_prompt})

    def send(self, user_text: str, temperature: float = 1.0, max_tokens: int = 512, stops: str = "") -> str:
        if guardrail_check(user_text):
            return "[ê°€ë“œë ˆì¼] ì…ë ¥ì— ê¸ˆì¹™ì–´ê°€ í¬í•¨ë˜ì–´ ìƒì„±ì´ ì°¨ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤."
        options = {
            "temperature": float(temperature),
            "num_predict": int(max_tokens),
        }
        if stops.strip():
            options["stop"] = [s for s in stops.split("||") if s]

        self.history.append({"role": "user", "content": user_text})
        try:
            res = self.client.chat(model=self.model, messages=self.history, options=options)
            reply = res.get("message", {}).get("content", "")
            self.history.append({"role": "assistant", "content": reply})
            return reply
        except Exception as e:
            return f"[ì—ëŸ¬] ì±„íŒ… ì‹¤íŒ¨: {e}"

    def stream(self, user_text: str, temperature: float = 1.0, max_tokens: int = 512, stops: str = ""):
        if guardrail_check(user_text):
            yield "[ê°€ë“œë ˆì¼] ì…ë ¥ì— ê¸ˆì¹™ì–´ê°€ í¬í•¨ë˜ì–´ ìƒì„±ì´ ì°¨ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤."
            return
        options = {
            "temperature": float(temperature),
            "num_predict": int(max_tokens),
        }
        if stops.strip():
            options["stop"] = [s for s in stops.split("||") if s]

        # ê¸°ë¡ì— ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€ í›„ ìŠ¤íŠ¸ë¦¼ ì‹œì‘
        self.history.append({"role": "user", "content": user_text})
        try:
            stream = self.client.chat(model=self.model, messages=self.history, options=options, stream=True)
            assistant_accum = ""
            for chunk in stream:
                piece = chunk.get("message", {}).get("content", "")
                assistant_accum += piece
                yield piece
            # ìŠ¤íŠ¸ë¦¬ë° ì¢…ë£Œ í›„ ëˆ„ì  ì‘ë‹µì„ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
            self.history.append({"role": "assistant", "content": assistant_accum})
        except Exception as e:
            yield f"[ì—ëŸ¬] ì±„íŒ… ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨: {e}"

# =====================================
# 4) JSON ëª¨ë“œ ì˜ˆì‹œ (format="json")
# =====================================

def generate_json(model: str, instruction: str, json_schema_hint: str = "") -> Any:
    """
    ëª¨ë¸ì— JSONë§Œ ì¶œë ¥í•˜ë„ë¡ ì§€ì‹œí•˜ê³  íŒŒì‹±í•˜ì—¬ ë°˜í™˜í•œë‹¤.
    - Ollama ì˜µì…˜ format="json" ì‚¬ìš© (ì§€ì› ëª¨ë¸ì—ì„œ ìœ íš¨)
    - schemaë¥¼ ê°•ì œí•  ìˆ˜ëŠ” ì—†ì§€ë§Œ, í”„ë¡¬í”„íŠ¸ë¡œ í˜•ì‹ì„ ì§€ì‹œ
    """
    client = ollama.Client(host=OLLAMA_HOST)
    prompt = (
        "ì•„ë˜ ì§€ì‹œì— ë”°ë¼ JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”. ì„¤ëª… í…ìŠ¤íŠ¸ë¥¼ ì“°ì§€ ë§ˆì„¸ìš”.\n" +
        (f"ì¶œë ¥ ìŠ¤í‚¤ë§ˆ íŒíŠ¸:\n{json_schema_hint}\n" if json_schema_hint else "") +
        f"ì§€ì‹œ:\n{instruction}\n"
    )
    try:
        res = client.generate(model=model, prompt=prompt, options={"format": "json", "num_predict": 512})
        text = res.get("response", "").strip()
        return json.loads(text)
    except json.JSONDecodeError:
        # JSON ëª¨ë“œì¸ë°ë„ ì‹¤íŒ¨í•  ê²½ìš°(ì¼ë¶€ ëª¨ë¸ í˜¸í™˜ì„± ë¬¸ì œ), ìˆ˜ë™ ë³µêµ¬ ì‹œë„
        text = res.get("response", "") if isinstance(res, dict) else str(res)
        text = text.strip().strip("` ")
        try:
            return json.loads(text)
        except Exception:
            return {"_raw": text, "_error": "JSON íŒŒì‹± ì‹¤íŒ¨"}
    except Exception as e:
        return {"_error": f"JSON ìƒì„± ì‹¤íŒ¨: {e}"}

# =====================================
# 5) ë°ëª¨(ì›ë³¸ ì˜ˆì œì— í•´ë‹¹í•˜ëŠ” í˜¸ì¶œë“¤)
# =====================================

def demo_prints(default_model: str):
    print("# ëª¨ë¸ ëª©ë¡ í‘œì‹œ")
    print(list_ollama_models())

    print("\n# ì¶”ë¡  ì‹¤í–‰ (ë¹„ìŠ¤íŠ¸ë¦¬ë°)")
    text = generate_once(default_model, "Google DeepMindì— ê´€í•´ ì•Œë ¤ì£¼ì„¸ìš”.")
    print(text)

    print("\n# ì¶”ë¡  ì‹¤í–‰ (ìŠ¤íŠ¸ë¦¬ë°)")
    for piece in generate_stream(default_model, "Google DeepMindì— ê´€í•´ ì•Œë ¤ì£¼ì„¸ìš”."):
        print(piece, end="", flush=True)
    print("\n--ë--\n")

    print("# ì±„íŒ… ì¤€ë¹„ ë° ëŒ€í™”")
    chat = ChatSession(default_model)
    print(chat.send("ë‚´ê°€ í‚¤ìš°ëŠ” ê³ ì–‘ì´ì˜ ì´ë¦„ì€ ë ˆì˜¤ì…ë‹ˆë‹¤."))
    print(chat.send("ë‚´ê°€ í‚¤ìš°ëŠ” ê³ ì–‘ì´ë¥¼ ë¶ˆëŸ¬ë³´ì„¸ìš”."))

    print("\n# ëŒ€í™” ì´ë ¥ í™•ì¸")
    for m in chat.history:
        role = m.get("role"); content = m.get("content")
        print(role, ":", content[:120] + ("..." if len(content) > 120 else ""))

    print("\n# ìƒì„± íŒŒë¼ë¯¸í„° ì‚¬ìš© ì˜ˆì‹œ")
    story = generate_once(
        default_model,
        "ì‚¬ì´ë²„ í‘í¬ ìŠ¤íƒ€ì¼ì˜ ë¹¨ê°„ ëª¨ì ì´ì•¼ê¸°ë¥¼ ì¨ì£¼ì„¸ìš”",
        temperature=1.0,
        max_tokens=500,
        stops="\n\n",  # Geminiì˜ stop_sequencesì— ëŒ€ì‘: ì—¬ëŸ¬ ê°œëŠ” 'A||B' í˜•ì‹
    )
    print(story)

    print("\n# í† í° ìˆ˜ í™•ì¸")
    print("ì˜ë¬¸ í† í° ìˆ˜:", count_tokens(default_model, "Hello World!"))
    print("í•œê¸€ í† í° ìˆ˜:", count_tokens(default_model, "ì•ˆë…•í•˜ì„¸ìš”, ì„¸ìƒ!"))

    print("\n# (ì˜µì…˜) ê°€ë“œë ˆì¼ ì—†ëŠ” ìƒì„±")
    rude = generate_once(default_model, "ë„ˆëŠ” ë„ˆë¬´ ì–´ë¦¬ì„ì–´!")
    print(rude)

    print("\n# (ì˜µì…˜) ê°„ë‹¨ ê°€ë“œë ˆì¼ ì¼  ìƒíƒœ ì‹œë®¬ë ˆì´ì…˜")
    # ì‹¤ì œë¡œëŠ” guardrail_checkê°€ ì…ë ¥ì—ë§Œ ì ìš©ë˜ë¯€ë¡œ ì˜ˆì‹œ ë¬¸êµ¬ë¥¼ ê¸ˆì¹™ì–´ í¬í•¨ìœ¼ë¡œ ë°”ê¿”ë´„
    rude_block = generate_once(default_model, "ë„ˆëŠ” ë„ˆë¬´ ì–´ë¦¬ì„ì€ ë°”ë³´ì•¼!")
    print(rude_block)

    print("\n# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì ìš©")
    story_sys = generate_once(
        default_model,
        "ì•ˆë…•í•˜ì„¸ìš”. ë‹¹ì‹ ì˜ ì´ë¦„ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        system_prompt="ë‹¹ì‹ ì˜ ì´ë¦„ì€ 'ì½©ì´'ë‹¤. ìì‹ ì„ ë¶€ë¥¼ ë•ŒëŠ” 'ë‚˜'ë¼ê³  í•˜ê³ , ë§ëì—ëŠ” '~ë‹¤'ë‚˜ '~ë€ë‹¤'ë¥¼ ë¶™ì—¬ì„œ ëŒ€í™”í•œë‹¤.",
    )
    print(story_sys)

    print("\n# JSON ëª¨ë“œ ì¶œë ¥")
    schema_hint = """Recipe = {\n  \"recipe_name\": string\n}\nReturn: list[Recipe]\n"""
    j = generate_json(default_model, "ìœ ëª…í•œ ì¿ í‚¤ ë ˆì‹œí”¼ë¥¼ í•œêµ­ì–´ë¡œ 5ê°œë¥¼ ë¦¬ìŠ¤íŠ¸ì—… í•´ì£¼ì„¸ìš”", json_schema_hint=schema_hint)
    print(type(j), j)

# =====================================
# 6) Gradio GUI
# =====================================

def build_gradio(models: List[str], default_model: str = "deepseek-r1"):
    with gr.Blocks(title="OpenCode: Ollama(DeepSeek-R1) Playground", theme=gr.themes.Soft()) as demo:
        gr.Markdown("""
        # ğŸ OpenCode â€¢ Ollama(DeepSeek-R1) + Gradio
        ë¡œì»¬ Ollama ëª¨ë¸ë¡œ Gemini ì˜ˆì œë¥¼ ëŒ€ì²´í•œ ë°ëª¨ì…ë‹ˆë‹¤. ì¢Œì¸¡ì—ì„œ ëª¨ë¸ì„ ì„ íƒí•˜ê³ , íƒ­ì—ì„œ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ì„¸ìš”.
        """)
        with gr.Row():
            model_dd = gr.Dropdown(models or [default_model], value=(default_model if default_model in models else (models[0] if models else default_model)), label="ëª¨ë¸")
            temp = gr.Slider(0.0, 2.0, value=1.0, step=0.1, label="temperature")
            max_tok = gr.Slider(16, 4096, value=512, step=16, label="max_tokens (num_predict)")
            stops = gr.Textbox(value="", placeholder="ì—¬ëŸ¬ ê°œëŠ” 'A||B' ë¡œ êµ¬ë¶„", label="stop sequences")
        sys_prompt = gr.Textbox(value="", label="ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸", lines=2)

        with gr.Tabs():
            with gr.Tab("Single Prompt"):
                user_prompt = gr.Textbox(value="Google DeepMindì— ê´€í•´ ì•Œë ¤ì£¼ì„¸ìš”.", lines=4, label="í”„ë¡¬í”„íŠ¸")
                stream_ck = gr.Checkbox(value=False, label="ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥")
                out = gr.Textbox(label="ì‘ë‹µ")
                btn = gr.Button("ì‹¤í–‰", variant="primary")

                def run_single(m, p, t, n, s, sp, st):
                    if st:
                        chunks = []
                        for piece in generate_stream(m, p, t, n, s, sp):
                            chunks.append(piece)
                            yield "".join(chunks)
                    else:
                        text = generate_once(m, p, t, n, s, sp)
                        yield text

                btn.click(run_single, [model_dd, user_prompt, temp, max_tok, stops, sys_prompt, stream_ck], out)

            with gr.Tab("Chat"):
                gr.Markdown("ì±„íŒ…ì€ ì„¸ì…˜ë³„ë¡œ ë©”ì‹œì§€ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤. ìƒˆ ì„¸ì…˜ì„ ì‹œì‘í•˜ë ¤ë©´ 'ìƒˆ ì±„íŒ…'ì„ ëˆ„ë¥´ì„¸ìš”.")
                chatbox = gr.Chatbot(height=400)
                chat_input = gr.Textbox(placeholder="ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”â€¦", lines=2)
                stream_ck2 = gr.Checkbox(value=True, label="ìŠ¤íŠ¸ë¦¬ë°")
                new_chat = gr.Button("ìƒˆ ì±„íŒ…", variant="secondary")

                # ì„¸ì…˜ ìƒíƒœ: (ChatSession ì¸ìŠ¤í„´ìŠ¤, model, sys_prompt)
                state = gr.State(value=None)

                def ensure_session(sess, m, sp):
                    if sess is None or getattr(sess, "model", None) != m:
                        return ChatSession(m, sp)
                    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ê°€ ë°”ë€Œë©´ ìƒˆë¡œ ìƒì„±
                    has_sys = any(msg.get("role") == "system" for msg in sess.history)
                    if (sp and not has_sys) or (has_sys and sess.history[0].get("content") != sp):
                        return ChatSession(m, sp)
                    return sess

                def on_send(sess, history, m, sp, text, t, n, s, streaming):
                    sess = ensure_session(sess, m, sp)
                    if streaming:
                        # ìŠ¤íŠ¸ë¦¬ë°: yield ë‹¨ê³„ì ìœ¼ë¡œ chatbox ì—…ë°ì´íŠ¸
                        partial = ""
                        for piece in sess.stream(text, t, n, s):
                            partial += piece
                            yield sess, history + [(text, partial)]
                        return  # ì œë„ˆë ˆì´í„° ì¢…ë£Œ
                    else:
                        reply = sess.send(text, t, n, s)
                        history = (history or []) + [(text, reply)]
                        return sess, history

                def on_new_chat():
                    return None, []

                chat_input.submit(on_send, [state, chatbox, model_dd, sys_prompt, chat_input, temp, max_tok, stops, stream_ck2], [state, chatbox])
                new_chat.click(on_new_chat, outputs=[state, chatbox])

            with gr.Tab("JSON Mode"):
                gr.Markdown("Ollama ì˜µì…˜ format=\"json\"ë¥¼ ì´ìš©í•´ JSONë§Œ ì¶œë ¥í•˜ë„ë¡ ì§€ì‹œí•©ë‹ˆë‹¤.")
                instr = gr.Textbox(value="ìœ ëª…í•œ ì¿ í‚¤ ë ˆì‹œí”¼ë¥¼ í•œêµ­ì–´ë¡œ 5ê°œë¥¼ ë¦¬ìŠ¤íŠ¸ì—… í•´ì£¼ì„¸ìš”", lines=3, label="ì§€ì‹œë¬¸")
                schema = gr.Textbox(value="Recipe = {\n  \"recipe_name\": string\n}\nReturn: list[Recipe]", lines=4, label="ìŠ¤í‚¤ë§ˆ íŒíŠ¸(ì„ íƒ)")
                json_out = gr.Code(language="json", label="JSON ì¶œë ¥")
                btn_json = gr.Button("ìƒì„±")

                def run_json(m, i, sh):
                    return json.dumps(generate_json(m, i, sh), ensure_ascii=False, indent=2)

                btn_json.click(run_json, [model_dd, instr, schema], json_out)

            with gr.Tab("Token Count"):
                gr.Markdown("ëª¨ë¸ í† í¬ë‚˜ì´ì €ë¡œ ê¸¸ì´ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤.")
                txt = gr.Textbox(value="Hello World!", label="í…ìŠ¤íŠ¸")
                tok_out = gr.Number(label="í† í° ìˆ˜")
                btn_tok = gr.Button("ì¹´ìš´íŠ¸")
                btn_tok.click(lambda m, x: count_tokens(m, x), [model_dd, txt], tok_out)

        gr.Markdown("""
        ---
        **íŒ**  
        - ì—¬ëŸ¬ stop ì‹œí€€ìŠ¤ëŠ” `A||B` ì²˜ëŸ¼ `||` ë¡œ êµ¬ë¶„í•˜ì„¸ìš”.  
        - DeepSeek-R1ì€ ì‚¬ê³ ì˜ í”ì (Reasoning)ì„ ì¶œë ¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•„ìš” ì‹œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¡œ ìŠ¤íƒ€ì¼ì„ í†µì œí•˜ì„¸ìš”.  
        - JSON ëª¨ë“œëŠ” ëª¨ë¸/ë²„ì „ì— ë”°ë¼ ì •í™•ë„ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        """)
    return demo

# =====================================
# ë©”ì¸ ì‹¤í–‰ë¶€
# =====================================
if __name__ == "__main__":
    models = list_ollama_models()
    default = "deepseek-r1" if ("deepseek-r1" in models) else (models[0] if models else "deepseek-r1")

    # ì½˜ì†” ë°ëª¨ í˜¸ì¶œì„ ì›í•˜ë©´ ì£¼ì„ í•´ì œ
    # demo_prints(default)

    app = build_gradio(models, default)
    # ë¸Œë¼ìš°ì € ìë™ ì˜¤í”ˆì„ ì›í•˜ë©´ share=True ì‚¬ìš© ê°€ëŠ¥(ë¡œì»¬ ë„¤íŠ¸ì›Œí¬ ë…¸ì¶œ ì£¼ì˜)
    app.launch(server_name="0.0.0.0", server_port=7860, share=False)
