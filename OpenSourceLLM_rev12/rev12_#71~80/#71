#71
# pip install langchain langchain-community langchain-text-splitters chromadb 
# gradio duckduckgo-search "unstructured[md]" html2text

"""
ì˜¤í”ˆì†ŒìŠ¤ LLM(Ollama + DeepSeek-R1) ê¸°ë°˜ RAG + Gradio ë°ëª¨
- ë‹«íŒ LLM(OpenAI) ì˜ì¡´ì„± ì œê±°
- LangChain + Chroma + Ollama ì„ë² ë”©/ì±—
- ê¸ˆìœµ ê´€ë ¨ í¬í„¸ í˜ì´ì§€ë¥¼ ê°„ë‹¨ í¬ë¡¤ë§í•˜ì—¬ ë¡œì»¬ RAG ì¸ë±ìŠ¤ êµ¬ì„±
- Gradio UI ì œê³µ

ì‘ì„±ì ë©”ëª¨:
- í¬ë¡¤ë§ ëŒ€ìƒ ì‚¬ì´íŠ¸ëŠ” robots ë° ì ‘ì† ì œí•œì— ë”°ë¼ ë¡œì»¬ í™˜ê²½ì—ì„œ ê²°ê³¼ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- í•„ìš” ì‹œ URLì„ ì—¬ëŸ¬ë¶„ í™˜ê²½ì— ë§ê²Œ ë°”ê¾¸ì„¸ìš”.
"""

# =========================
# 1) ê¸°ë³¸ ì˜ì¡´ì„± ì„í¬íŠ¸
# =========================
import os
from typing import List, Dict, Any, Tuple

# LangChain - ë¡œë”/ì„ë² ë”©/LLM/ë²¡í„°ìŠ¤í† ì–´
from langchain_community.document_loaders import WebBaseLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.chat_models import ChatOllama
from langchain_community.vectorstores import Chroma
from langchain.schema import Document

# ì¶œë ¥ íŒŒì„œ/í”„ë¡¬í”„íŠ¸
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

# UI
import gradio as gr


# =========================
# 2) ì„¤ì •(ëª¨ë¸/URL ë“±)
# =========================
# ë¡œì»¬ Ollama ëª¨ë¸ ì´ë¦„ (ì‚¬ì „ pull í•„ìš”)
CHAT_MODEL_NAME = "deepseek-r1:latest"        # ì¶”ë¡ /ë‹µë³€ìš©
EMBED_MODEL_NAME = "nomic-embed-text:latest"  # ë¬¸ì„œ ì„ë² ë”©ìš©

# ì¸ë±ì‹±í•  URL ëª©ë¡ (í•„ìš” ì‹œ êµì²´/ì¶”ê°€)
CRAWL_URLS = [
    "https://finance.naver.com/",
    "https://finance.yahoo.com/",
    "https://finance.daum.net/",
]

# Chroma ì»¬ë ‰ì…˜/ì €ì¥ ê²½ë¡œ (ì›í•˜ë©´ ì˜ì† ë””ë ‰í„°ë¦¬ ì‚¬ìš©)
CHROMA_COLLECTION = "rag-chroma"
CHROMA_PERSIST_DIR = None  # ì˜ˆ: "./.chroma_finance" ë¡œ ë‘ë©´ ë””ìŠ¤í¬ì— ì˜ì†í™”


# =========================
# 3) ìœ í‹¸: ë¬¸ì„œ ë¡œë“œ/ë¶„í• /ìƒ‰ì¸
# =========================
def load_docs(urls: List[str]) -> List[Document]:
    """
    ê°„ë‹¨í•œ ì›¹ ë¬¸ì„œ ë¡œë”.
    - WebBaseLoaderëŠ” ì‚¬ì´íŠ¸ì— ë”°ë¼ ì°¨ë‹¨ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ í•„ìš”ì‹œ ë¡œë” êµì²´.
    """
    docs: List[Document] = []
    for u in urls:
        try:
            loader = WebBaseLoader(u)
            loaded = loader.load()
            docs.extend(loaded)
        except Exception as e:
            # í¬ë¡¤ë§ ì‹¤íŒ¨í•´ë„ ì „ì²´ íŒŒì´í”„ë¼ì¸ì€ ê³„ì† ì§„í–‰
            print(f"[WARN] Failed to load {u}: {e}")
    return docs


def split_docs(docs: List[Document],
               chunk_size: int = 800,
               chunk_overlap: int = 160) -> List[Document]:
    """
    OpenAI í† í¬ë‚˜ì´ì €ì— ì˜ì¡´í•˜ì§€ ì•ŠëŠ” ì¼ë°˜ ë¬¸ì ê¸°ë°˜ ë¶„í• ê¸°.
    """
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separators=["\n\n", "\n", " ", ""],
    )
    return splitter.split_documents(docs)


def build_vectorstore(doc_splits: List[Document]) -> Chroma:
    """
    Ollama ì„ë² ë”©ìœ¼ë¡œ Chroma ë²¡í„°ìŠ¤í† ì–´ êµ¬ì„±.
    """
    embeddings = OllamaEmbeddings(model=EMBED_MODEL_NAME)
    vs = Chroma.from_documents(
        documents=doc_splits,
        embedding=embeddings,
        collection_name=CHROMA_COLLECTION,
        persist_directory=CHROMA_PERSIST_DIR
    )
    return vs


# =========================
# 4) LLM ê´€ë ¨(ì§ˆì˜ ì¬ì‘ì„±/íŒì •/ìƒì„±)
# =========================
def get_chat_model(temperature: float = 0.2) -> ChatOllama:
    """
    ë¡œì»¬ Ollama ê¸°ë°˜ DeepSeek-R1 ì±— ëª¨ë¸ êµ¬ì„±.
    """
    return ChatOllama(model=CHAT_MODEL_NAME, temperature=temperature)


def grade_relevance(question: str, context: str) -> bool:
    """
    ë¬¸ë§¥(context)ì´ ì§ˆë¬¸(question)ê³¼ ê´€ë ¨ ìˆëŠ”ì§€ ì´ì§„ íŒì •.
    DeepSeek-R1ì— ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ë¡œ yes/no ìš”ì²­.
    """
    prompt = PromptTemplate.from_template(
        """ë‹¤ìŒ ë¬¸ë§¥ì´ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ì˜ë¯¸ì ìœ¼ë¡œ ê´€ë ¨ì´ ìˆìœ¼ë©´ 'yes', ì•„ë‹ˆë©´ 'no'ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
[ì§ˆë¬¸]
{question}

[ë¬¸ë§¥]
{context}

ì •ë‹µ:"""
    )
    chain = prompt | get_chat_model(temperature=0.0) | StrOutputParser()
    try:
        result = chain.invoke({"question": question, "context": context}).strip().lower()
        return "yes" in result[:5]  # 'yes'ë¡œ ì‹œì‘/í¬í•¨í•˜ë©´ True
    except Exception as e:
        print(f"[WARN] grade_relevance error: {e}")
        return True  # íŒì • ì‹¤íŒ¨ ì‹œ ë³´ìˆ˜ì ìœ¼ë¡œ ê´€ë ¨ ìˆë‹¤ê³  ì²˜ë¦¬


def rewrite_query(original: str) -> str:
    """
    ì§ˆì˜ê°€ ëª¨í˜¸í•˜ê±°ë‚˜ ê´€ë ¨ ë¬¸ì„œë¥¼ ëª» ì°¾ì€ ê²½ìš°, ì§ˆì˜ ì¬ì‘ì„±.
    """
    prompt = PromptTemplate.from_template(
        """ë‹¤ìŒ ì§ˆë¬¸ì„ ë” ëª…í™•í•˜ê³  ê²€ìƒ‰ ì¹œí™”ì ìœ¼ë¡œ 1ë¬¸ì¥ìœ¼ë¡œ ì¬ì‘ì„±í•˜ì„¸ìš”.
ì›ë¬¸: {q}
ì¬ì‘ì„±:"""
    )
    chain = prompt | get_chat_model(temperature=0.2) | StrOutputParser()
    try:
        return chain.invoke({"q": original}).strip()
    except Exception as e:
        print(f"[WARN] rewrite_query error: {e}")
        return original


def generate_answer(question: str, context: str, max_sentences: int = 3) -> str:
    """
    RAG ìµœì¢… ì‘ë‹µ ìƒì„±.
    - ëª¨ë¥´ë©´ 'ëª¨ë¥´ê² ìŠµë‹ˆë‹¤'ë¡œ ì‘ë‹µ
    - í•œêµ­ì–´ ê°„ê²° ë‹µë³€(ìµœëŒ€ Në¬¸ì¥)
    """
    prompt = PromptTemplate.from_template(
        """ë‹¹ì‹ ì€ ê¸ˆìœµ ê´€ë ¨ ì§ˆë¬¸ì— ê°„ê²°íˆ ë‹µí•˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì•„ë˜ 'ë¬¸ë§¥'ì„ í™œìš©í•´ 'ì§ˆë¬¸'ì— ë‹µí•˜ì„¸ìš”.
- ë¬¸ë§¥ì— ê·¼ê±°ê°€ ë¶€ì¡±í•˜ë©´ "ëª¨ë¥´ê² ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µí•˜ì„¸ìš”.
- í•œêµ­ì–´ë¡œ {n}ë¬¸ì¥ ì´ë‚´ë¡œ ê°„ê²°íˆ ì‘ì„±í•˜ì„¸ìš”.

[ì§ˆë¬¸]
{question}

[ë¬¸ë§¥]
{context}

[ë‹µë³€]"""
    )
    chain = prompt | get_chat_model(temperature=0.2) | StrOutputParser()
    try:
        return chain.invoke({"question": question, "context": context, "n": max_sentences}).strip()
    except Exception as e:
        print(f"[ERROR] generate_answer error: {e}")
        return "ëª¨ë¥´ê² ìŠµë‹ˆë‹¤."


# =========================
# 5) RAG íŒŒì´í”„ë¼ì¸ (í•œ ë²ˆ ì¬ì‘ì„±ê¹Œì§€)
# =========================
def run_rag_pipeline(
    question: str,
    vectorstore: Chroma,
    k: int = 6
) -> Tuple[str, List[Dict[str, Any]]]:
    """
    - retrieverë¡œ top-k ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
    - ê´€ë ¨ì„± íŒì • ì‹¤íŒ¨ ì‹œ ì§ˆì˜ ì¬ì‘ì„± â†’ ë‹¤ì‹œ ê²€ìƒ‰
    - ìµœì¢… ì»¨í…ìŠ¤íŠ¸ë¡œ ë‹µì•ˆ ìƒì„±
    - ë°˜í™˜: (answer, sources)
      sources: [{"source": url/path, "snippet": text}, ...]
    """
    retriever = vectorstore.as_retriever(search_kwargs={"k": k})

    def fetch_ctx(q: str) -> Tuple[str, List[Document]]:
        docs = retriever.get_relevant_documents(q)
        # ê°„ë‹¨íˆ ìƒìœ„ ëª‡ ê°œë¥¼ concatí•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        ctx_parts = []
        for d in docs:
            # ë„ˆë¬´ ê¸¸ë©´ ì»¨í…ìŠ¤íŠ¸ í­ì£¼í•˜ë¯€ë¡œ ì•ë¶€ë¶„ë§Œ ì‚¬ìš©
            text = d.page_content.strip().replace("\n", " ")
            ctx_parts.append(text[:1200])
        return "\n\n---\n\n".join(ctx_parts), docs

    # 1ì°¨ ê²€ìƒ‰
    context, docs = fetch_ctx(question)

    # ê´€ë ¨ì„± ì ê²€
    if not context or not grade_relevance(question, context):
        # ì¬ì‘ì„± í›„ ì¬ì‹œë„
        rewritten = rewrite_query(question)
        context, docs = fetch_ctx(rewritten)

    # ìµœì¢… ë‹µë³€
    answer = generate_answer(question, context)

    # ì†ŒìŠ¤ ì •ë¦¬
    sources = []
    for d in docs:
        meta = d.metadata or {}
        src = meta.get("source") or meta.get("url") or "unknown"
        snippet = d.page_content.strip().split("\n")
        snippet = " ".join(snippet)[:300]
        sources.append({"source": src, "snippet": snippet})

    return answer, sources


# =========================
# 6) ì´ˆê¸° ì¸ë±ìŠ¤ ë¹Œë“œ
# =========================
def build_index() -> Chroma:
    print("[INFO] Loading documents...")
    raw_docs = load_docs(CRAWL_URLS)
    if not raw_docs:
        print("[WARN] No documents loaded. RAG will likely answer 'ëª¨ë¥´ê² ìŠµë‹ˆë‹¤.'")

    print(f"[INFO] Loaded {len(raw_docs)} document(s). Splitting...")
    splits = split_docs(raw_docs)

    print(f"[INFO] Split into {len(splits)} chunks. Building vector store...")
    vs = build_vectorstore(splits)
    print("[INFO] Vector store ready.")
    return vs


# =========================
# 7) Gradio UI
# =========================
def make_ui(vectorstore: Chroma):
    """
    ê°„ë‹¨í•œ Gradio ì•±:
    - ì…ë ¥: ì§ˆë¬¸
    - ì¶œë ¥: ëª¨ë¸ ë‹µë³€ + ì¶œì²˜ í…Œì´ë¸”
    """
    with gr.Blocks(theme=gr.themes.Soft()) as demo:
        gr.Markdown("## ğŸ’¬ ë¡œì»¬ DeepSeek-R1 RAG (ê¸ˆìœµ í¬í„¸ í¬ë¡¤ë§)")
        gr.Markdown(
            "- ì§ˆì˜ë¥¼ ì…ë ¥í•˜ë©´ ë¡œì»¬ ì„ë² ë”© ë° LLM(DeepSeek-R1)ìœ¼ë¡œ RAG ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.\n"
            "- ì´ˆê¸° ì¸ë±ìŠ¤ëŠ” ë„¤ì´ë²„/ì•¼í›„/ë‹¤ìŒ ê¸ˆìœµ ë©”ì¸ í˜ì´ì§€ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤."
        )

        with gr.Row():
            query = gr.Textbox(
                label="ì§ˆë¬¸(ì˜ˆ: ì‚¼ì„±ì „ì ì£¼ê°€ ì „ë§ì€?)",
                placeholder="ë¬´ì—‡ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?",
                lines=2
            )
        with gr.Row():
            btn = gr.Button("ì§ˆì˜ ì‹¤í–‰", variant="primary")

        with gr.Row():
            answer = gr.Textbox(label="ë‹µë³€", lines=6)
        sources = gr.Dataframe(
            headers=["source", "snippet"],
            datatype=["str", "str"],
            row_count=(0, "dynamic"),
            col_count=(2, "fixed"),
            label="ì°¸ê³ í•œ ë¬¸ì„œ(ì¼ë¶€ ë‚´ìš©)"
        )

        def on_submit(q):
            if not q or not q.strip():
                return "ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.", []
            ans, srcs = run_rag_pipeline(q.strip(), vectorstore=vectorstore, k=6)
            rows = [[s["source"], s["snippet"]] for s in srcs]
            return ans, rows

        btn.click(on_submit, inputs=[query], outputs=[answer, sources])
        query.submit(on_submit, inputs=[query], outputs=[answer, sources])

    return demo


# =========================
# 8) main
# =========================
if __name__ == "__main__":
    # ì¸ë±ìŠ¤ ë¹Œë“œ(ì•± ì‹œì‘ ì‹œ 1íšŒ)
    vs = build_index()

    # Gradio ì•± ì‹¤í–‰
    app = make_ui(vs)
    # share=TrueëŠ” ì™¸ë¶€ ê³µìœ ìš©, ë¡œì»¬ë§Œ ì›í•˜ë©´ False
    app.launch(server_name="0.0.0.0", server_port=7860, share=False)



