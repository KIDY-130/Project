#88
"""
OpenCode ë³€í™˜ë³¸: Whisper + DeepSeek-R1(Ollama) + Gradio GUI ì˜¬ì¸ì› íŒŒì´ì¬ ìŠ¤í¬ë¦½íŠ¸
----------------------------------------------------------------------------------
ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë‹¤ìŒì˜ 'Closed LLM' ì˜ì¡´ ì½”ë“œë¥¼ ëª¨ë‘ **ì˜¤í”ˆì†ŒìŠ¤ ë¡œì»¬ í™˜ê²½**ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.

- OpenAI API (gpt-4o, whisper-1)  -> âŒ ì œê±°
- Gradio ê¸°ë°˜ì˜ ê°„ë‹¨/í™•ì¥í˜• GUI ì œê³µ -> âœ… ì¶”ê°€
- ë¡œì»¬ STT: Whisper ê³„ì—´(ê¶Œì¥: faster-whisper) -> âœ… ì‚¬ìš©
- ë¡œì»¬ LLM: **Ollama + deepseek-r1** (ë¡œì»¬ ì„¤ì¹˜ ê°€ì •) -> âœ… ì‚¬ìš©
- API Key í•„ìš” ì—†ìŒ -> âœ…

ì‚¬ì „ ì¤€ë¹„
1) (ê¶Œì¥) GPUê°€ ìˆë‹¤ë©´ `faster-whisper`ê°€ ë§¤ìš° ë¹ ë¦…ë‹ˆë‹¤.
   pip install -U faster-whisper gradio requests soundfile numpy

   â€» pure Python whisperë¥¼ ì“°ê³  ì‹¶ë‹¤ë©´:
   pip install -U openai-whisper gradio requests soundfile numpy
   ë¡œ ë°”ê¾¸ê³ , ì•„ë˜ ASR_BACKEND="faster-whisper"ë¥¼ "whisper"ë¡œ ë°”ê¾¸ì„¸ìš”.

2) Ollama ì„¤ì¹˜ ë° ëª¨ë¸ ì¤€ë¹„ (ë¡œì»¬ PCì— ì„¤ì¹˜ë˜ì–´ ìˆë‹¤ê³  ê°€ì •)
   - https://ollama.com/download ì—ì„œ ì„¤ì¹˜
   - ëª¨ë¸ ë‹¤ìš´ë¡œë“œ(ì˜ˆì‹œ, 7B/32B ì¤‘ íƒ1):
     ollama pull deepseek-r1:7b
     # ë˜ëŠ”
     ollama pull deepseek-r1:32b

3) ì‹¤í–‰:
   python app_transcribe_ollama_gradio.py
   ë¸Œë¼ìš°ì €ê°€ ì—´ë¦¬ë©´ ì˜¤ë””ì˜¤ ì—…ë¡œë“œ í›„ ì „ì‚¬/ë²ˆì—­/í›„ì²˜ë¦¬ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.
"""

from __future__ import annotations
import os
import re
import time
import tempfile
import requests
from dataclasses import dataclass
from typing import Optional, Dict, Any, Tuple

import gradio as gr
import numpy as np

# ===============================
# ì„¤ì • ì„¹ì…˜
# ===============================

# ASR ë°±ì—”ë“œ ì„ íƒ: "faster-whisper" (ê¶Œì¥) ë˜ëŠ” "whisper"
ASR_BACKEND = "faster-whisper"  # "whisper" ë¡œ ë°”ê¿”ë„ ë™ì‘ (ì†ë„ëŠ” ëŠë¦´ ìˆ˜ ìˆìŒ)

# faster-whisper ëª¨ë¸ í¬ê¸° ì˜ˆì‹œ:
#  - tiny, base, small, medium, large-v3 (CPU/GPU ì—¬ê±´ì— ë§ê²Œ ì„ íƒ)
DEFAULT_ASR_MODEL = "base"

# Ollama ì„¤ì • (ê¸°ë³¸ í¬íŠ¸ 11434)
OLLAMA_HOST = os.environ.get("OLLAMA_HOST", "http://127.0.0.1:11434")
OLLAMA_MODEL = os.environ.get("OLLAMA_MODEL", "deepseek-r1:7b")  # í™˜ê²½ì— ë§ê²Œ ë³€ê²½ ê°€ëŠ¥

# DeepSeek-R1ì˜ <think>...</think> ì¶”ë¡  í…ìŠ¤íŠ¸ë¥¼ ìˆ¨ê¸°ê³  ê²°ê³¼ë§Œ ë³´ì—¬ì¤„ì§€ ì—¬ë¶€
HIDE_REASONING_BY_DEFAULT = True

# ===============================
# ìœ í‹¸
# ===============================

def human_time(s: float) -> str:
    """ì´ˆ -> ì‚¬ëŒì´ ì½ê¸° ì‰¬ìš´ ë¬¸ìì—´"""
    if s < 1.0:
        return f"{s*1000:.0f} ms"
    m, ss = divmod(int(s), 60)
    if m == 0:
        return f"{s:.2f} s"
    h, mm = divmod(m, 60)
    if h == 0:
        return f"{mm}m {ss}s"
    return f"{h}h {mm}m {ss}s"


def strip_deepseek_think(text: str, keep_reasoning: bool = False) -> Tuple[str, Optional[str]]:
    """
    DeepSeek-R1ì€ <think> ... </think> í˜•íƒœì˜ 'ì‚¬ê³ (Reasoning)'ë¥¼ í•¨ê»˜ ë°˜í™˜í•©ë‹ˆë‹¤.
    - keep_reasoning=True  : ë‘˜ ë‹¤ ë°˜í™˜
    - keep_reasoning=False : ì‚¬ê³ ë¥¼ ì œê±°í•˜ê³  'ìµœì¢… ë‹µë³€'ë§Œ ë°˜í™˜
    """
    think_blocks = re.findall(r"<think>(.*?)</think>", text, flags=re.DOTALL)
    clean = re.sub(r"<think>.*?</think>", "", text, flags=re.DOTALL).strip()
    reasoning = "\n\n---\n".join(tb.strip() for tb in think_blocks) if think_blocks else None
    if keep_reasoning:
        return clean, reasoning
    return clean, None


def lang_label_to_code(label: str) -> Optional[str]:
    """UI ë¼ë²¨ì„ Whisper ì–¸ì–´ì½”ë“œë¡œ ë§¤í•‘ (Noneì€ ìë™ ê°ì§€)"""
    mapping = {
        "ìë™ ê°ì§€ (Auto)": None,
        "ì˜ì–´ (en)": "en",
        "í•œêµ­ì–´ (ko)": "ko",
        "ì¤‘êµ­ì–´ (zh)": "zh",
        "ì¼ë³¸ì–´ (ja)": "ja",
        "ìŠ¤í˜ì¸ì–´ (es)": "es",
        "í”„ë‘ìŠ¤ì–´ (fr)": "fr",
        "ë…ì¼ì–´ (de)": "de",
        "ì´íƒˆë¦¬ì•„ì–´ (it)": "it",
        "í¬ë¥´íˆ¬ê°ˆì–´ (pt)": "pt",
        "ëŸ¬ì‹œì•„ì–´ (ru)": "ru",
        "ë² íŠ¸ë‚¨ì–´ (vi)": "vi",
        "íƒœêµ­ì–´ (th)": "th",
    }
    return mapping.get(label, None)


# ===============================
# ASR (Whisper) ë˜í¼
# ===============================

@dataclass
class ASRResult:
    text: str
    lang: Optional[str]
    segments: Optional[list]
    info: Dict[str, Any]


class ASRWhisper:
    """
    ë¡œì»¬ Whisper STT:
    - faster-whisper (ê¶Œì¥): GPU ê°€ì†, ë¹ ë¥´ê³  ë©”ëª¨ë¦¬ íš¨ìœ¨ì 
    - openai-whisper (íŒŒì´ì¬íŒ): ì„¤ì¹˜ ê°„ë‹¨, ì†ë„ëŠ” ëŠë¦´ ìˆ˜ ìˆìŒ
    """

    def __init__(self, model_size: str = DEFAULT_ASR_MODEL, backend: str = ASR_BACKEND):
        self.backend = backend
        self.model_size = model_size
        self._model = None

    def load(self):
        if self._model is not None:
            return
        if self.backend == "faster-whisper":
            from faster_whisper import WhisperModel  # lazy import
            # device="auto" ê°€ GPU ìˆìœ¼ë©´ cuda ì‚¬ìš©, ì—†ìœ¼ë©´ CPU
            self._model = WhisperModel(self.model_size, device="auto", compute_type="auto")
        elif self.backend == "whisper":
            import whisper  # lazy import
            self._model = whisper.load_model(self.model_size)
        else:
            raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” ASR ë°±ì—”ë“œì…ë‹ˆë‹¤.")

    def transcribe(
        self,
        audio_path: str,
        language: Optional[str] = None,
        task: str = "transcribe",  # 'transcribe' ì›ë¬¸ì „ì‚¬, 'translate' ì˜ì–´ë¡œ ë²ˆì—­
        initial_prompt: Optional[str] = None,
    ) -> ASRResult:
        """
        audio_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        language: ì–¸ì–´ ì½”ë“œ (Noneì´ë©´ ìë™ ê°ì§€)
        task: "transcribe" ë˜ëŠ” "translate"
        initial_prompt: ë‹¨ì–´ ì² ì íŒíŠ¸(ì˜ˆ: 'Cypher') ë“±
        """
        self.load()
        t0 = time.time()

        if self.backend == "faster-whisper":
            # Faster-WhisperëŠ” generate ì˜µì…˜ì„ dictë¡œ ë°›ìŒ
            segments, info = self._model.transcribe(
                audio_path,
                language=language,
                task=task,
                initial_prompt=initial_prompt,
                beam_size=5,
                vad_filter=True,
            )
            text_parts = []
            seg_list = []
            for s in segments:
                text_parts.append(s.text)
                seg_list.append({
                    "start": s.start,
                    "end": s.end,
                    "text": s.text,
                    "avg_logprob": s.avg_logprob,
                    "no_speech_prob": s.no_speech_prob,
                    "compression_ratio": s.compression_ratio,
                })
            full_text = "".join(text_parts).strip()
            elapsed = time.time() - t0
            return ASRResult(
                text=full_text,
                lang=info.language,
                segments=seg_list,
                info={
                    "duration_s": info.duration,
                    "time": elapsed,
                    "backend": self.backend,
                    "model_size": self.model_size,
                },
            )

        # openai-whisper (íŒŒì´ì¬)
        import whisper
        # task='translate'ì¸ ê²½ìš°, ì˜ì–´ë¡œ ë²ˆì—­ë¨
        result = self._model.transcribe(
            audio_path,
            language=language,
            task=task,
            initial_prompt=initial_prompt,
            verbose=False
        )
        elapsed = time.time() - t0
        return ASRResult(
            text=result.get("text", "").strip(),
            lang=result.get("language"),
            segments=result.get("segments"),
            info={
                "duration_s": result.get("duration"),
                "time": elapsed,
                "backend": self.backend,
                "model_size": self.model_size,
            },
        )


# ===============================
# Ollama (DeepSeek-R1) ë˜í¼
# ===============================

class OllamaClient:
    """
    Ollama ë¡œì»¬ ì„œë²„ì™€ ëŒ€í™”.
    - ê¸°ë³¸ ì—”ë“œí¬ì¸íŠ¸: POST /api/chat
    - ëª¨ë¸: deepseek-r1:7b (ê¸°ë³¸ê°’, í™˜ê²½ì— ë”°ë¼ ì¡°ì •)
    """

    def __init__(self, host: str = OLLAMA_HOST, model: str = OLLAMA_MODEL, timeout: int = 120):
        self.host = host.rstrip("/")
        self.model = model
        self.timeout = timeout

    def chat(self, system_prompt: str, user_prompt: str, temperature: float = 0.2) -> str:
        url = f"{self.host}/api/chat"
        payload = {
            "model": self.model,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            "options": {
                "temperature": temperature
            },
            "stream": False,  # ê°„ë‹¨íˆ ì¼ê´„ ì‘ë‹µ
        }
        resp = requests.post(url, json=payload, timeout=self.timeout)
        resp.raise_for_status()
        data = resp.json()
        # Ollama /api/chat í¬ë§·: {'message': {'content': '...'}}
        return data.get("message", {}).get("content", "")


# ===============================
# ì „ì‚¬ + í›„ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
# ===============================

ASR = ASRWhisper(model_size=DEFAULT_ASR_MODEL, backend=ASR_BACKEND)
OLLAMA = OllamaClient()

DEFAULT_SYSTEM_PROMPT = """ë‹¹ì‹ ì€ ì „ë¬¸ ì˜¤ë””ì˜¤ ì „ì‚¬ êµì •ê°€ì…ë‹ˆë‹¤.
ê·œì¹™:
1) ì£¼ì–´ì§„ ì „ì‚¬ í…ìŠ¤íŠ¸ì˜ ì˜¤íƒ€/ì˜¤ì¸ì‹ì„ ë¬¸ë§¥ì— ë§ê²Œ êµì •í•˜ì„¸ìš”.
2) ì¸ëª…/ì§€ëª…/ì „ë¬¸ìš©ì–´ëŠ” ì¼ë°˜ì ìœ¼ë¡œ í†µìš©ë˜ëŠ” í‘œê¸°ë²•ìœ¼ë¡œ í†µì¼í•˜ì„¸ìš”.
3) ë¶ˆí•„ìš”í•œ ì¶”ë¡  ì„¤ëª…ì€ ì¶œë ¥í•˜ì§€ ë§ê³ , **êµì •ëœ ë³¸ë¬¸ë§Œ** ë‚´ë³´ë‚´ì„¸ìš”.
4) íƒ€ì„ìŠ¤íƒ¬í”„ë‚˜ ê´„í˜¸ ê°™ì€ ë©”íƒ€ ì •ë³´ê°€ ìˆìœ¼ë©´ ìœ ì§€í•˜ë˜, ë¬¸ì¥ë¶€í˜¸ì™€ ë„ì–´ì“°ê¸°ëŠ” ìì—°ìŠ¤ëŸ½ê²Œ ì •ë¦¬í•˜ì„¸ìš”.
"""

EXAMPLE_HINT = "ì˜ˆ: 'Cypher' ë‹¨ì–´ë¥¼ ìš°ì„ ì‹œí•´ì„œ ì¸ì‹í•´ì¤˜"

def run_pipeline(
    audio_file: Optional[str],
    asr_model_size: str,
    mode: str,  # "transcribe" or "translate"
    ui_lang_label: str,
    hint: str,
    do_postprocess: bool,
    sys_prompt: str,
    temperature: float,
    show_reasoning: bool,
) -> Tuple[str, str, str, Dict[str, Any]]:
    """
    Gradio ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬:
    - audio_file: ì—…ë¡œë“œ íŒŒì¼ ê²½ë¡œ
    - asr_model_size: tiny/base/small/medium/large-v3...
    - mode: 'transcribe' (ì›ë¬¸ ì „ì‚¬), 'translate' (ì˜ì–´ ë²ˆì—­)
    - ui_lang_label: ì–¸ì–´ ì„ íƒ ë¼ë²¨
    - hint: ì´ˆê¸° íŒíŠ¸(ìš©ì–´/ì² ì)
    - do_postprocess: LLM í›„ì²˜ë¦¬ ì—¬ë¶€
    - sys_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸(í›„ì²˜ë¦¬ ê·œì¹™)
    - temperature: LLM ìƒ˜í”Œë§ ì˜¨ë„
    - show_reasoning: DeepSeek ì‚¬ê³ (think) ë…¸ì¶œ ì—¬ë¶€
    """
    if not audio_file:
        return "", "", "ì˜¤ë””ì˜¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.", {}

    # ì„ì‹œ íŒŒì¼ ìƒì„± (ê·¸ëŒ€ë¡œ ê²½ë¡œ ì „ë‹¬í•´ë„ ë˜ì§€ë§Œ, í™•ì‹¤í•˜ê²Œ ì²˜ë¦¬)
    tmp = audio_file

    # ASR ëª¨ë¸ ì ìš©
    ASR.model_size = asr_model_size
    ASR._model = None  # ëª¨ë¸ í¬ê¸° ë°”ë€Œë©´ ë‹¤ì‹œ ë¡œë“œ
    lang_code = lang_label_to_code(ui_lang_label)
    task = "translate" if mode == "translate" else "transcribe"

    asr_res = ASR.transcribe(
        tmp,
        language=lang_code,
        task=task,
        initial_prompt=hint or None
    )

    raw_txt = asr_res.text or ""
    meta = {
        "asr_backend": asr_res.info.get("backend"),
        "asr_model": asr_res.info.get("model_size"),
        "detected_language": asr_res.lang,
        "audio_duration_s": asr_res.info.get("duration_s"),
        "asr_time": human_time(asr_res.info.get("time", 0.0)),
        "mode": mode,
    }

    if not do_postprocess or len(raw_txt.strip()) == 0:
        return raw_txt, "", "LLM í›„ì²˜ë¦¬ ë¯¸ì‹¤í–‰ (ë˜ëŠ” ì „ì‚¬ ê²°ê³¼ ì—†ìŒ)", meta

    # LLM í›„ì²˜ë¦¬ (Ollama + deepseek-r1)
    try:
        user_prompt = raw_txt
        llm_out = OLLAMA.chat(system_prompt=sys_prompt.strip() or DEFAULT_SYSTEM_PROMPT,
                              user_prompt=user_prompt,
                              temperature=float(temperature))
        cleaned, reasoning = strip_deepseek_think(llm_out, keep_reasoning=show_reasoning)
        return raw_txt, cleaned, (reasoning or "ì‚¬ê³ (think) ìˆ¨ê¹€"), meta
    except Exception as e:
        return raw_txt, "", f"Ollama í›„ì²˜ë¦¬ ì˜¤ë¥˜: {e}", meta


# ===============================
# Gradio UI
# ===============================

ASR_MODEL_CHOICES = ["tiny", "base", "small", "medium", "large-v3"]
LANG_CHOICES = [
    "ìë™ ê°ì§€ (Auto)",
    "ì˜ì–´ (en)", "í•œêµ­ì–´ (ko)", "ì¤‘êµ­ì–´ (zh)", "ì¼ë³¸ì–´ (ja)",
    "ìŠ¤í˜ì¸ì–´ (es)", "í”„ë‘ìŠ¤ì–´ (fr)", "ë…ì¼ì–´ (de)",
    "ì´íƒˆë¦¬ì•„ì–´ (it)", "í¬ë¥´íˆ¬ê°ˆì–´ (pt)", "ëŸ¬ì‹œì•„ì–´ (ru)",
    "ë² íŠ¸ë‚¨ì–´ (vi)", "íƒœêµ­ì–´ (th)",
]

with gr.Blocks(title="Whisper + DeepSeek-R1 (Ollama) ë¡œì»¬ ì „ì‚¬/ë²ˆì—­", theme=gr.themes.Soft()) as demo:
    gr.Markdown(
        """
        # ğŸ™ï¸ Whisper + ğŸ§  DeepSeek-R1 (Ollama) â€” ë¡œì»¬ ì „ì‚¬Â·ë²ˆì—­Â·êµì •
        - **ASR**: faster-whisper / whisper (ë¡œì»¬)
        - **LLM í›„ì²˜ë¦¬**: deepseek-r1 (Ollama, ë¡œì»¬)
        - API Key ë¶ˆí•„ìš”, ì¸í„°ë„· ì—°ê²° ë¶ˆí•„ìš”(ëª¨ë¸ë§Œ ë¡œì»¬ ì„¤ì¹˜ë˜ì–´ ìˆìœ¼ë©´ ë¨)
        """
    )

    with gr.Row():
        audio = gr.Audio(label="ì˜¤ë””ì˜¤ íŒŒì¼ ì—…ë¡œë“œ", sources=["upload"], type="filepath")

    with gr.Row():
        asr_model = gr.Dropdown(ASR_MODEL_CHOICES, value=DEFAULT_ASR_MODEL, label="ASR ëª¨ë¸ í¬ê¸°")
        lang = gr.Dropdown(LANG_CHOICES, value="ìë™ ê°ì§€ (Auto)", label="ì–¸ì–´")
        mode = gr.Radio(choices=["transcribe (ì›ë¬¸ ì „ì‚¬)", "translate (ì˜ì–´ ë²ˆì—­)"],
                        value="transcribe (ì›ë¬¸ ì „ì‚¬)", label="ì‘ì—… ëª¨ë“œ")
    hint = gr.Textbox(value=EXAMPLE_HINT, label="ì „ì‚¬ íŒíŠ¸(ì„ íƒ, ìš©ì–´/ì² ì ë“±)", placeholder="ì˜ˆ: Cypher, Kubernetes, Winston Churchill ...")

    with gr.Accordion("LLM(DeepSeek-R1) í›„ì²˜ë¦¬ ì„¤ì •", open=False):
        enable_post = gr.Checkbox(value=True, label="í›„ì²˜ë¦¬ ì‚¬ìš© (ê¶Œì¥)")
        sys_prompt = gr.Textbox(value=DEFAULT_SYSTEM_PROMPT, lines=6, label="ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸")
        temperature = gr.Slider(0.0, 1.2, value=0.2, step=0.1, label="ìƒ˜í”Œë§ ì˜¨ë„")
        show_reasoning = gr.Checkbox(value=(not HIDE_REASONING_BY_DEFAULT), label="DeepSeek ì‚¬ê³ (think) ì¶œë ¥ í‘œì‹œ")

    run_btn = gr.Button("ì‹¤í–‰", variant="primary")

    with gr.Row():
        raw_out = gr.Textbox(lines=10, label="ì „ì‚¬ ê²°ê³¼ (RAW)")
        post_out = gr.Textbox(lines=10, label="í›„ì²˜ë¦¬ ê²°ê³¼ (CLEANED)")
    reason_out = gr.Textbox(lines=8, label="(ì„ íƒ) ëª¨ë¸ ì‚¬ê³ (think) ë˜ëŠ” ë©”ì‹œì§€", value="ì‚¬ê³ (think) ìˆ¨ê¹€")

    meta_json = gr.JSON(label="ë©”íƒ€ ì •ë³´")

    def _run(audio_path, asr_size, mode_label, lang_label, hint_text,
             do_post, sys_p, temp, show_think):
        mode_key = "translate" if "translate" in mode_label else "transcribe"
        return run_pipeline(audio_path, asr_size, mode_key, lang_label, hint_text, do_post, sys_p, temp, show_think)

    run_btn.click(
        _run,
        inputs=[audio, asr_model, mode, lang, hint, enable_post, sys_prompt, temperature, show_reasoning],
        outputs=[raw_out, post_out, reason_out, meta_json]
    )

    gr.Markdown(
        """
        ### ì‚¬ìš© íŒ
        - **ë²ˆì—­ ëª¨ë“œ**: Whisperì˜ `translate`ì€ ê²°ê³¼ë¥¼ ì˜ì–´ë¡œ í†µì¼í•©ë‹ˆë‹¤.
        - **íŒíŠ¸ ì‚¬ìš©**: ê³ ìœ ëª…ì‚¬Â·ë¸Œëœë“œÂ·ì „ë¬¸ìš©ì–´ë¥¼ `ì „ì‚¬ íŒíŠ¸`ì— ì ìœ¼ë©´ ì˜¤ì¸ì‹ì´ ì¤„ì–´ë“­ë‹ˆë‹¤.
        - **ëª¨ë¸ í¬ê¸°**: `tiny/base`ëŠ” ë¹ ë¥´ì§€ë§Œ ì •í™•ë„ ë‚®ìŒ, `medium/large-v3`ëŠ” ëŠë¦¬ì§€ë§Œ ì •í™•ë„ ë†’ìŒ.
        - **DeepSeek ì‚¬ê³ **: ì²´í¬ í•´ì œ ì‹œ `<think>...</think>`ëŠ” ì œê±°ë˜ê³  ê²°ê³¼ë§Œ ë³´ì—¬ì¤ë‹ˆë‹¤.
        """
    )

# ===============================
# CLI ì‹¤í–‰ë„ ì§€ì› (ì˜µì…˜)
# ===============================

def cli_demo_example():
    """
    ê°„ë‹¨ CLI ì˜ˆì‹œ:
    python app_transcribe_ollama_gradio.py --cli path/to/audio.wav --lang "ì˜ì–´ (en)" --mode transcribe
    """
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--cli", type=str, help="ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ(ì§€ì • ì‹œ CLI ëª¨ë“œë¡œ 1íšŒ ì‹¤í–‰)")
    parser.add_argument("--lang", type=str, default="ìë™ ê°ì§€ (Auto)")
    parser.add_argument("--mode", type=str, choices=["transcribe", "translate"], default="transcribe")
    parser.add_argument("--model", type=str, default=DEFAULT_ASR_MODEL)
    parser.add_argument("--hint", type=str, default="")
    parser.add_argument("--no-post", action="store_true", help="LLM í›„ì²˜ë¦¬ ë¹„í™œì„±í™”")
    parser.add_argument("--temp", type=float, default=0.2)
    parser.add_argument("--show-think", action="store_true")
    args = parser.parse_args()

    if not args.cli:
        return False

    raw, cleaned, msg, meta = run_pipeline(
        audio_file=args.cli,
        asr_model_size=args.model,
        mode=args.mode,
        ui_lang_label=args.lang,
        hint=args.hint,
        do_postprocess=(not args.no_post),
        sys_prompt=DEFAULT_SYSTEM_PROMPT,
        temperature=args.temp,
        show_reasoning=args.show_think,
    )
    print("=== RAW ===")
    print(raw)
    print("\n=== CLEANED ===")
    print(cleaned)
    print("\n=== MSG/THINK ===")
    print(msg)
    print("\n=== META ===")
    print(meta)
    return True


if __name__ == "__main__":
    # CLI ë‹¨ë°œ ì‹¤í–‰ ëª¨ë“œ
    if not cli_demo_example():
        # GUI ì‹¤í–‰
        demo.launch()
