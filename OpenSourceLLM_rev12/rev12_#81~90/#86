#86
"""
OpenCode ë³€í™˜ë³¸
- OpenAI ì „ìš© ì½”ë“œ(ENV, OpenAI í´ë¼ì´ì–¸íŠ¸, gpt-4o-mini ë“±)ë¥¼
  ì˜¤í”ˆì†ŒìŠ¤ LLM(DeepSeek-R1 @ Ollama) + Gradio GUIë¡œ ë™ì‘í•˜ë„ë¡ ë³€í™˜í–ˆìŠµë‹ˆë‹¤.
- âœ… ì™„ì „ ë¡œì»¬: API Key ë¶ˆí•„ìš”, .env ë¶ˆí•„ìš”
- âœ… OpenAI chat.completions.create ì™€ ìœ ì‚¬í•œ ì¸í„°í˜ì´ìŠ¤ ì œê³µ
- âœ… stop ì‹œí€€ìŠ¤, max_tokens, temperature, usage(í† í°/ìŠ¤í… ìœ ì‚¬ì¹˜) í‘œì‹œ
- âœ… ì—¬ëŸ¬ ë°ëª¨ íƒ­: ê¸°ë³¸ ëŒ€í™”, ê¸¸ì´ ë¹„êµ, stop ì‚¬ìš©, ì ‘ë‘ì–´, JSON ì˜ˆì‹œ, ëª¨ë¸ ëª©ë¡

ì‚¬ì „ ì¤€ë¹„
1) Ollama ì„¤ì¹˜: https://ollama.com
2) ëª¨ë¸ ë°›ê¸°(ìµœì´ˆ 1íšŒ):  `ollama pull deepseek-r1`
3) íŒŒì´ì¬ íŒ¨í‚¤ì§€:       `pip install gradio ollama`
"""

import re
import time
from typing import List, Dict, Any, Generator, Optional, Tuple

import gradio as gr
import ollama  # ë¡œì»¬ Ollama ì„œë²„(ê¸°ë³¸ http://localhost:11434)


# ==============================
# DeepSeek-R1ì˜ <think> ë‚´ë¶€ ì¶”ë¡  ìˆ¨ê¹€ ìœ í‹¸
# ==============================
_THINK_TAG = re.compile(r"<think>.*?</think>", re.DOTALL | re.IGNORECASE)

def strip_think(text: str) -> str:
    """DeepSeek-R1ì´ ì¶œë ¥í•˜ëŠ” ë‚´ë¶€ ì¶”ë¡ (<think>...</think>)ì„ ìˆ¨ê¹ë‹ˆë‹¤."""
    return _THINK_TAG.sub("", text).strip()


# ==============================
# OpenAI í˜¸í™˜ ëŠë‚Œì˜ í´ë¼ì´ì–¸íŠ¸ ë˜í¼
# ==============================
class LocalOpenSourceClient:
    """
    OpenAIì˜ `client.chat.completions.create(...)`ë¥¼ í‰ë‚´ ë‚´ëŠ” ê°„ë‹¨ ë˜í¼.
    - messages: [{"role": "system"|"user"|"assistant", "content": "..."}]
    - model: ê¸°ë³¸ 'deepseek-r1'
    - options: max_tokens(num_predict), temperature, stop(list|str) ë“±
    - ë°˜í™˜: {"message": {"content": ...}, "usage": {...}} í˜•íƒœì˜ dict
    """

    def __init__(self, model: str = "deepseek-r1"):
        self.model = model

    def chat_complete(
        self,
        messages: List[Dict[str, str]],
        model: Optional[str] = None,
        max_tokens: int = 256,
        temperature: float = 0.7,
        stop: Optional[List[str] | str] = None,
    ) -> Dict[str, Any]:
        """
        ë¹„(é)ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ. OpenAI chat.completions.create ëŒ€ì²´.
        """
        model = model or self.model

        resp = ollama.chat(
            model=model,
            messages=messages,
            options={
                "num_predict": int(max_tokens),
                "temperature": float(temperature),
                **({"stop": stop} if stop else {}),
            },
        )
        # Ollama í‘œì¤€ ì‘ë‹µ ì˜ˆ:
        # {
        #   "model": "deepseek-r1",
        #   "message": {"role":"assistant","content":"..."},
        #   "prompt_eval_count": 123,
        #   "eval_count": 256,
        #   "total_duration": ... (ns)
        # }
        content_raw = resp.get("message", {}).get("content", "")
        content = strip_think(content_raw)
        usage = {
            # OpenAIì˜ usageë¥¼ í‰ë‚´ëƒ„: í”„ë¡¬í”„íŠ¸, ìƒì„± í† í° ê·¼ì‚¬ì¹˜
            "prompt_tokens": resp.get("prompt_eval_count"),
            "completion_tokens": resp.get("eval_count"),
            "total_tokens": (
                (resp.get("prompt_eval_count") or 0) + (resp.get("eval_count") or 0)
            ),
            # ì°¸ê³ ìš© ì‹œê°„ ì •ë³´(ë‚˜ë…¸ì´ˆ)
            "total_duration_ns": resp.get("total_duration"),
        }
        return {
            "message": {"role": "assistant", "content": content},
            "usage": usage,
            "model": resp.get("model", model),
        }

    def chat_stream(
        self,
        messages: List[Dict[str, str]],
        model: Optional[str] = None,
        max_tokens: int = 256,
        temperature: float = 0.7,
        stop: Optional[List[str] | str] = None,
    ) -> Generator[str, None, None]:
        """
        ìŠ¤íŠ¸ë¦¬ë° ìƒì„±ê¸°. í† í°/ì²­í¬ ë‹¨ìœ„ë¡œ ë¬¸ìì—´ì„ yield í•©ë‹ˆë‹¤.
        """
        model = model or self.model

        stream = ollama.chat(
            model=model,
            messages=messages,
            stream=True,
            options={
                "num_predict": int(max_tokens),
                "temperature": float(temperature),
                **({"stop": stop} if stop else {}),
            },
        )

        buffer = ""
        for chunk in stream:
            delta = chunk.get("message", {}).get("content", "")
            if not delta:
                continue
            buffer += delta
            # ë‚´ë¶€ ì¶”ë¡  ì œê±° í›„ ì¦ë¶„ë§Œ ë‚´ë³´ë‚´ê¸°
            clean = strip_think(buffer)
            yield clean

    # OpenAIì˜ client.models.list() ìœ ì‚¬
    def models_list(self) -> List[Dict[str, Any]]:
        li = ollama.list()  # {'models': [{'name': 'deepseek-r1', ...}, ...]}
        return li.get("models", [])

    def model_ids(self) -> List[str]:
        return [m.get("name") for m in self.models_list() if m.get("name")]


# ì „ì—­ í´ë¼ì´ì–¸íŠ¸(ê¸°ë³¸ ëª¨ë¸: deepseek-r1)
client = LocalOpenSourceClient(model="deepseek-r1")


# ==============================
# Gradio ì•±
# ==============================
def build_app():
    with gr.Blocks(title="DeepSeek-R1 (Local Â· Ollama) â€” OpenCode ë³€í™˜") as demo:
        gr.Markdown(
            """
            # ğŸ§  DeepSeek-R1 (ë¡œì»¬ Â· Ollama) â€” OpenCode ë³€í™˜
            - OpenAI ì½”ë“œ ì˜ˆì‹œë¥¼ ë¡œì»¬ ì˜¤í”ˆì†ŒìŠ¤ LLMìœ¼ë¡œ 1:1ì— ê°€ê¹ê²Œ ë°”ê¾¼ ë°ëª¨ì…ë‹ˆë‹¤.
            - **API Key ë¶ˆí•„ìš”**, `.env` ë¶ˆí•„ìš”, ë‚´ë¶€ ì¶”ë¡ (`<think>`) ìë™ ìˆ¨ê¹€.
            """
        )

        # ============ íƒ­ 1: ê¸°ë³¸ ëŒ€í™” ============
        with gr.Tab("ê¸°ë³¸ ëŒ€í™” (system/user)"):
            with gr.Row():
                model = gr.Textbox(value="deepseek-r1", label="ëª¨ë¸ íƒœê·¸", interactive=True)
                temp = gr.Slider(0.0, 1.5, value=0.7, step=0.05, label="temperature")
                max_toks = gr.Slider(16, 2048, value=256, step=1, label="max_tokens")
            sys = gr.Textbox(label="system", value="ë‹¹ì‹ ì€ ë˜‘ë˜‘í•˜ê³  ì°½ì˜ì ì¸ AIì…ë‹ˆë‹¤.")
            usr = gr.Textbox(label="user", value="ì•ˆë…•í•˜ì„¸ìš”!")
            run = gr.Button("ìƒì„±")
            out = gr.Textbox(label="ì‘ë‹µ", lines=8, show_copy_button=True)
            usage_box = gr.JSON(label="usage")

            def on_basic(m, t, mt, s, u):
                resp = client.chat_complete(
                    messages=[
                        {"role": "system", "content": s},
                        {"role": "user", "content": u},
                    ],
                    model=m, temperature=float(t), max_tokens=int(mt),
                )
                return resp["message"]["content"], resp["usage"]

            run.click(on_basic, [model, temp, max_toks, sys, usr], [out, usage_box])

        # ============ íƒ­ 2: ê¸¸ì´ ë¹„êµ(max_tokens) ============
        with gr.Tab("ê¸¸ì´ ë¹„êµ (ì§§ê²Œ vs ê¸¸ê²Œ)"):
            sys2 = gr.Textbox(label="system", value="ë‹¹ì‹ ì€ ë˜‘ë˜‘í•˜ê³  ì°½ì˜ì ì¸ ì¡°ìˆ˜ì…ë‹ˆë‹¤.")
            usr2 = gr.Textbox(label="user", value="í•œë‹ˆë°œ(Hannibal)ì€ ëˆ„êµ¬ì¸ê°€ìš”?")
            mtag2 = gr.Textbox(value="deepseek-r1", label="ëª¨ë¸ íƒœê·¸")
            short_btn = gr.Button("ì§§ê²Œ ìƒì„± (max_tokens=50)")
            long_btn = gr.Button("ì•½ê°„ ê¸¸ê²Œ ìƒì„± (max_tokens=300)")
            short_out = gr.Textbox(label="ì§§ì€ ì‘ë‹µ", lines=8, show_copy_button=True)
            long_out = gr.Textbox(label="ì•½ê°„ ê¸´ ì‘ë‹µ", lines=12, show_copy_button=True)

            def gen_len(m, sys_msg, user_msg, mt):
                r = client.chat_complete(
                    messages=[{"role": "system", "content": sys_msg},
                              {"role": "user", "content": user_msg}],
                    model=m, max_tokens=int(mt), temperature=0.7
                )
                return r["message"]["content"]

            short_btn.click(gen_len, [mtag2, sys2, usr2, gr.Number(50)], short_out)
            long_btn.click(gen_len, [mtag2, sys2, usr2, gr.Number(300)], long_out)

        # ============ íƒ­ 3: stop ì‹œí€€ìŠ¤ ============
        with gr.Tab("stop ì‹œí€€ìŠ¤"):
            mtag3 = gr.Textbox(value="deepseek-r1", label="ëª¨ë¸ íƒœê·¸")
            sys3 = gr.Textbox(label="system", value="ë‹¹ì‹ ì€ ë˜‘ë˜‘í•˜ê³  ì°½ì˜ì ì¸ ì¡°ìˆ˜ì…ë‹ˆë‹¤.")
            usr3 = gr.Textbox(label="user", value="í•œë‹ˆë°œ(Hannibal)ì€ ëˆ„êµ¬ì¸ê°€ìš”?")
            stop_str = gr.Textbox(label="stop (ì‰¼í‘œë¡œ êµ¬ë¶„)", value=".\n,Human:,AI:")
            mt3 = gr.Slider(16, 1024, value=200, step=1, label="max_tokens")
            run3 = gr.Button("ìƒì„±(ì¤‘ë‹¨ ì‹œí€€ìŠ¤ ì ìš©)")
            out3 = gr.Textbox(label="ì‘ë‹µ", lines=10, show_copy_button=True)

            def on_stop(m, s, u, stop_csv, mt):
                stops = [x.strip() for x in stop_csv.split(",") if x.strip()]
                r = client.chat_complete(
                    messages=[{"role": "system", "content": s},
                              {"role": "user", "content": u}],
                    model=m, max_tokens=int(mt), stop=stops
                )
                return r["message"]["content"]

            run3.click(on_stop, [mtag3, sys3, usr3, stop_str, mt3], out3)

        # ============ íƒ­ 4: ì ‘ë‘ì–´(prefix) ============
        with gr.Tab("ì ‘ë‘ì–´ + ì§ˆë¬¸"):
            mtag4 = gr.Textbox(value="deepseek-r1", label="ëª¨ë¸ íƒœê·¸")
            prefix = gr.Textbox(value="\n\n1. ", label="ì ‘ë‘ì–´(prefix)")
            usr4 = gr.Textbox(label="user", value="ì„¸ê³„ 7ëŒ€ ë¶ˆê°€ì‚¬ì˜ëŠ” ë¬´ì—‡ì¼ê¹Œìš”?")
            run4 = gr.Button("ìƒì„±")
            out4 = gr.Textbox(label="ì‘ë‹µ", lines=10, show_copy_button=True)

            def on_prefix(m, pfx, u):
                r = client.chat_complete(
                    messages=[{"role": "user", "content": f"{u}{pfx}"}],
                    model=m
                )
                return pfx + r["message"]["content"]

            run4.click(on_prefix, [mtag4, prefix, usr4], out4)

        # ============ íƒ­ 5: JSON ì˜ˆì‹œ(ì†Œìˆ˜) ============
        with gr.Tab("JSON ì‘ë‹µ(í”„ë¡¬í”„íŠ¸ ì²´ì¸)"):
            gr.Markdown("LLMì—ê²Œ JSON í˜•íƒœë¡œ ë‹µì„ ìš”êµ¬í•˜ëŠ” ì˜ˆì‹œ(ì›ë³¸ í”„ë¡¬í”„íŠ¸ ì²´ì¸ ìœ ì‚¬).")
            mtag5 = gr.Textbox(value="deepseek-r1", label="ëª¨ë¸ íƒœê·¸")
            prompt5 = gr.Textbox(
                label="ìš”ì²­",
                value="ë‹¤ìŒì„ í¬í•¨í•˜ëŠ” JSONí˜• ë°˜í™˜: 11ê³¼ 65 ì‚¬ì´ì˜ ì†Œìˆ˜(Prime numbers)"
            )
            run5 = gr.Button("ìƒì„±")
            out5 = gr.Code(label="JSON í›„ë³´(LLM ì‘ë‹µ ì›ë¬¸)", language="json")

            def on_json(m, p):
                r = client.chat_complete(
                    messages=[{"role": "user", "content": p}],
                    model=m, temperature=0.2,  # ë” êµ¬ì¡°ì  ì‘ë‹µ ìœ ë„
                    max_tokens=300
                )
                return r["message"]["content"]

            run5.click(on_json, [mtag5, prompt5], out5)

        # ============ íƒ­ 6: ëª¨ë¸ ëª©ë¡ ============
        with gr.Tab("ëª¨ë¸ ëª©ë¡(ollama list)"):
            mlist_btn = gr.Button("ëª¨ë¸ ë‚˜ì—´")
            mlist_json = gr.JSON(label="ollama.list() ì›ë³¸")
            mids_btn = gr.Button("ëª¨ë¸ IDë§Œ")
            mids_txt = gr.Textbox(label="IDs", lines=8, show_copy_button=True)

            def on_list():
                return {"models": client.models_list()}

            def on_ids():
                return "\n".join(client.model_ids())

            mlist_btn.click(on_list, outputs=mlist_json)
            mids_btn.click(on_ids, outputs=mids_txt)

        # ============ íƒ­ 7: ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… ============
        with gr.Tab("ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ…"):
            mtag7 = gr.Textbox(value="deepseek-r1", label="ëª¨ë¸ íƒœê·¸")
            sys7 = gr.Textbox(value="ë‹¹ì‹ ì€ ìœ ìš©í•˜ê³  ì •ì¤‘í•œ í•œêµ­ì–´ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.", label="system")
            temp7 = gr.Slider(0.0, 1.5, value=0.7, step=0.05, label="temperature")
            mt7 = gr.Slider(16, 2048, value=256, step=1, label="max_tokens")
            chat = gr.ChatInterface(
                fn=lambda history, message, m, s, t, mt: stream_adapter(history, message, m, s, t, mt),
                additional_inputs=[mtag7, sys7, temp7, mt7],
                title="ë¡œì»¬ LLM ìŠ¤íŠ¸ë¦¬ë°",
                chatbot=gr.Chatbot(height=420, show_copy_button=True, likeable=True),
                textbox=gr.Textbox(placeholder="ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”...", lines=2),
                cache_examples=False,
                clear_btn="ëŒ€í™” ì§€ìš°ê¸°",
                retry_btn=None,
                undo_btn=None,
            )

        gr.Markdown(
            """
            ### ì°¸ê³ : OpenAI â†’ ë¡œì»¬ ë§¤í•‘
            - `OpenAI(api_key=...)` â†’ (ë¶ˆí•„ìš”) ë¡œì»¬ ì‹¤í–‰
            - `client.chat.completions.create(...)` â†’ `LocalOpenSourceClient.chat_complete(...)`
            - `model="gpt-4o-mini"` â†’ `model="deepseek-r1"`(ë˜ëŠ” ì„¤ì¹˜í•œ ë‹¤ë¥¸ ë¡œì»¬ ëª¨ë¸ íƒœê·¸)
            - `max_tokens` â†’ `num_predict` ë¡œ ëŒ€ì‘
            - `temperature` ë™ì¼
            - `stop=["...", "..."]` ë™ì¼
            - `response.usage` â†’ `prompt_eval_count`/`eval_count`ë¡œ ê·¼ì‚¬
            """
        )

    return demo


def stream_adapter(
    history: List[Tuple[str, str]],
    message: str,
    model: str,
    system: str,
    temperature: float,
    max_tokens: int,
):
    """
    Gradio ChatInterfaceìš© ìŠ¤íŠ¸ë¦¬ë° ì–´ëŒ‘í„°.
    ë‚´ë¶€ì ìœ¼ë¡œ ì „ì²´ íˆìŠ¤í† ë¦¬ë¥¼ ì¬êµ¬ì„±í•´ë„ ë˜ì§€ë§Œ,
    ì—¬ê¸°ì„œëŠ” ìµœê·¼ user 1í„´ë§Œ ëª¨ë¸ì— ì „ë‹¬(ê°„ë‹¨í™”).
    """
    history = history + [(message, "")]
    msgs = []
    if system.strip():
        msgs.append({"role": "system", "content": system})
    msgs.append({"role": "user", "content": message})

    buffer = ""
    for clean in client.chat_stream(
        messages=msgs,
        model=model,
        temperature=float(temperature),
        max_tokens=int(max_tokens),
    ):
        # cleanì€ ì „ì²´ ëˆ„ì (clean) í…ìŠ¤íŠ¸. ì¦ë¶„ë§Œ ë³´ì—¬ì£¼ê¸°.
        inc = clean[len(buffer):]
        buffer = clean
        if inc:
            history[-1] = (message, buffer)
            yield history, None
        time.sleep(0.01)
    history[-1] = (message, buffer)
    yield history, None


# ==============================
# ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
# ==============================
if __name__ == "__main__":
    # CLI ì˜ˆì‹œ(ì›í•˜ë©´ ì£¼ì„ í•´ì œ)
    # resp = client.chat_complete(
    #     messages=[{"role": "system", "content": "ë‹¹ì‹ ì€ ì‹œë¥¼ ì˜ ì“°ëŠ” AIì…ë‹ˆë‹¤."},
    #               {"role": "user", "content": "AIì— ëŒ€í•œ ì‹œë¥¼ í•˜ë‚˜ ì‘ì„±í•´ì¤˜"}],
    #     max_tokens=200, temperature=0.7
    # )
    # print(resp["message"]["content"])
    app = build_app()
    app.launch(server_name="0.0.0.0", server_port=7860, show_error=True)




