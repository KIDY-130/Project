#84
"""
OpenCode ë³€í™˜ë³¸: Closed LLM ì˜ˆì œ(ì˜ˆ: ChatGPT/OpenAI)ë‚˜ ë‹¨ìˆœ transformers íŒŒì´í”„ë¼ì¸ ì½”ë“œë¥¼
ë¡œì»¬ ì˜¤í”ˆì†ŒìŠ¤ LLM(DeepSeek-R1, Ollama) + Gradio GUIë¡œ ë°”ê¾¼ ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.

âœ… ì „ì œ
- ë¡œì»¬ PCì— Ollamaê°€ ì„¤ì¹˜ë˜ì–´ ìˆê³ , deepseek-r1 ëª¨ë¸ì´ ì„¤ì¹˜ë˜ì–´ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
  (ì—†ë‹¤ë©´ í„°ë¯¸ë„ì—ì„œ: `ollama pull deepseek-r1`)
- API Keyê°€ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (ì™„ì „ ë¡œì»¬ ì¶”ë¡ )

ğŸ§© ì£¼ìš” ê¸°ëŠ¥
1) ê°„ë‹¨í•œ â€œì—¬ëŸ¬ ì‹œë„â€ í…ìŠ¤íŠ¸ ìƒì„±(ì›ë³¸ ì½”ë“œì˜ num_return_sequences ìœ ì‚¬)
2) ëŒ€í™”í˜• ì±„íŒ… UI(Gradio) â€” ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥
3) DeepSeek-R1ì˜ ë‚´ë¶€ ì¶”ë¡ (<think>...</think>)ì„ ìë™ìœ¼ë¡œ ìˆ¨ê¹€
4) íŒŒë¼ë¯¸í„°(ì˜¨ë„, ìµœëŒ€ í† í°, ì‹œë„ íšŸìˆ˜) ì¡°ì ˆ

ğŸ“¦ í•„ìš” íŒ¨í‚¤ì§€
- gradio, ollama (íŒŒì´ì¬ íŒ¨í‚¤ì§€ë§Œ ì„¤ì¹˜í•˜ë©´ ë©ë‹ˆë‹¤)
  pip install gradio ollama
"""

import re
import time
from typing import List, Generator, Tuple

import gradio as gr

# Ollama íŒŒì´ì¬ í´ë¼ì´ì–¸íŠ¸ (ë¡œì»¬ ì„œë²„ http://localhost:11434 ì— ë¶™ìŠµë‹ˆë‹¤)
import ollama


# -------------------------------
# ìœ í‹¸: DeepSeek-R1ì˜ <think> ë¸”ë¡ ì œê±°
# -------------------------------
THINK_TAG_PATTERN = re.compile(r"<think>.*?</think>", re.DOTALL | re.IGNORECASE)

def strip_think(text: str) -> str:
    """
    DeepSeek-R1ì€ ë‚´ë¶€ ì¶”ë¡ ì„ <think>...</think>ë¡œ ë‚´ë³´ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    ì‚¬ìš©ìì—ê²ŒëŠ” ê³µê°œí•˜ì§€ ì•Šë„ë¡ í•´ë‹¹ ë¸”ë¡ì„ ëª¨ë‘ ì œê±°í•©ë‹ˆë‹¤.
    """
    return THINK_TAG_PATTERN.sub("", text).strip()


# -------------------------------
# ë‹¤ì¤‘ ì‹œë„ í…ìŠ¤íŠ¸ ìƒì„± (ì›ë³¸ pipeline + num_return_sequences ìœ ì‚¬)
# -------------------------------
def generate_n_completions(
    prompt: str,
    n: int = 3,
    max_tokens: int = 50,
    temperature: float = 0.8,
    model: str = "deepseek-r1",
) -> List[str]:
    """
    promptì— ëŒ€í•´ në²ˆ ìƒì„±í•©ë‹ˆë‹¤.
    ê° ì‹œë„ëŠ” ë™ì¼í•œ íŒŒë¼ë¯¸í„°ë¡œ ë…ë¦½ì ìœ¼ë¡œ í˜¸ì¶œí•˜ì—¬ ë‹¤ì–‘ì„±ì„ í™•ë³´í•©ë‹ˆë‹¤.
    """
    outputs = []
    for _ in range(max(1, n)):
        # ollama.chatì€ Chat í˜•ì‹(message list)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
        # num_predict: ìµœëŒ€ ìƒì„± í† í° ìˆ˜, temperature: ì°½ì˜ì„±
        resp = ollama.chat(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            options={
                "num_predict": int(max_tokens),
                "temperature": float(temperature),
            },
        )
        content = resp.get("message", {}).get("content", "")
        outputs.append(strip_think(content))
    return outputs


# -------------------------------
# ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ…(Gradio ChatInterfaceìš©)
# -------------------------------
def chat_stream(
    history: List[Tuple[str, str]],
    message: str,
    model: str = "deepseek-r1",
    temperature: float = 0.7,
    max_tokens: int = 256,
) -> Generator[Tuple[List[Tuple[str, str]], str], None, None]:
    """
    Gradio ChatInterfaceì˜ stream ì‘ë‹µ í•¨ìˆ˜.
    Ollamaì˜ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ë°›ì•„ í† í°ë³„ë¡œ í˜ë ¤ë³´ëƒ…ë‹ˆë‹¤.
    ë‚´ë¶€ <think>ëŠ” ëˆ„ì  ë²„í¼ì—ì„œ ì œê±°í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ë³´ì´ì§€ ì•Šê²Œ í•©ë‹ˆë‹¤.
    """
    # Gradioê°€ ê¸°ëŒ€í•˜ëŠ” êµ¬ì¡°: (history + [(user, assistant_partial)], None)
    # ë¨¼ì € ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
    history = history + [(message, "")]

    # ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ: ollama.chat(stream=True)
    stream = ollama.chat(
        model=model,
        messages=[{"role": "user", "content": message}],
        stream=True,
        options={
            "num_predict": int(max_tokens),
            "temperature": float(temperature),
        },
    )

    buffer = ""  # ëˆ„ì  ë²„í¼
    visible = ""  # ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì¤„(think ì œê±°ëœ) ë²„ì „

    for chunk in stream:
        delta = chunk.get("message", {}).get("content", "")
        if not delta:
            continue
        buffer += delta

        # ë§¤ í† í°ë§ˆë‹¤ <think> ì œê±° ë²„ì „ì„ ê³„ì‚° (ê°„ë‹¨/ì•ˆì „í•˜ê²Œ ì „ì²´ ì¬ê³„ì‚°)
        clean = strip_think(buffer)

        # ì¦ë¶„ë§Œ ê³„ì‚°í•˜ì—¬ í‘œì‹œ(ê¹œë¹¡ì„ ì¤„ì´ê¸°)
        increment = clean[len(visible):]
        if increment:
            visible += increment
            history[-1] = (message, visible)
            # ChatInterface ìŠ¤íŠ¸ë¦¬ë° ê·œì•½: yield (history, None)
            yield history, None

        # ë„ˆë¬´ ë¹ ë¥¸ í† í° í˜ë¦¼ì„ ë°©ì§€(ì‹œê°ì  ì•ˆì •)
        time.sleep(0.01)

    # ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ ì‹œ í•œ ë²ˆ ë” í™•ì • ê°±ì‹ 
    history[-1] = (message, strip_think(buffer))
    yield history, None


# -------------------------------
# Gradio ì•± êµ¬ì„±
# -------------------------------
def build_app():
    with gr.Blocks(title="DeepSeek-R1 (Ollama, Local)") as demo:
        gr.Markdown(
            """
            # ğŸ§  DeepSeek-R1 (ë¡œì»¬ Â· Ollama)  
            - ì™„ì „ ë¡œì»¬ ì¶”ë¡  Â· API Key ë¶ˆí•„ìš”  
            - ë‚´ë¶€ ì¶”ë¡ (`<think>...</think>`)ì€ ìë™ìœ¼ë¡œ ìˆ¨ê¹ë‹ˆë‹¤  
            """
        )

        with gr.Tab("ì—¬ëŸ¬ ì‹œë„ í…ìŠ¤íŠ¸ ìƒì„±"):
            with gr.Row():
                prompt = gr.Textbox(
                    label="í”„ë¡¬í”„íŠ¸",
                    placeholder="ì˜ˆ) Hello, I'm a language model",
                    lines=3,
                    value="Hello, I'm a language model",
                )
            with gr.Row():
                n = gr.Slider(1, 10, value=3, step=1, label="ì‹œë„ íšŸìˆ˜ (num_return_sequences)")
                max_tokens = gr.Slider(16, 1024, value=50, step=1, label="ìµœëŒ€ ìƒì„± í† í° (max_tokens)")
            with gr.Row():
                temperature = gr.Slider(0.0, 1.5, value=0.8, step=0.05, label="ì˜¨ë„ (temperature)")
                model = gr.Textbox(value="deepseek-r1", label="ëª¨ë¸(ë¡œì»¬ Ollama íƒœê·¸)", interactive=True)
            run_btn = gr.Button("ìƒì„±í•˜ê¸°", variant="primary")
            outputs = gr.Dataset(
                components=[gr.Textbox(label="ìƒì„± ê²°ê³¼", lines=4, show_copy_button=True)],
                label="ìƒì„± í…ìŠ¤íŠ¸",
                samples=[],
            )

            def on_generate(p, n, max_t, temp, model_tag):
                results = generate_n_completions(
                    prompt=p, n=int(n), max_tokens=int(max_t), temperature=float(temp), model=model_tag
                )
                # Datasetì— í•œ ë²ˆì— ë„£ê¸° ìœ„í•´ [[text], [text], ...] í˜•íƒœë¡œ ë°˜í™˜
                return [[r] for r in results]

            run_btn.click(
                on_generate,
                inputs=[prompt, n, max_tokens, temperature, model],
                outputs=[outputs],
            )

        with gr.Tab("ì±„íŒ… (ìŠ¤íŠ¸ë¦¬ë°)"):
            with gr.Row():
                model2 = gr.Textbox(value="deepseek-r1", label="ëª¨ë¸(ë¡œì»¬ Ollama íƒœê·¸)", interactive=True)
                temperature2 = gr.Slider(0.0, 1.5, value=0.7, step=0.05, label="ì˜¨ë„ (temperature)")
                max_tokens2 = gr.Slider(16, 2048, value=256, step=1, label="ìµœëŒ€ ìƒì„± í† í° (max_tokens)")

            chat = gr.ChatInterface(
                fn=lambda history, message: chat_stream(
                    history, message, model=model2.value, temperature=temperature2.value, max_tokens=max_tokens2.value
                ),
                title="ë¡œì»¬ LLM ì±„íŒ…",
                chatbot=gr.Chatbot(height=420, show_copy_button=True, likeable=True),
                textbox=gr.Textbox(placeholder="ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”...", lines=2),
                additional_inputs=[model2, temperature2, max_tokens2],
                cache_examples=False,
                retry_btn=None,
                undo_btn=None,
                clear_btn="ëŒ€í™” ì§€ìš°ê¸°",
            )

        gr.Markdown(
            """
            ### ì‚¬ìš© íŒ
            - ëª¨ë¸ì´ ì—†ë‹¤ë©´ í„°ë¯¸ë„ì—ì„œ ë¨¼ì € ì„¤ì¹˜: `ollama pull deepseek-r1`
            - ë‹¤ë¥¸ ë¡œì»¬ ëª¨ë¸ íƒœê·¸ë„ ì‚¬ìš© ê°€ëŠ¥(ì˜ˆ: `deepseek-r1:latest`).
            - ìƒì„±ì´ ë„ˆë¬´ ì§§ë‹¤ë©´ `ìµœëŒ€ ìƒì„± í† í°`ì„ ëŠ˜ë ¤ë³´ì„¸ìš”.
            """
        )
    return demo

# -------------------------------
# ìŠ¤í¬ë¦½íŠ¸ ì§ì ‘ ì‹¤í–‰ ì§„ì…ì 
# -------------------------------
if __name__ == "__main__":
    """
    ì›ë³¸ ì½”ë“œì™€ì˜ ë§¤í•‘
    - pipeline('text-generation', model='gpt2')  ->  Ollama ë¡œì»¬ ëª¨ë¸ 'deepseek-r1'
    - generator(..., num_return_sequences=3)     ->  generate_n_completions(..., n=3)
    - max_length                                 ->  max_tokens
    - OpenAI í‚¤ ë¶ˆí•„ìš”, ì¸í„°ë„· ì—°ê²° ë¶ˆí•„ìš”(ì™„ì „ ë¡œì»¬)
    """
    # ê°„ë‹¨í•œ CLI ë°ëª¨(ì˜µì…˜): ì£¼ì„ì„ í•´ì œí•˜ë©´ í„°ë¯¸ë„ì—ì„œë„ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥
    # prompt = input("You: ")
    # outs = generate_n_completions(prompt, n=3, max_tokens=50, temperature=0.8, model="deepseek-r1")
    # for i, o in enumerate(outs, 1):
    #     print(f"\n=== ì‹œë„ {i} ===\n{o}")

    # Gradio UI ì‹¤í–‰
    app = build_app()
    # share=Trueë¥¼ ì›í•˜ë©´ ì™¸ë¶€ ì ‘ì†ë„ í—ˆìš©ë©ë‹ˆë‹¤.
    app.launch(server_name="0.0.0.0", server_port=7860, show_error=True)
