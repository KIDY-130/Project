#89
"""
OpenCode ë³€í™˜ë³¸ (v2): Whisper + DeepSeek-R1(Ollama) + Gradio GUI ì˜¬ì¸ì› ìŠ¤í¬ë¦½íŠ¸
--------------------------------------------------------------------------
ì‚¬ìš©ìê°€ ì œê³µí•œ Closed LLM ì˜ˆì œë“¤(OpenAI API: whisper-1, gpt-4o ë“±)ì„
**ì˜¤í”ˆì†ŒìŠ¤ ë¡œì»¬ í™˜ê²½**(Whisper/faster-whisper + Ollama deepseek-r1)ìœ¼ë¡œ ì¹˜í™˜í–ˆìŠµë‹ˆë‹¤.
- OpenAI API Key ì˜ì¡´ì„± ì œê±°
- ì „ì‚¬(STT): faster-whisper(ê¶Œì¥) ë˜ëŠ” openai-whisper (ë‘˜ ë‹¤ ë¡œì»¬)
- ë²ˆì—­: Whisperì˜ translate ëª¨ë“œ(ì˜ì–´ë¡œ ë²ˆì—­)
- í›„ì²˜ë¦¬(Post-process): Ollamaì˜ deepseek-r1 ë¡œì»¬ LLM
- GUI: Gradio

ì›ë³¸ íŒŒì¼ ëŒ€ì‘
- basic whisper ì˜ˆì œ            â†’ ë³¸ ìŠ¤í¬ë¦½íŠ¸ì˜ "transcribe (ì›ë¬¸ ì „ì‚¬)"
- whisper_test.py(ë‹¤êµ­ì–´)       â†’ ì–¸ì–´ ë“œë¡­ë‹¤ìš´ìœ¼ë¡œ ë™ì¼ ìˆ˜í–‰
- transcribe_with_prompt.py     â†’ "ì „ì‚¬ íŒíŠ¸" ì…ë ¥ìœ¼ë¡œ ë™ì¼ ìˆ˜í–‰
- transcribe.py/translate.py    â†’ íŒŒì¼ ì—…ë¡œë“œ + ëª¨ë“œ ì„ íƒìœ¼ë¡œ ìˆ˜í–‰
- post_process.py               â†’ "í›„ì²˜ë¦¬ ì‚¬ìš©" ì²´í¬ + ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì…ë ¥

ì‚¬ì „ ì¤€ë¹„
1) Python íŒ¨í‚¤ì§€ ì„¤ì¹˜
   pip install -U faster-whisper gradio requests soundfile numpy
   # ë˜ëŠ” openai-whisperë¥¼ ì“°ë ¤ë©´
   pip install -U openai-whisper gradio requests soundfile numpy

2) Ollama ì„¤ì¹˜ ë° ëª¨ë¸ ì¤€ë¹„(ë¡œì»¬ PC ì„¤ì¹˜ ê°€ì •)
   https://ollama.com/download
   ollama pull deepseek-r1:7b   # ë˜ëŠ” :32b

3) ì‹¤í–‰
   python app_whisper_ollama_gradio.py
   # ë¸Œë¼ìš°ì €ì—ì„œ ì˜¤ë””ì˜¤ ì—…ë¡œë“œ í›„ ì „ì‚¬/ë²ˆì—­/í›„ì²˜ë¦¬ ì‹¤í–‰
"""
from __future__ import annotations
import os
import re
import time
import requests
from dataclasses import dataclass
from typing import Optional, Dict, Any, Tuple, List

import gradio as gr

# ===============================
# ì„¤ì • ì„¹ì…˜
# ===============================

# ASR ë°±ì—”ë“œ ì„ íƒ: "faster-whisper"(ê¶Œì¥) ë˜ëŠ” "whisper"
ASR_BACKEND = os.environ.get("ASR_BACKEND", "faster-whisper")

# faster-whisper/openai-whisper ê³µìš© ëª¨ë¸ í¬ê¸° (tiny/base/small/medium/large-v3)
DEFAULT_ASR_MODEL = os.environ.get("ASR_MODEL", "base")

# Ollama ì„¤ì •
OLLAMA_HOST = os.environ.get("OLLAMA_HOST", "http://127.0.0.1:11434")
OLLAMA_MODEL = os.environ.get("OLLAMA_MODEL", "deepseek-r1:7b")

# DeepSeek-R1ì˜ <think> ë…¸ì¶œ ê¸°ë³¸ ì—¬ë¶€
HIDE_REASONING_BY_DEFAULT = True

# ===============================
# ìœ í‹¸ í•¨ìˆ˜
# ===============================

def human_time(s: float) -> str:
    """ì´ˆ â†’ ì‚¬ëŒì´ ì½ê¸° ì‰¬ìš´ ë¬¸ìì—´"""
    if s is None:
        return "-"
    if s < 1.0:
        return f"{s*1000:.0f} ms"
    m, ss = divmod(int(s), 60)
    if m == 0:
        return f"{s:.2f} s"
    h, mm = divmod(m, 60)
    if h == 0:
        return f"{mm}m {ss}s"
    return f"{h}h {mm}m {ss}s"


def strip_deepseek_think(text: str, keep_reasoning: bool = False) -> Tuple[str, Optional[str]]:
    """DeepSeek-R1ì˜ <think>...</think> ë¸”ë¡ì„ ì œê±°/ë¶„ë¦¬"""
    think_blocks = re.findall(r"<think>(.*?)</think>", text, flags=re.DOTALL)
    clean = re.sub(r"<think>.*?</think>", "", text, flags=re.DOTALL).strip()
    reasoning = "\n\n---\n".join(tb.strip() for tb in think_blocks) if think_blocks else None
    if keep_reasoning:
        return clean, reasoning
    return clean, None


def lang_label_to_code(label: str) -> Optional[str]:
    """UI ë¼ë²¨ â†’ ISO ì–¸ì–´ì½”ë“œ(Noneì€ ìë™)"""
    mapping = {
        "ìë™ ê°ì§€ (Auto)": None,
        "ì˜ì–´ (en)": "en", "í•œêµ­ì–´ (ko)": "ko", "ì¤‘êµ­ì–´ (zh)": "zh", "ì¼ë³¸ì–´ (ja)": "ja",
        "ìŠ¤í˜ì¸ì–´ (es)": "es", "í”„ë‘ìŠ¤ì–´ (fr)": "fr", "ë…ì¼ì–´ (de)": "de",
        "ì´íƒˆë¦¬ì•„ì–´ (it)": "it", "í¬ë¥´íˆ¬ê°ˆì–´ (pt)": "pt", "ëŸ¬ì‹œì•„ì–´ (ru)": "ru",
        "ë² íŠ¸ë‚¨ì–´ (vi)": "vi", "íƒœêµ­ì–´ (th)": "th",
    }
    return mapping.get(label, None)

# ===============================
# ASR (Whisper) ë˜í¼
# ===============================

@dataclass
class ASRResult:
    text: str
    lang: Optional[str]
    segments: Optional[List[dict]]
    info: Dict[str, Any]


class ASRWhisper:
    """
    ë¡œì»¬ Whisper STT ë˜í¼
    - faster-whisper: GPU ê°€ì†, ë¹ ë¥´ê³  ë©”ëª¨ë¦¬ íš¨ìœ¨ì 
    - openai-whisper: ì„¤ì¹˜ ê°„ë‹¨, ì†ë„ëŠ” ëŠë¦´ ìˆ˜ ìˆìŒ
    """
    def __init__(self, model_size: str = DEFAULT_ASR_MODEL, backend: str = ASR_BACKEND):
        self.backend = backend
        self.model_size = model_size
        self._model = None

    def load(self):
        if self._model is not None:
            return
        if self.backend == "faster-whisper":
            from faster_whisper import WhisperModel  # lazy import
            # device="auto" â†’ GPU ìˆìœ¼ë©´ cuda, ì—†ìœ¼ë©´ CPU
            self._model = WhisperModel(self.model_size, device="auto", compute_type="auto")
        elif self.backend == "whisper":
            import whisper  # lazy import
            self._model = whisper.load_model(self.model_size)
        else:
            raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” ASR ë°±ì—”ë“œì…ë‹ˆë‹¤: " + str(self.backend))

    def transcribe(
        self,
        audio_path: str,
        language: Optional[str] = None,
        task: str = "transcribe",  # 'transcribe' ì›ë¬¸, 'translate' ì˜ì–´ ë²ˆì—­
        initial_prompt: Optional[str] = None,
    ) -> ASRResult:
        self.load()
        t0 = time.time()

        if self.backend == "faster-whisper":
            segments, info = self._model.transcribe(
                audio_path,
                language=language,
                task=task,
                initial_prompt=initial_prompt,
                beam_size=5,
                vad_filter=True,
            )
            text_parts = []
            seg_list: List[dict] = []
            for s in segments:
                text_parts.append(s.text)
                seg_list.append({
                    "start": s.start, "end": s.end, "text": s.text,
                    "avg_logprob": s.avg_logprob, "no_speech_prob": s.no_speech_prob,
                    "compression_ratio": s.compression_ratio,
                })
            full_text = "".join(text_parts).strip()
            elapsed = time.time() - t0
            return ASRResult(
                text=full_text,
                lang=getattr(info, "language", None),
                segments=seg_list,
                info={
                    "duration_s": getattr(info, "duration", None),
                    "time": elapsed,
                    "backend": self.backend,
                    "model_size": self.model_size,
                },
            )

        # openai-whisper (íŒŒì´ì¬íŒ)
        import whisper
        result = self._model.transcribe(
            audio_path,
            language=language,
            task=task,
            initial_prompt=initial_prompt,
            verbose=False,
        )
        elapsed = time.time() - t0
        return ASRResult(
            text=result.get("text", "").strip(),
            lang=result.get("language"),
            segments=result.get("segments"),
            info={
                "duration_s": result.get("duration"),
                "time": elapsed,
                "backend": self.backend,
                "model_size": self.model_size,
            },
        )

# ===============================
# Ollama (DeepSeek-R1) ë˜í¼
# ===============================

class OllamaClient:
    """Ollama /api/chat ê°„ë‹¨ ë˜í¼"""
    def __init__(self, host: str = OLLAMA_HOST, model: str = OLLAMA_MODEL, timeout: int = 120):
        self.host = host.rstrip("/")
        self.model = model
        self.timeout = timeout

    def chat(self, system_prompt: str, user_prompt: str, temperature: float = 0.2) -> str:
        url = f"{self.host}/api/chat"
        payload = {
            "model": self.model,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            "options": {"temperature": float(temperature)},
            "stream": False,
        }
        resp = requests.post(url, json=payload, timeout=self.timeout)
        resp.raise_for_status()
        data = resp.json()
        return data.get("message", {}).get("content", "")

# ===============================
# ì „ì‚¬ + í›„ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
# ===============================

ASR = ASRWhisper(model_size=DEFAULT_ASR_MODEL, backend=ASR_BACKEND)
OLLAMA = OllamaClient()

DEFAULT_SYSTEM_PROMPT = (
    "ë‹¹ì‹ ì€ ì „ë¬¸ ì˜¤ë””ì˜¤ ì „ì‚¬ êµì •ê°€ì…ë‹ˆë‹¤.\n"
    "ê·œì¹™:\n"
    "1) ì£¼ì–´ì§„ ì „ì‚¬ í…ìŠ¤íŠ¸ì˜ ì˜¤íƒ€/ì˜¤ì¸ì‹ì„ ë¬¸ë§¥ì— ë§ê²Œ êµì •í•˜ì„¸ìš”.\n"
    "2) ì¸ëª…/ì§€ëª…/ì „ë¬¸ìš©ì–´ëŠ” ì¼ë°˜ í‘œê¸°ë²•ìœ¼ë¡œ í†µì¼í•˜ì„¸ìš”.\n"
    "3) ë¶ˆí•„ìš”í•œ ì„¤ëª…ì€ ì œê±°í•˜ê³  **êµì •ëœ ë³¸ë¬¸ë§Œ** ì¶œë ¥í•˜ì„¸ìš”.\n"
    "4) íƒ€ì„ìŠ¤íƒ¬í”„ê°€ ìˆë‹¤ë©´ ìœ ì§€í•˜ë˜ ë¬¸ì¥ë¶€í˜¸/ë„ì–´ì“°ê¸°ë¥¼ ì •ë¦¬í•˜ì„¸ìš”.\n"
)

EXAMPLE_HINT = "ì˜ˆ: 'Cypher' ë‹¨ì–´ë¥¼ ìš°ì„ ì‹œí•´ì„œ ì¸ì‹í•´ì¤˜"


def run_pipeline(
    audio_file: Optional[str],
    asr_model_size: str,
    mode: str,  # "transcribe" | "translate"
    ui_lang_label: str,
    hint: str,
    do_postprocess: bool,
    sys_prompt: str,
    temperature: float,
    show_reasoning: bool,
) -> Tuple[str, str, str, Dict[str, Any]]:
    """Gradio ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬"""
    if not audio_file:
        return "", "", "ì˜¤ë””ì˜¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.", {}

    # ASR ëª¨ë¸(í¬ê¸°) ê°±ì‹  ì‹œ ì¬ë¡œë”©
    ASR.model_size = asr_model_size
    ASR._model = None

    lang_code = lang_label_to_code(ui_lang_label)
    task = "translate" if mode == "translate" else "transcribe"

    asr_res = ASR.transcribe(
        audio_file,
        language=lang_code,
        task=task,
        initial_prompt=hint or None,
    )

    raw_txt = asr_res.text or ""
    meta = {
        "asr_backend": asr_res.info.get("backend"),
        "asr_model": asr_res.info.get("model_size"),
        "detected_language": asr_res.lang,
        "audio_duration_s": asr_res.info.get("duration_s"),
        "asr_time": human_time(asr_res.info.get("time", 0.0)),
        "mode": mode,
    }

    if not do_postprocess or len(raw_txt.strip()) == 0:
        return raw_txt, "", "LLM í›„ì²˜ë¦¬ ë¯¸ì‹¤í–‰ (ë˜ëŠ” ì „ì‚¬ ê²°ê³¼ ì—†ìŒ)", meta

    try:
        llm_out = OLLAMA.chat(system_prompt=sys_prompt.strip() or DEFAULT_SYSTEM_PROMPT,
                              user_prompt=raw_txt,
                              temperature=float(temperature))
        cleaned, reasoning = strip_deepseek_think(llm_out, keep_reasoning=show_reasoning)
        return raw_txt, cleaned, (reasoning or "ì‚¬ê³ (think) ìˆ¨ê¹€"), meta
    except Exception as e:
        return raw_txt, "", f"Ollama í›„ì²˜ë¦¬ ì˜¤ë¥˜: {e}", meta

# ===============================
# Gradio UI
# ===============================

ASR_MODEL_CHOICES = ["tiny", "base", "small", "medium", "large-v3"]
LANG_CHOICES = [
    "ìë™ ê°ì§€ (Auto)",
    "ì˜ì–´ (en)", "í•œêµ­ì–´ (ko)", "ì¤‘êµ­ì–´ (zh)", "ì¼ë³¸ì–´ (ja)",
    "ìŠ¤í˜ì¸ì–´ (es)", "í”„ë‘ìŠ¤ì–´ (fr)", "ë…ì¼ì–´ (de)",
    "ì´íƒˆë¦¬ì•„ì–´ (it)", "í¬ë¥´íˆ¬ê°ˆì–´ (pt)", "ëŸ¬ì‹œì•„ì–´ (ru)",
    "ë² íŠ¸ë‚¨ì–´ (vi)", "íƒœêµ­ì–´ (th)",
]

with gr.Blocks(title="Whisper + DeepSeek-R1 (Ollama) ë¡œì»¬ ì „ì‚¬/ë²ˆì—­", theme=gr.themes.Soft()) as demo:
    gr.Markdown(
        """
        # ğŸ™ï¸ Whisper + ğŸ§  DeepSeek-R1 (Ollama) â€” ë¡œì»¬ ì „ì‚¬Â·ë²ˆì—­Â·êµì •
        - **ASR**: faster-whisper / whisper (ë¡œì»¬)
        - **LLM í›„ì²˜ë¦¬**: deepseek-r1 (Ollama, ë¡œì»¬)
        - API Key ë¶ˆí•„ìš”, ì¸í„°ë„· ë¶ˆí•„ìš”(ëª¨ë¸ë§Œ ì„¤ì¹˜ë˜ì–´ ìˆìœ¼ë©´ ë¨)
        """
    )

    with gr.Row():
        audio = gr.Audio(label="ì˜¤ë””ì˜¤ íŒŒì¼ ì—…ë¡œë“œ", sources=["upload"], type="filepath")

    with gr.Row():
        asr_model = gr.Dropdown(ASR_MODEL_CHOICES, value=DEFAULT_ASR_MODEL, label="ASR ëª¨ë¸ í¬ê¸°")
        lang = gr.Dropdown(LANG_CHOICES, value="ìë™ ê°ì§€ (Auto)", label="ì–¸ì–´")
        mode = gr.Radio(choices=["transcribe (ì›ë¬¸ ì „ì‚¬)", "translate (ì˜ì–´ ë²ˆì—­)"],
                        value="transcribe (ì›ë¬¸ ì „ì‚¬)", label="ì‘ì—… ëª¨ë“œ")

    hint = gr.Textbox(value=EXAMPLE_HINT, label="ì „ì‚¬ íŒíŠ¸(ê³ ìœ ëª…ì‚¬/ì² ì)", placeholder="ì˜ˆ: Cypher, Kubernetes, Winston Churchill ...")

    with gr.Accordion("LLM(DeepSeek-R1) í›„ì²˜ë¦¬ ì„¤ì •", open=False):
        enable_post = gr.Checkbox(value=True, label="í›„ì²˜ë¦¬ ì‚¬ìš© (ê¶Œì¥)")
        sys_prompt = gr.Textbox(value=DEFAULT_SYSTEM_PROMPT, lines=6, label="ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸")
        temperature = gr.Slider(0.0, 1.2, value=0.2, step=0.1, label="ìƒ˜í”Œë§ ì˜¨ë„")
        show_reasoning = gr.Checkbox(value=(not HIDE_REASONING_BY_DEFAULT), label="DeepSeek ì‚¬ê³ (think) í‘œì‹œ")

    run_btn = gr.Button("ì‹¤í–‰", variant="primary")

    with gr.Row():
        raw_out = gr.Textbox(lines=10, label="ì „ì‚¬ ê²°ê³¼ (RAW)")
        post_out = gr.Textbox(lines=10, label="í›„ì²˜ë¦¬ ê²°ê³¼ (CLEANED)")
    reason_out = gr.Textbox(lines=8, label="(ì„ íƒ) ëª¨ë¸ ì‚¬ê³ (think) ë˜ëŠ” ë©”ì‹œì§€", value="ì‚¬ê³ (think) ìˆ¨ê¹€")

    meta_json = gr.JSON(label="ë©”íƒ€ ì •ë³´")

    def _run(audio_path, asr_size, mode_label, lang_label, hint_text, do_post, sys_p, temp, show_think):
        mode_key = "translate" if "translate" in mode_label else "transcribe"
        return run_pipeline(audio_path, asr_size, mode_key, lang_label, hint_text, do_post, sys_p, temp, show_think)

    run_btn.click(
        _run,
        inputs=[audio, asr_model, mode, lang, hint, enable_post, sys_prompt, temperature, show_reasoning],
        outputs=[raw_out, post_out, reason_out, meta_json],
    )

    gr.Markdown(
        """
        ### ì‚¬ìš© íŒ
        - **ì–¸ì–´**: íŠ¹ì • ì–¸ì–´ë¥¼ ê°•ì œí•˜ë©´(ì˜ˆ: ì˜ì–´), ë‹¤êµ­ì–´ í˜¼í•© ìŒì„±ì˜ í˜¼ì„ ì„ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        - **ë²ˆì—­ ëª¨ë“œ**: Whisperì˜ `translate`ì€ ê²°ê³¼ë¥¼ ì˜ì–´ë¡œ ì¼ê´„ ë²ˆì—­í•©ë‹ˆë‹¤.
        - **íŒíŠ¸**: ê³ ìœ ëª…ì‚¬/ë¸Œëœë“œ/ì „ë¬¸ìš©ì–´ë¥¼ íŒíŠ¸ì— ì ìœ¼ë©´ ì˜¤ì¸ì‹ì´ ì¤„ì–´ë“­ë‹ˆë‹¤.
        - **ëª¨ë¸ í¬ê¸°**: `tiny/base`ëŠ” ë¹ ë¥´ì§€ë§Œ ì •í™•ë„ ë‚®ìŒ, `medium/large-v3`ëŠ” ëŠë¦¬ì§€ë§Œ ì •í™•ë„ ë†’ìŒ.
        - **DeepSeek ì‚¬ê³ **: ì²´í¬ í•´ì œ ì‹œ `<think>...</think>`ëŠ” ì œê±°ë˜ê³  ê²°ê³¼ë§Œ ë³´ì—¬ì¤ë‹ˆë‹¤.
        """
    )

# ===============================
# CLI ì‹¤í–‰(ì˜µì…˜)
# ===============================

def cli_demo_example() -> bool:
    """CLI ë‹¨ë°œ ì‹¤í–‰ ì˜ˆì‹œ: íŒŒì¼ ê²½ë¡œë§Œ ì£¼ê³  ë°”ë¡œ ì „ì‚¬"""
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--cli", type=str, help="ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ(ì§€ì • ì‹œ CLI ëª¨ë“œë¡œ 1íšŒ ì‹¤í–‰)")
    parser.add_argument("--lang", type=str, default="ìë™ ê°ì§€ (Auto)")
    parser.add_argument("--mode", type=str, choices=["transcribe", "translate"], default="transcribe")
    parser.add_argument("--model", type=str, default=DEFAULT_ASR_MODEL)
    parser.add_argument("--hint", type=str, default="")
    parser.add_argument("--no-post", action="store_true", help="LLM í›„ì²˜ë¦¬ ë¹„í™œì„±í™”")
    parser.add_argument("--temp", type=float, default=0.2)
    parser.add_argument("--show-think", action="store_true")
    args = parser.parse_args()

    if not args.cli:
        return False

    raw, cleaned, msg, meta = run_pipeline(
        audio_file=args.cli,
        asr_model_size=args.model,
        mode=args.mode,
        ui_lang_label=args.lang,
        hint=args.hint,
        do_postprocess=(not args.no_post),
        sys_prompt=DEFAULT_SYSTEM_PROMPT,
        temperature=args.temp,
        show_reasoning=args.show_think,
    )
    print("=== RAW ===\n" + (raw or ""))
    print("\n=== CLEANED ===\n" + (cleaned or ""))
    print("\n=== MSG/THINK ===\n" + (msg or ""))
    print("\n=== META ===\n" + str(meta))
    return True


if __name__ == "__main__":
    # CLI ë‹¨ë°œ ì‹¤í–‰ì´ ì•„ë‹ˆë©´ GUI ì‹¤í–‰
    if not cli_demo_example():
        demo.launch()



