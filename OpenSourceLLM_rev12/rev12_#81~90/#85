#85
"""
OpenCode ë³€í™˜ë³¸: OpenAI ì „ìš© ì½”ë“œ(í™˜ê²½ë³€ìˆ˜Â·API KeyÂ·OpenAI í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©)ë¥¼
ì˜¤í”ˆì†ŒìŠ¤ LLM(DeepSeek-R1 @ Ollama) ê¸°ë°˜, API Key ë¶ˆí•„ìš”í•œ ë¡œì»¬ ì‹¤í–‰ + Gradio GUIë¡œ ë³€í™˜í•œ ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.

âœ… ì „ì œ
- ë¡œì»¬ PCì— Ollamaê°€ ì„¤ì¹˜ë˜ì–´ ìˆê³  deepseek-r1 ëª¨ë¸ì„ ë°›ì•„ë‘ì—ˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
  ì„¤ì¹˜ ì˜ˆ) https://ollama.com  /  ëª¨ë¸ ë°›ê¸°: `ollama pull deepseek-r1`
- ì¸í„°ë„·/í´ë¼ìš°ë“œ í•„ìš” ì—†ìŒ. API Keyë„ í•„ìš” ì—†ìŒ.

ğŸ“¦ íŒŒì´ì¬ íŒ¨í‚¤ì§€
- gradio, ollama
  ì„¤ì¹˜:  pip install gradio ollama
"""

import re
import time
from typing import List, Tuple, Generator, Optional

import gradio as gr
import ollama  # ë¡œì»¬ Ollama ì„œë²„(ê¸°ë³¸ http://localhost:11434)ì— ì—°ë™


# ==============================
# DeepSeek-R1ì˜ ë‚´ë¶€ ì¶”ë¡ (<think>...</think>) ìˆ¨ê¹€ ìœ í‹¸
# ==============================
THINK_TAG_PATTERN = re.compile(r"<think>.*?</think>", re.DOTALL | re.IGNORECASE)

def strip_think(text: str) -> str:
    """
    DeepSeek-R1ì€ ë‚´ë¶€ ì¶”ë¡ ì„ <think>...</think>ìœ¼ë¡œ ì¶œë ¥í•˜ê¸°ë„ í•©ë‹ˆë‹¤.
    ì‚¬ìš©ìê°€ ë³´ì§€ ì•Šë„ë¡ í•´ë‹¹ ë¸”ë¡ì„ ì œê±°í•©ë‹ˆë‹¤.
    """
    return THINK_TAG_PATTERN.sub("", text).strip()


# ==============================
# ë‹¨ì¼ ìš”ì²­(ë¹„ìŠ¤íŠ¸ë¦¬ë°) â€” OpenAI chat.completions.create ëŒ€ì²´
# ==============================
def local_chat_complete(
    prompt: str,
    model: str = "deepseek-r1",
    max_tokens: int = 256,
    temperature: float = 0.7,
    system: Optional[str] = None,
) -> str:
    """
    OpenAIì˜ `client.chat.completions.create(...)`ì™€ ìœ ì‚¬í•œ ì‚¬ìš©ê°ì„
    ë¡œì»¬ Ollama APIë¡œ ì¬í˜„í•©ë‹ˆë‹¤(ë‹¨ìˆœí™”).
    - messages: (system) + user 1í„´ë§Œ ì²˜ë¦¬(í•„ìš”í•˜ë©´ historyë¡œ í™•ì¥ ê°€ëŠ¥)
    """
    messages = []
    if system:
        messages.append({"role": "system", "content": system})
    messages.append({"role": "user", "content": prompt})

    resp = ollama.chat(
        model=model,
        messages=messages,
        options={
            "num_predict": int(max_tokens),   # ìµœëŒ€ ìƒì„± í† í°
            "temperature": float(temperature) # ì°½ì˜ì„±/ëœë¤ì„±
        }
    )
    content = resp.get("message", {}).get("content", "")
    return strip_think(content)


# ==============================
# ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… â€” Gradio ChatInterfaceìš©
# ==============================
def chat_stream(
    history: List[Tuple[str, str]],
    message: str,
    model: str = "deepseek-r1",
    temperature: float = 0.7,
    max_tokens: int = 256,
    system: Optional[str] = None,
) -> Generator[Tuple[List[Tuple[str, str]], str], None, None]:
    """
    Gradio ChatInterfaceì™€ í˜¸í™˜ë˜ëŠ” ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ í•¨ìˆ˜.
    - Ollamaì˜ stream=Trueë¥¼ ì‚¬ìš©í•˜ì—¬ í† í° ë‹¨ìœ„ë¡œ í˜ë ¤ë³´ëƒ…ë‹ˆë‹¤.
    - <think> ë¸”ë¡ì€ ë§¤ í† í° ëˆ„ì  ë²„í¼ì—ì„œ ì œê±°í•´ ê°€ê³µ í›„ ë…¸ì¶œí•©ë‹ˆë‹¤.
    """
    # ëŒ€í™” ê¸°ë¡ì— ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    history = history + [(message, "")]

    messages = []
    if system:
        messages.append({"role": "system", "content": system})
    messages.append({"role": "user", "content": message})

    stream = ollama.chat(
        model=model,
        messages=messages,
        stream=True,
        options={
            "num_predict": int(max_tokens),
            "temperature": float(temperature),
        },
    )

    buffer = ""   # ëª¨ë¸ ì›ë³¸ ëˆ„ì 
    visible = ""  # ì‚¬ìš©ì ë…¸ì¶œìš©(think ì œê±°) ëˆ„ì 
    for chunk in stream:
        delta = chunk.get("message", {}).get("content", "")
        if not delta:
            continue
        buffer += delta

        # ë§¤ ìŠ¤í…ë§ˆë‹¤ ë‚´ë¶€ ì¶”ë¡  ì œê±°
        clean = strip_think(buffer)
        # ì¦ë¶„ë§Œ ë°˜ì˜í•˜ì—¬ ê¹œë¹¡ì„ ê°ì†Œ
        inc = clean[len(visible):]
        if inc:
            visible += inc
            history[-1] = (message, visible)
            yield history, None
        time.sleep(0.01)  # ê³¼ë„í•œ ì—…ë°ì´íŠ¸ ì†ë„ ì™„í™”

    # ì¢…ë£Œ ì‹œ ìµœì¢… ì •ë¦¬
    history[-1] = (message, strip_think(buffer))
    yield history, None


# ==============================
# Gradio UI â€” OpenAI ì˜ˆì œì˜ â€œì‹œ ìƒì„±â€ ë°ëª¨ í¬í•¨
# ==============================
def build_app():
    with gr.Blocks(title="DeepSeek-R1 (Local Â· Ollama)") as demo:
        gr.Markdown(
            """
            # ğŸ§  DeepSeek-R1 (ë¡œì»¬ Â· Ollama)
            - OpenAI API Key ì—†ì´ ì™„ì „ ë¡œì»¬ë¡œ ë™ì‘í•©ë‹ˆë‹¤.
            - ë‚´ë¶€ ì¶”ë¡ (`<think>...</think>`)ì€ ìë™ìœ¼ë¡œ ìˆ¨ê¹ë‹ˆë‹¤.
            """
        )

        with gr.Tab("ë‹¨ì¼ ìš”ì²­(ì‹œ ìƒì„± ë°ëª¨)"):
            gr.Markdown(
                "> OpenAI ì˜ˆì œì˜ `client.chat.completions.create(model='gpt-4o', ... )`ë¥¼\n"
                "> ë¡œì»¬ LLM í˜¸ì¶œë¡œ ë°”ê¾¼ ê°„ë‹¨ í¼ì…ë‹ˆë‹¤."
            )
            with gr.Row():
                sys = gr.Textbox(
                    label="System ë©”ì‹œì§€(ì„ íƒ)",
                    placeholder="ì–´ì‹œìŠ¤í„´íŠ¸ì˜ ë§íˆ¬/ì—­í• ì„ ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                    value="ë‹¹ì‹ ì€ ê°„ê²°í•˜ê³  ì°½ì˜ì ì¸ ì‘ì‹œ(è©©) ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."
                )
            with gr.Row():
                prompt = gr.Textbox(
                    label="User ë©”ì‹œì§€",
                    lines=4,
                    value="AIì— ëŒ€í•œ ì‹œë¥¼ í•˜ë‚˜ ì‘ì„±í•´ì¤˜"
                )
            with gr.Row():
                model = gr.Textbox(value="deepseek-r1", label="ëª¨ë¸(ë¡œì»¬ Ollama íƒœê·¸)", interactive=True)
                max_tokens = gr.Slider(32, 1024, value=256, step=1, label="ìµœëŒ€ ìƒì„± í† í°")
                temperature = gr.Slider(0.0, 1.5, value=0.7, step=0.05, label="ì˜¨ë„(ì°½ì˜ì„±)")

            run_btn = gr.Button("ìƒì„±í•˜ê¸°", variant="primary")
            out = gr.Textbox(label="ì‘ë‹µ", lines=10, show_copy_button=True)

            def on_run(system_text, user_text, model_tag, max_t, temp):
                return local_chat_complete(
                    prompt=user_text,
                    model=model_tag,
                    max_tokens=int(max_t),
                    temperature=float(temp),
                    system=system_text.strip() or None,
                )

            run_btn.click(on_run, inputs=[sys, prompt, model, max_tokens, temperature], outputs=[out])

        with gr.Tab("ëŒ€í™”í˜• ì±„íŒ…(ìŠ¤íŠ¸ë¦¬ë°)"):
            with gr.Row():
                model2 = gr.Textbox(value="deepseek-r1", label="ëª¨ë¸(ë¡œì»¬ Ollama íƒœê·¸)", interactive=True)
                temperature2 = gr.Slider(0.0, 1.5, value=0.7, step=0.05, label="ì˜¨ë„")
                max_tokens2 = gr.Slider(16, 2048, value=256, step=1, label="ìµœëŒ€ ìƒì„± í† í°")
                sys2 = gr.Textbox(
                    label="System ë©”ì‹œì§€(ì„ íƒ)",
                    value="ë‹¹ì‹ ì€ ìœ ìš©í•˜ê³  ì •ì¤‘í•œ í•œêµ­ì–´ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."
                )

            chat = gr.ChatInterface(
                fn=lambda history, message, m, t, mt, s: chat_stream(
                    history, message, model=m, temperature=t, max_tokens=mt, system=s
                ),
                additional_inputs=[model2, temperature2, max_tokens2, sys2],
                title="ë¡œì»¬ LLM ì±„íŒ…",
                chatbot=gr.Chatbot(height=420, likeable=True, show_copy_button=True),
                textbox=gr.Textbox(placeholder="ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”...", lines=2),
                cache_examples=False,
                clear_btn="ëŒ€í™” ì§€ìš°ê¸°",
                retry_btn=None,
                undo_btn=None,
            )

        gr.Markdown(
            """
            ### ì‚¬ìš© íŒ
            - ëª¨ë¸ ì—†ìœ¼ë©´ ë¨¼ì €: `ollama pull deepseek-r1`
            - ë‹µë³€ì´ ì§§ìœ¼ë©´ **ìµœëŒ€ ìƒì„± í† í°**ì„ ì˜¬ë¦¬ì„¸ìš”.
            - ë„ˆë¬´ ì‚°ë§Œí•˜ë©´ **ì˜¨ë„**ë¥¼ ë‚®ì¶°ë³´ì„¸ìš”(ì˜ˆ: 0.2~0.5).
            """
        )
    return demo


# ==============================
# ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
# ==============================
if __name__ == "__main__":
    """
    ğŸ”„ ì›ë³¸(OpenAI) ëŒ€ë¹„ ë§µí•‘
    - openai.OpenAI(api_key=...)                      ->  ë¶ˆí•„ìš”(ë¡œì»¬ ì‹¤í–‰)
    - client.chat.completions.create(model="gpt-4o")  ->  ollama.chat(model="deepseek-r1", ...)
    - messages(user="AIì— ëŒ€í•œ ì‹œ...")                 ->  ë™ì¼ ì˜ë¯¸ì˜ prompt ì „ë‹¬
    - completion.choices[0].message.content           ->  í•¨ìˆ˜ ë°˜í™˜ ë¬¸ìì—´(think ì œê±°)
    """
    # ê°„ë‹¨ CLI í…ŒìŠ¤íŠ¸(ì›í•˜ë©´ ì£¼ì„ í•´ì œ)
    # print(local_chat_complete("AIì— ëŒ€í•œ ì‹œë¥¼ í•˜ë‚˜ ì‘ì„±í•´ì¤˜"))

    # Gradio ì‹¤í–‰
    app = build_app()
    app.launch(server_name="0.0.0.0", server_port=7860, show_error=True)


