#82
"""
Gemini íŒŒì¸íŠœë‹ ì˜ˆì œ â†’ ë¡œì»¬ ì˜¤í”ˆì†ŒìŠ¤ LLM íŒŒì¸íŠœë‹(QLoRA) + Gradio ë³€í™˜

â–£ ëª©í‘œ
- API í‚¤ ì—†ì´ 100% ë¡œì»¬ì—ì„œ í•™ìŠµ/ì¶”ë¡ 
- DeepSeek-R1 ê³„ì—´ ê³µê°œ ì²´í¬í¬ì¸íŠ¸(ì˜ˆ: deepseek-r1-distill-qwen-7b)ë¡œ **SFT(ì§€ë„í•™ìŠµ)** ìˆ˜í–‰
- **QLoRA(4bit)** + **PEFT** ë¡œ VRAM ì ˆê°
- í•™ìŠµ ë°ì´í„°ëŠ” CSV(train.csv) ì˜ (text_input, output) 2ì—´ êµ¬ì¡°ë¥¼ ì‚¬ìš©
- í•™ìŠµ í›„ ì–´ëŒ‘í„°(LoRA)ë¡œ ì¶”ë¡  ë° Gradio ë°ëª¨ ì œê³µ

â–£ ì‚¬ì „ ì¤€ë¹„
- Python 3.10+
- GPU ê¶Œì¥(ìµœì†Œ 12~16GB VRAM; CPUë„ ê°€ëŠ¥í•˜ë‚˜ ë§¤ìš° ëŠë¦¼)
- ë¡œì»¬ì— ëª¨ë¸ì„ ë‚´ë ¤ë°›ìœ¼ë©°, ì™¸ë¶€ APIëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ

â–£ ì°¸ê³ 
- OllamaëŠ” í˜„ì¬(2025-09 ê¸°ì¤€) ë‚´ë¶€ ë¯¸ì„¸íŠœë‹ë³´ë‹¤ëŠ” ëª¨ë¸ ì‹¤í–‰/ì„œë¹™ì— ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
  ë³¸ ìŠ¤í¬ë¦½íŠ¸ëŠ” **Transformers+PEFT**ë¡œ ë¡œì»¬ì—ì„œ ë¯¸ì„¸íŠœë‹ì„ ìˆ˜í–‰í•˜ê³ , ì¶”ë¡ ì€ Transformersë¡œ ë°”ë¡œ í•˜ê±°ë‚˜,
  (ì„ íƒ) í•™ìŠµ ê²°ê³¼ë¥¼ ë³‘í•©/ë³€í™˜í•˜ì—¬ Ollamaìš© GGUFë¡œ ë‚´ë³´ë‚´ëŠ” ì ˆì°¨ë¥¼ ì•ˆë‚´í•©ë‹ˆë‹¤.

ì‘ì„±: OpenCode
"""

# =========================
# ì˜ì¡´ì„± ì„¤ì¹˜ (ìµœì´ˆ 1íšŒ í•„ìš”)
# =========================
try:
    import torch, transformers, peft, datasets  # type: ignore
except Exception:
    import sys, subprocess
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", 
        "torch", 
        "transformers>=4.42.0", 
        "peft>=0.11.0", 
        "accelerate>=0.30.0", 
        "bitsandbytes>=0.43.1", 
        "datasets", 
        "trl>=0.9.6", 
        "gradio>=4.44.0", 
        "pandas", 
        "seaborn", 
        "matplotlib"
    ])

import os, json, math, time, random
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from dataclasses import dataclass
from typing import List, Dict, Any

import torch
from datasets import Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
)
from peft import LoraConfig, get_peft_model, PeftModel
from trl import SFTTrainer, SFTConfig
import gradio as gr

# =========================
# ê²½ë¡œ/ëª¨ë¸ ì„¤ì •
# =========================
@dataclass
class TrainConfig:
    base_model: str = "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"  # ê³µê°œ ì²´í¬í¬ì¸íŠ¸ ì˜ˆì‹œ
    output_dir: str = "./sft_outputs"
    run_name: str = f"r1_sft_{int(time.time())}"
    lora_r: int = 16
    lora_alpha: int = 32
    lora_dropout: float = 0.05
    target_modules: List[str] = None  # None â†’ ì¼ë°˜ì ì¸ q_proj,k_proj,v_proj,o_proj ìë™íƒìƒ‰ ì‹œë„
    lr: float = 1e-4
    epochs: int = 3
    per_device_train_batch_size: int = 2
    gradient_accumulation_steps: int = 8
    max_seq_len: int = 1024
    warmup_ratio: float = 0.03
    weight_decay: float = 0.0
    fp16: bool = False
    bf16: bool = True  # ìµœì‹  GPU(AMPERE+) ê¶Œì¥
    use_4bit: bool = True  # QLoRA
    packing: bool = True  # ì—¬ëŸ¬ ìƒ˜í”Œì„ í•œ ì‹œí€€ìŠ¤ë¡œ íŒ¨í‚¹í•´ íš¨ìœ¨â†‘


CFG = TrainConfig()

# =========================
# 1) í•™ìŠµ ë°ì´í„° ë¡œë”© (CSV â†’ Dataset)
# -------------------------
# CSV í¬ë§·: text_input, output (í—¤ë” í¬í•¨)
# Gemini ì˜ˆì œì˜ training_data ë¦¬ìŠ¤íŠ¸ì™€ ë™ì¼ ê°œë…
# =========================

def load_csv_as_dataset(csv_path: str) -> Dataset:
    if not os.path.exists(csv_path):
        raise FileNotFoundError(f"CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}")
    df = pd.read_csv(csv_path)
    # ì»¬ëŸ¼ëª… í‘œì¤€í™”
    if not {"text_input", "output"}.issubset(df.columns):
        # ì²« ë‘ ì—´ì„ ì‚¬ìš©
        cols = list(df.columns)
        df = df.rename(columns={cols[0]: "text_input", cols[1]: "output"})
    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ êµ¬ì„±
    def format_example(row):
        # ì‹¬í”Œí•œ ì¸ìŠ¤íŠ¸ëŸ­ì…˜ í¬ë§·(í•„ìš” ì‹œ ì‚¬ìš©ì ì •ì˜)
        return (
            "### ì§€ì‹œë¬¸:\n" + str(row["text_input"]).strip() + "\n\n" +
            "### ì‘ë‹µ:\n" + str(row["output"]).strip()
        )
    df["text"] = df.apply(format_example, axis=1)
    ds = Dataset.from_pandas(df[["text"]])
    return ds

# =========================
# 2) ëª¨ë¸/í† í¬ë‚˜ì´ì € ë¡œë”© (4bit ì–‘ìí™”)
# =========================

def load_base_and_tokenizer(cfg: TrainConfig):
    bnb_cfg = None
    if cfg.use_4bit:
        bnb_cfg = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_use_double_quant=True,
            bnb_4bit_compute_dtype=torch.bfloat16 if cfg.bf16 else torch.float16,
        )
    tok = AutoTokenizer.from_pretrained(cfg.base_model, use_fast=True)
    # ì¼ë¶€ Qwen ê³„ì—´ì€ pad í† í° ë¯¸ì •ì˜ â†’ eosë¡œ ì±„ì›€
    if tok.pad_token is None:
        tok.pad_token = tok.eos_token
    model = AutoModelForCausalLM.from_pretrained(
        cfg.base_model,
        quantization_config=bnb_cfg,
        torch_dtype=torch.bfloat16 if cfg.bf16 else torch.float16,
        device_map="auto",
    )
    return model, tok

# =========================
# 3) Trainer ì„¤ì • ë° í•™ìŠµ ì‹¤í–‰
# =========================

def train(csv_path: str, cfg: TrainConfig = CFG) -> str:
    ds = load_csv_as_dataset(csv_path)
    model, tok = load_base_and_tokenizer(cfg)

    # LoRA êµ¬ì„±
    lora = LoraConfig(
        r=cfg.lora_r,
        lora_alpha=cfg.lora_alpha,
        lora_dropout=cfg.lora_dropout,
        target_modules=cfg.target_modules,  # None â†’ ìë™
        bias="none",
        task_type="CAUSAL_LM",
    )

    # SFT íŠ¸ë ˆì´ë„ˆ
    sft_cfg = SFTConfig(
        output_dir=os.path.join(cfg.output_dir, cfg.run_name),
        num_train_epochs=cfg.epochs,
        per_device_train_batch_size=cfg.per_device_train_batch_size,
        gradient_accumulation_steps=cfg.gradient_accumulation_steps,
        learning_rate=cfg.lr,
        lr_scheduler_type="cosine",
        warmup_ratio=cfg.warmup_ratio,
        weight_decay=cfg.weight_decay,
        max_seq_length=cfg.max_seq_len,
        bf16=cfg.bf16,
        fp16=cfg.fp16,
        logging_steps=10,
        save_steps=200,
        save_total_limit=3,
        packing=cfg.packing,
        report_to=[]  # wandb ë“± ë¯¸ì‚¬ìš©
    )

    trainer = SFTTrainer(
        model=model,
        tokenizer=tok,
        train_dataset=ds,
        peft_config=lora,
        args=sft_cfg,
        formatting_func=lambda x: x["text"],
        dataset_text_field=None,
    )

    trainer.train()

    # ì–´ëŒ‘í„° ì €ì¥
    out_dir = os.path.join(cfg.output_dir, cfg.run_name)
    trainer.model.save_pretrained(out_dir)
    tok.save_pretrained(out_dir)

    print(f"\n[ì™„ë£Œ] LoRA ì–´ëŒ‘í„° ì €ì¥: {out_dir}")
    return out_dir

# =========================
# 4) í•™ìŠµ ìŠ¤ëƒ…ìƒ·/ë¡œìŠ¤ ê³¡ì„  ì‹œê°í™”(ì„ íƒ)
#   (TRL/SFTTrainerëŠ” ê¸°ë³¸ì ìœ¼ë¡œ state.log_history ì œê³µ)
# =========================

def plot_loss_curve(trainer, save_path: str = None):
    logs = trainer.state.log_history
    df = pd.DataFrame([l for l in logs if "loss" in l])
    if df.empty:
        print("ë¡œê¹…ëœ lossê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None
    plt.figure(figsize=(6,4))
    sns.lineplot(data=df, x=df.index, y="loss")
    plt.title("Training Loss")
    plt.xlabel("Step")
    plt.ylabel("Loss")
    if save_path:
        plt.savefig(save_path, bbox_inches="tight")
        print("ì†ì‹¤ ê³¡ì„  ì €ì¥:", save_path)
    return df

# =========================
# 5) ì¶”ë¡ (ì–´ëŒ‘í„° ë¡œë“œ)
# =========================

def load_for_inference(adapter_dir: str, cfg: TrainConfig = CFG):
    base, tok = load_base_and_tokenizer(cfg)
    model = PeftModel.from_pretrained(base, adapter_dir, device_map="auto")
    model.eval()
    return model, tok

@torch.inference_mode()
def generate(model, tok, prompt: str, max_new_tokens=256, temperature=0.7, top_p=0.9) -> str:
    tpl = (
        "### ì§€ì‹œë¬¸:\n" + prompt.strip() + "\n\n### ì‘ë‹µ:\n"
    )
    inputs = tok(tpl, return_tensors="pt").to(model.device)
    out = model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        do_sample=True,
        temperature=temperature,
        top_p=top_p,
        pad_token_id=tok.eos_token_id,
    )
    text = tok.decode(out[0], skip_special_tokens=True)
    # í”„ë¡¬í”„íŠ¸ ì´í›„ë§Œ ì˜ë¼ ë°˜í™˜
    return text.split("### ì‘ë‹µ:\n", 1)[-1].strip()

# =========================
# 6) Gradio ë°ëª¨
# =========================
with gr.Blocks(title="OpenCode â€¢ ë¡œì»¬ SFT(QLoRA) ë°ëª¨", theme=gr.themes.Soft()) as demo:
    gr.Markdown("""
    # ğŸ§ª OpenCode â€¢ ë¡œì»¬ íŒŒì¸íŠœë‹(QLoRA) + ì¶”ë¡  ë°ëª¨
    - CSV(train.csv)ì˜ `(text_input, output)` í˜ì–´ë¡œ **SFT** ìˆ˜í–‰
    - í•™ìŠµëœ LoRA ì–´ëŒ‘í„°ë¥¼ ë¡œë“œí•´ ì¶”ë¡ 
    - ì™¸ë¶€ APIí‚¤ ë¶ˆí•„ìš” / 100% ë¡œì»¬
    """)

    with gr.Tab("Train"):
        csv_path = gr.Textbox(value="train.csv", label="í•™ìŠµ CSV ê²½ë¡œ(text_input,output)")
        run_btn = gr.Button("í•™ìŠµ ì‹œì‘", variant="primary")
        train_status = gr.Textbox(label="ìƒíƒœ", lines=8)
        adapter_out = gr.Textbox(label="ì–´ëŒ‘í„° ê²½ë¡œ")

        def do_train(path):
            try:
                out = train(path, CFG)
                return "âœ… í•™ìŠµ ì™„ë£Œ", out
            except Exception as e:
                return f"âŒ ì˜¤ë¥˜: {e}", ""

        run_btn.click(do_train, [csv_path], [train_status, adapter_out])

    with gr.Tab("Infer"):
        adapter_dir = gr.Textbox(value="", label="í•™ìŠµëœ ì–´ëŒ‘í„° ë””ë ‰í„°ë¦¬")
        load_btn = gr.Button("ëª¨ë¸ ë¡œë“œ")
        sys_info = gr.Markdown("ìƒíƒœ: ëŒ€ê¸° ì¤‘")
        user_in = gr.Textbox(value="ì˜ˆ) 123ê³¼ 456ì˜ í•©ì„ êµ¬í•˜ê³ , ì„¤ëª…ê¹Œì§€ í•´ì¤˜", lines=3, label="í”„ë¡¬í”„íŠ¸")
        max_new = gr.Slider(32, 1024, value=256, step=32, label="max_new_tokens")
        temp = gr.Slider(0.1, 1.5, value=0.7, step=0.05, label="temperature")
        topp = gr.Slider(0.1, 1.0, value=0.9, step=0.05, label="top_p")
        go_btn = gr.Button("ìƒì„±")
        out_md = gr.Markdown()

        state_model = gr.State(value=None)
        state_tok = gr.State(value=None)

        def do_load(dirpath):
            m, t = load_for_inference(dirpath, CFG)
            return (m, t), "âœ… ë¡œë“œ ì™„ë£Œ"

        load_btn.click(do_load, [adapter_dir], [state_model, state_tok, sys_info])

        def do_gen(m, t, prompt, mx, te, tp):
            if m is None or t is None:
                return "ë¨¼ì € ëª¨ë¸ì„ ë¡œë“œí•˜ì„¸ìš”."
            return generate(m, t, prompt, max_new_tokens=int(mx), temperature=float(te), top_p=float(tp))

        go_btn.click(do_gen, [state_model, state_tok, user_in, max_new, temp, topp], out_md)

    with gr.Tab("Utils"):
        gr.Markdown("""
        ### (ì„ íƒ) Ollamaë¡œ ë‚´ë³´ë‚´ê¸° ê°€ì´ë“œ
        1) ì–´ëŒ‘í„°ë¥¼ **base ëª¨ë¸ì— ë³‘í•©** í›„ ì €ì¥
        2) `llama.cpp` ë³€í™˜ ìŠ¤í¬ë¦½íŠ¸ë¡œ GGUF ìƒì„±
        3) Modelfile ì‘ì„± í›„ `ollama create`ë¡œ ë“±ë¡

        ì•„ë˜ëŠ” 1) ë³‘í•© ì˜ˆì‹œ ì½”ë“œ ìŠ¤ë‹ˆí«ì…ë‹ˆë‹¤.
        """)
        code = gr.Code(language="python", value='''
from peft import PeftModel
from transformers import AutoModelForCausalLM
base = AutoModelForCausalLM.from_pretrained(CFG.base_model, torch_dtype=torch.bfloat16, device_map="auto")
merged = PeftModel.from_pretrained(base, "./sft_outputs/.../", device_map="auto")
merged = merged.merge_and_unload()  # LoRA ë³‘í•©
merged.save_pretrained("./merged_model")
''')
        gr.Markdown("ë³‘í•©ëœ `./merged_model`ì„ GGUFë¡œ ë³€í™˜ í›„ Ollamaì— ë“±ë¡í•˜ë©´ ë¡œì»¬ ì„œë¹™ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.")

if __name__ == "__main__":
    # Gradio ì•± êµ¬ë™
    demo.launch(server_name="0.0.0.0", server_port=7864, share=False)


