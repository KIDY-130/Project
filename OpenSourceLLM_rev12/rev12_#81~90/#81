#81
"""
Gemini Function Calling ì˜ˆì œ âœ ë¡œì»¬ ì˜¤í”ˆì†ŒìŠ¤ LLM(Ollama + DeepSeek-R1)ìš© íˆ´ì½œ ë°ëª¨
- ì™¸ë¶€ API í‚¤ ë¶ˆí•„ìš”, 100% ë¡œì»¬
- Ollama(Chat)ë¡œ ëª¨ë¸ í˜¸ì¶œ, ìš°ë¦¬ê°€ ì§ì ‘ "í•¨ìˆ˜ í˜¸ì¶œ(JSON) â†’ ì‹¤í–‰ â†’ ê²°ê³¼ ë°˜ì˜" íŒŒì´í”„ë¼ì¸ êµ¬í˜„
- Gradio UIë¡œ ëª¨ë“œ(NONE/AUTO/ANY)ì™€ í—ˆìš© í•¨ìˆ˜ ì„ íƒ ì œê³µ

ì‘ì„±: OpenCode
"""

# ========================
# ì˜ì¡´ íŒ¨í‚¤ì§€ ì„¤ì¹˜(í•„ìš” ì‹œ ìë™)
# ========================
try:
    import ollama
except Exception:
    import sys, subprocess
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "ollama"])
    import ollama

try:
    import gradio as gr
except Exception:
    import sys, subprocess
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "gradio>=4.44.0"])  
    import gradio as gr

import json, re, math, time
from typing import Dict, Any, List, Optional, Tuple, Union

OLLAMA_HOST = "http://localhost:11434"
DEFAULT_MODEL = "deepseek-r1"  # í•„ìš” ì‹œ ë°”ê¾¸ì„¸ìš”

# ========================
# 1) ì˜ˆì‹œ ë„êµ¬(í•¨ìˆ˜)
# ========================

def add(a: float, b: float) -> float:
    """returns a + b."""
    return a + b


def subtract(a: float, b: float) -> float:
    """returns a - b."""
    return a - b


def multiply(a: float, b: float) -> float:
    """returns a * b."""
    return a * b


def divide(a: float, b: float) -> float:
    """returns a / b (0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€)."""
    if b == 0:
        raise ValueError("0ìœ¼ë¡œ ë‚˜ëˆŒ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    return a / b


def get_temperature(location: str) -> str:
    """get current temperature (ë”ë¯¸)."""
    if location == "ì„œìš¸":
        return "20ë„"
    elif location == "ë¶€ì‚°":
        return "22ë„"
    else:
        return "10ë„"

# ë ˆì§€ìŠ¤íŠ¸ë¦¬: ì´ë¦„â†’ì‹¤ì œ í•¨ìˆ˜
FUNCTIONS: Dict[str, Any] = {
    "add": add,
    "subtract": subtract,
    "multiply": multiply,
    "divide": divide,
    "get_temperature": get_temperature,
}

# í•¨ìˆ˜ ì‹œê·¸ë‹ˆì²˜ ì„¤ëª…(í”„ë¡¬í”„íŠ¸ìš©)
FUNCTION_SPECS: Dict[str, Dict[str, Any]] = {
    "add": {"description": "returns a + b", "params": {"a": "number", "b": "number"}},
    "subtract": {"description": "returns a - b", "params": {"a": "number", "b": "number"}},
    "multiply": {"description": "returns a * b", "params": {"a": "number", "b": "number"}},
    "divide": {"description": "returns a / b", "params": {"a": "number", "b": "number"}},
    "get_temperature": {"description": "get current temperature (dummy)", "params": {"location": "string"}},
}

# ========================
# 2) ëª¨ë¸ í˜¸ì¶œ ë˜í¼
# ========================

def chat_once(model: str, messages: List[Dict[str, str]], *, json_mode: bool = False, temperature: float = 0.2) -> str:
    """Ollama chat í˜¸ì¶œ. json_mode=Trueë©´ format='json' ì‹œë„ í›„ ì‹¤íŒ¨ ì‹œ ì¼ë°˜ í…ìŠ¤íŠ¸ íŒŒì‹±."""
    client = ollama.Client(host=OLLAMA_HOST)
    options = {"temperature": float(temperature)}
    if json_mode:
        options["format"] = "json"
    try:
        res = client.chat(model=model, messages=messages, options=options)
        return res.get("message", {}).get("content", "")
    except Exception as e:
        return f"[model_error] {e}"

# ========================
# 3) í”„ë¡¬í”„íŠ¸: í•¨ìˆ˜ í˜¸ì¶œ í”„ë¡œí† ì½œ
# ========================

def build_tools_block(allowed: List[str]) -> str:
    lines = []
    for name in allowed:
        spec = FUNCTION_SPECS[name]
        params = ", ".join(f"{k}:{v}" for k, v in spec["params"].items())
        lines.append(f"- {name}({params}) : {spec['description']}")
    return "\n".join(lines)

PROTO_EXAMPLE = {
    "tool_call": {"name": "multiply", "args": {"a": 57, "b": 44}},
    # ë˜ëŠ”
    # "final_answer": "ë¬¸ì¥ìœ¼ë¡œ ëœ ìµœì¢… ë‹µë³€"
}

SYSTEM_BASE = (
    "ë„ˆëŠ” ë„êµ¬ë¥¼ ì„ íƒí•´ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ì¡°ìˆ˜ë‹¤. ë°˜ë“œì‹œ ì•„ë˜ JSON ìŠ¤í‚¤ë§ˆ ì¤‘ í•˜ë‚˜ë¡œë§Œ ì‘ë‹µí•˜ë¼.\n"
    "- ë„êµ¬ê°€ í•„ìš”í•˜ë©´: {\"tool_call\": {\"name\": <string>, \"args\": <object>}}\n"
    "- ë„êµ¬ê°€ í•„ìš” ì—†ìœ¼ë©´: {\"final_answer\": <string>}\n"
    "ì¶”ê°€ í…ìŠ¤íŠ¸/ì„¤ëª…/ì½”ë“œë¸”ë¡ ì—†ì´ JSONë§Œ ì¶œë ¥.\n"
)

# ========================
# 4) íŒŒì„œ: JSON ë˜ëŠ” ì½”ë“œë¸”ë¡ì—ì„œ JSON ì¶”ì¶œ
# ========================

def try_parse_json(text: str) -> Optional[Dict[str, Any]]:
    text = text.strip()
    # ì½”ë“œíœìŠ¤ ì œê±°
    m = re.search(r"```(?:json)?\n(.*?)\n```", text, re.S | re.I)
    if m:
        text = m.group(1).strip()
    try:
        return json.loads(text)
    except Exception:
        # ì¤‘ê´„í˜¸ ìŠ¤ë‹ˆí«ë§Œ ì¶”ì¶œ ì‹œë„
        m2 = re.search(r"\{[\s\S]*\}\s*$", text)
        if m2:
            try:
                return json.loads(m2.group(0))
            except Exception:
                return None
        return None

# ========================
# 5) íˆ´ì½œ ì‹¤í–‰ íŒŒì´í”„ë¼ì¸
# ========================
class ToolMode:
    NONE = "none"   # í•¨ìˆ˜ ì‚¬ìš© ê¸ˆì§€
    AUTO = "auto"   # ëª¨ë¸ì´ ììœ ë¡­ê²Œ ì„ íƒ
    ANY = "any"     # ì¼ë¶€ í•¨ìˆ˜ë§Œ í—ˆìš©


def run_tool_call(user_prompt: str, *, mode: str, allowed_tools: Optional[List[str]] = None, model: str = DEFAULT_MODEL, temperature: float = 0.2) -> Dict[str, Any]:
    """ìœ ì € ì…ë ¥â†’(ì˜µì…˜) í•¨ìˆ˜ í˜¸ì¶œâ†’ìµœì¢… ë‹µë³€ ìƒì„±ê¹Œì§€ ìˆ˜í–‰í•˜ê³  ë””ë²„ê·¸ ì •ë³´ë¥¼ ë°˜í™˜."""
    allowed = []
    if mode == ToolMode.NONE:
        # ë„êµ¬ ì—†ì´ ê³§ë°”ë¡œ ë‹µë³€
        sys_prompt = SYSTEM_BASE + "\nì§€ê¸ˆì€ ë„êµ¬ ì‚¬ìš©ì„ ê¸ˆì§€í•œë‹¤. ë°˜ë“œì‹œ {\"final_answer\": ...} í˜•íƒœë¡œë§Œ ë‹µí•˜ë¼."
    elif mode == ToolMode.ANY:
        allowed = [n for n in (allowed_tools or []) if n in FUNCTIONS]
        if not allowed:
            return {"error": "í—ˆìš© í•¨ìˆ˜ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."}
        sys_prompt = SYSTEM_BASE + "\nì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡:\n" + build_tools_block(allowed)
    else:  # AUTO
        allowed = list(FUNCTIONS.keys())
        sys_prompt = SYSTEM_BASE + "\nì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡:\n" + build_tools_block(allowed)

    messages = [
        {"role": "system", "content": sys_prompt},
        {"role": "user", "content": user_prompt},
    ]

    # 1ë‹¨ê³„: ëª¨ë¸ì—ê²Œ JSON ì˜ì‚¬ê²°ì • ìš”ì²­
    model_raw = chat_once(model, messages, json_mode=True, temperature=temperature)
    parsed = try_parse_json(model_raw) or {}

    debug_steps: List[Dict[str, Any]] = [
        {"step": "model_decision", "raw": model_raw, "parsed": parsed}
    ]

    # 2ë‹¨ê³„: ë„êµ¬ í˜¸ì¶œ ë˜ëŠ” ìµœì¢…ë‹µë³€
    if "tool_call" in parsed:
        call = parsed["tool_call"]
        name = call.get("name")
        args = call.get("args", {}) if isinstance(call, dict) else {}
        if name not in allowed:
            return {"error": f"í—ˆìš©ë˜ì§€ ì•Šì€ í•¨ìˆ˜ í˜¸ì¶œ ì‹œë„: {name}", "debug": debug_steps}
        if name not in FUNCTIONS:
            return {"error": f"ì•Œ ìˆ˜ ì—†ëŠ” í•¨ìˆ˜: {name}", "debug": debug_steps}
        # ì‹¤ì œ í•¨ìˆ˜ ì‹¤í–‰
        try:
            result = FUNCTIONS[name](**args)
        except Exception as e:
            result = f"[tool_error] {e}"
        debug_steps.append({"step": "tool_execute", "name": name, "args": args, "result": result})

        # 3ë‹¨ê³„: í•¨ìˆ˜ ê²°ê³¼ë¥¼ ì£¼ê³  ìµœì¢… ë‹µë³€ ìƒì„±
        follow_messages = messages + [
            {"role": "assistant", "content": json.dumps({"tool_call": {"name": name, "args": args}}, ensure_ascii=False)},
            {"role": "user", "content": f"í•¨ìˆ˜ `{name}` ì˜ ì‹¤í–‰ ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\nRESULT: {result}\nì´ ê²°ê³¼ë¥¼ ì´ìš©í•´ ì§ˆë¬¸ì— ìµœì¢… ë‹µì„ í•œêµ­ì–´ë¡œ ê°„ê²°íˆ ì‘ì„±í•˜ë¼."},
        ]
        final_text = chat_once(model, follow_messages, json_mode=False, temperature=temperature)
        debug_steps.append({"step": "final_answer", "text": final_text})
        return {"answer": final_text, "debug": debug_steps}

    elif "final_answer" in parsed:
        return {"answer": parsed.get("final_answer", ""), "debug": debug_steps}

    else:
        # ëª¨ë¸ì´ JSON ê·œì•½ì„ ì–´ê¸´ ê²½ìš°: ì›ë¬¸ ê·¸ëŒ€ë¡œ ë°˜í™˜
        return {"answer": model_raw, "debug": debug_steps, "warning": "JSON íŒŒì‹± ì‹¤íŒ¨ ë˜ëŠ” í”„ë¡œí† ì½œ ìœ„ë°˜"}

# ========================
# 6) Gradio UI
# ========================
with gr.Blocks(title="OpenCode â€¢ ë¡œì»¬ íˆ´ì½œ(Function Calling)") as demo:
    gr.Markdown("""
    # ğŸ› ï¸ OpenCode â€¢ ë¡œì»¬ íˆ´ì½œ(Function Calling) ë°ëª¨
    - ëª¨ë¸: **DeepSeek-R1** (Ollama)
    - ëª¨ë“œ: `none`(í•¨ìˆ˜ ê¸ˆì§€) / `auto`(ì „ì²´ í—ˆìš©) / `any`(ì¼ë¶€ í—ˆìš©)
    - JSON í”„ë¡œí† ì½œë¡œ í•¨ìˆ˜ ì„ íƒ â†’ ì‹¤ì œ íŒŒì´ì¬ í•¨ìˆ˜ ì‹¤í–‰ â†’ ê²°ê³¼ ë°˜ì˜
    """)

    with gr.Row():
        mode = gr.Radio(["none", "auto", "any"], value="auto", label="Function Mode")
        allowed = gr.CheckboxGroup(list(FUNCTIONS.keys()), value=["multiply", "get_temperature"], label="í—ˆìš© í•¨ìˆ˜(any ëª¨ë“œì—ì„œ ì‚¬ìš©)")
        model_name = gr.Textbox(value=DEFAULT_MODEL, label="ëª¨ë¸ëª…(Ollama)")
        temp = gr.Slider(0.0, 1.5, value=0.2, step=0.05, label="temperature")

    with gr.Tabs():
        with gr.Tab("Quick Math"):
            q1 = gr.Textbox(value="ì €ëŠ” 57ë§ˆë¦¬ì˜ ê³ ì–‘ì´ë¥¼ í‚¤ìš°ê³  ìˆê³ , ê° 44ê°œì˜ ì†ì‹¸ê°œë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ì†ì‹¸ê°œëŠ” ì´ ëª‡ ê°œ ì¼ê¹Œìš”?", lines=2, label="ì§ˆë¬¸")
            run1 = gr.Button("ì‹¤í–‰", variant="primary")
            out1 = gr.Markdown(label="ë‹µë³€")
            dbg1 = gr.JSON(label="ë””ë²„ê·¸")

            def go1(m, alw, mdl, t):
                res = run_tool_call(m, mode=m, allowed_tools=alw, model=mdl, temperature=t)
                return res.get("answer", ""), res

            run1.click(go1, [mode, allowed, model_name, temp], [out1, dbg1])

        with gr.Tab("Weather"):
            q2 = gr.Textbox(value="í˜„ì¬ ì„œìš¸ê³¼ ë¶€ì‚°ì˜ ê¸°ì˜¨ì€?", label="ì§ˆë¬¸")
            gr.Markdown("ì°¸ê³ : ì˜ˆì œ í•¨ìˆ˜ëŠ” ê°œë³„ í˜¸ì¶œë§Œ ì§€ì›í•˜ë¯€ë¡œ ëª¨ë¸ì´ í•œ ë²ˆì— ë‘ ë„ì‹œë¥¼ ì²˜ë¦¬í•˜ë„ë¡ í›„ì† í”„ë¡¬í”„íŠ¸ì—ì„œ í•©ì„±í•©ë‹ˆë‹¤.")
            run2 = gr.Button("ì‹¤í–‰")
            out2 = gr.Markdown()
            dbg2 = gr.JSON()

            def go2(q, m, alw, mdl, t):
                res = run_tool_call(q, mode=m, allowed_tools=alw, model=mdl, temperature=t)
                return res.get("answer", ""), res

            run2.click(go2, [q2, mode, allowed, model_name, temp], [out2, dbg2])

        with gr.Tab("Free Chat + Tools"):
            q3 = gr.Textbox(value="ì•„ë¬´ê±°ë‚˜ ë¬¼ì–´ë³´ì„¸ìš”. í•„ìš” ì‹œ ë„êµ¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.", lines=2, label="ì§ˆë¬¸")
            run3 = gr.Button("ì‹¤í–‰")
            out3 = gr.Markdown()
            dbg3 = gr.JSON()
            run3.click(lambda q, m, alw, mdl, t: (r:=run_tool_call(q, mode=m, allowed_tools=alw, model=mdl, temperature=t)).get("answer", ""), [q3, mode, allowed, model_name, temp], out3)
            run3.click(lambda q, m, alw, mdl, t: run_tool_call(q, mode=m, allowed_tools=alw, model=mdl, temperature=t), [q3, mode, allowed, model_name, temp], dbg3)

    gr.Markdown("""
    ---
    ### ì‚¬ìš© íŒ
    - `none`: ëª¨ë¸ì´ ê³„ì‚°ì„ ìŠ¤ìŠ¤ë¡œ ì„¤ëª…/ë‹µë³€(ë„êµ¬ ì‚¬ìš© ì•ˆ í•¨)
    - `auto`: ë“±ë¡ëœ ëª¨ë“  ë„êµ¬ ì¤‘ì—ì„œ ëª¨ë¸ì´ ì„ íƒ
    - `any`: ì²´í¬ëœ ë„êµ¬ë§Œ ì‚¬ìš© ê°€ëŠ¥(í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸)
    - ëª¨ë¸ì´ JSONì„ ì–´ê¸¸ ìˆ˜ ìˆì–´ ê²¬ê³ í•œ íŒŒì„œ(ì½”ë“œë¸”ë¡/ìŠ¤ë‹ˆí« ì¶”ì¶œ)ë¡œ ë³´ì™„í–ˆìŠµë‹ˆë‹¤.
    - í•„ìš” ì‹œ ì—¬ëŸ¬ í•¨ìˆ˜ í˜¸ì¶œë„ í™•ì¥ ê°€ëŠ¥í•©ë‹ˆë‹¤(í˜„ì¬ ìƒ˜í”Œì€ 1íšŒ í˜¸ì¶œ ê¸°ì¤€).
    """)

if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7863, share=False)


