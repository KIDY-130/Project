#87
"""
OpenCode ë³€í™˜ë³¸
- OpenAI ì „ìš© ì½”ë“œ(gpt-4o, API Key, chat.completions ë“±)ë¥¼
  ì˜¤í”ˆì†ŒìŠ¤ LLM(DeepSeek-R1 @ Ollama)ë¡œ 1:1ì— ê°€ê¹ê²Œ ì¹˜í™˜í•˜ê³ ,
  Gradio GUIë¡œ ì—¬ëŸ¬ ì˜ˆì œë¥¼ í•œ ë²ˆì— ì‹¤í–‰í•  ìˆ˜ ìˆê²Œ êµ¬ì„±í–ˆìŠµë‹ˆë‹¤.

âœ… ì™„ì „ ë¡œì»¬: API Key ë¶ˆí•„ìš” / .env ë¶ˆí•„ìš” / ì¸í„°ë„· ë¶ˆí•„ìš”
âœ… ëª¨ë¸: deepseek-r1 (ë³€ê²½ ê°€ëŠ¥, ë‹¤ë¥¸ ë¡œì»¬ íƒœê·¸ ì‚¬ìš© ê°€ëŠ¥)
âœ… ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥, n(ë‹¤ì¤‘ í›„ë³´), stop ì‹œí€€ìŠ¤, temperature/top_p ë¹„êµ
âœ… (ìœ ì‚¬) í˜ë„í‹° ì‹¤í—˜: Ollamaì˜ repeat_penaltyë¡œ ê·¼ì‚¬(ì£¼ì˜ì‚¬í•­ ì•„ë˜ ì°¸ì¡°)
âœ… ì•ˆì „ ì˜µì…˜: ì…¸ ëª…ë ¹ ì‹¤í–‰ íƒ­ì€ ê¸°ë³¸ â€œì‹¤í–‰ ê¸ˆì§€â€ ìƒíƒœ(ë¡œì»¬ì—ì„œ ë³¸ì¸ ì±…ì„ìœ¼ë¡œë§Œ)

ì‚¬ì „ ì¤€ë¹„
1) Ollama ì„¤ì¹˜: https://ollama.com
2) ëª¨ë¸ ë‹¤ìš´ë¡œë“œ(ìµœì´ˆ 1íšŒ):  `ollama pull deepseek-r1`
3) íŒŒì´ì¬ íŒ¨í‚¤ì§€:             `pip install gradio ollama`

ì¤‘ìš” ì£¼ì˜
- DeepSeek-R1ì€ ë‚´ë¶€ ì¶”ë¡ ì„ <think>...</think>ë¡œ ì¶œë ¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  ë³¸ ì•±ì€ í•´ë‹¹ ë¸”ë¡ì„ ìë™ ì œê±°í•˜ì—¬ ì‚¬ìš©ìì—ê² ìˆ¨ê¹ë‹ˆë‹¤.
- OpenAIì˜ frequency_penalty / presence_penaltyëŠ” Ollamaì—ì„œ ë™ì¼í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
  ì—¬ê¸°ì„œëŠ” repeat_penaltyë¥¼ ê°€ë³€ ì ìš©í•˜ì—¬ â€œë°˜ë³µ ì–µì œâ€ë¥¼ ê·¼ì‚¬í•©ë‹ˆë‹¤.
  (ëª¨ë¸/ë²„ì „ì— ë”°ë¼ ë¬´ì‹œë˜ê±°ë‚˜ ì²´ê°ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.)
"""

from __future__ import annotations
import re
import time
import json
import subprocess
from typing import List, Dict, Any, Generator, Optional, Tuple

import gradio as gr
import ollama


# ==============================
# ê³µí†µ ìœ í‹¸
# ==============================
_THINK_TAG = re.compile(r"<think>.*?</think>", re.DOTALL | re.IGNORECASE)

def strip_think(text: str) -> str:
    """DeepSeek-R1ì´ ì¶œë ¥í•˜ëŠ” ë‚´ë¶€ ì¶”ë¡ (<think>...</think>)ì„ ìˆ¨ê¹ë‹ˆë‹¤."""
    return _THINK_TAG.sub("", text).strip()

def ollama_complete(
    messages: List[Dict[str, str]],
    model: str = "deepseek-r1",
    max_tokens: int = 256,
    temperature: float = 0.7,
    top_p: Optional[float] = None,
    stop: Optional[List[str] | str] = None,
    repeat_penalty: Optional[float] = None,
) -> Dict[str, Any]:
    """
    OpenAI chat.completions.create() ëŠë‚Œì˜ ë‹¨ë°œ í˜¸ì¶œ.
    - Ollama options ì£¼ìš” í‚¤: num_predict, temperature, top_p, stop, repeat_penalty
    """
    opts: Dict[str, Any] = {
        "num_predict": int(max_tokens),
        "temperature": float(temperature),
    }
    if top_p is not None:
        opts["top_p"] = float(top_p)
    if repeat_penalty is not None:
        opts["repeat_penalty"] = float(repeat_penalty)
    if stop:
        opts["stop"] = stop

    resp = ollama.chat(model=model, messages=messages, options=opts)
    content = strip_think(resp.get("message", {}).get("content", ""))
    usage = {
        "prompt_tokens": resp.get("prompt_eval_count"),
        "completion_tokens": resp.get("eval_count"),
        "total_tokens": (resp.get("prompt_eval_count") or 0) + (resp.get("eval_count") or 0),
        "total_duration_ns": resp.get("total_duration"),
    }
    return {
        "message": {"role": "assistant", "content": content},
        "usage": usage,
        "model": resp.get("model", model),
    }

def ollama_stream(
    messages: List[Dict[str, str]],
    model: str = "deepseek-r1",
    max_tokens: int = 256,
    temperature: float = 0.7,
    top_p: Optional[float] = None,
    stop: Optional[List[str] | str] = None,
    repeat_penalty: Optional[float] = None,
) -> Generator[str, None, None]:
    """ìŠ¤íŠ¸ë¦¬ë° ìƒì„±ê¸°: ëˆ„ì (clean) í…ìŠ¤íŠ¸ë¥¼ ê³„ì† ë‚´ë³´ëƒ…ë‹ˆë‹¤."""
    opts: Dict[str, Any] = {
        "num_predict": int(max_tokens),
        "temperature": float(temperature),
    }
    if top_p is not None:
        opts["top_p"] = float(top_p)
    if repeat_penalty is not None:
        opts["repeat_penalty"] = float(repeat_penalty)
    if stop:
        opts["stop"] = stop

    stream = ollama.chat(model=model, messages=messages, stream=True, options=opts)
    buffer = ""
    for chunk in stream:
        delta = chunk.get("message", {}).get("content", "")
        if not delta:
            continue
        buffer += delta
        clean = strip_think(buffer)
        yield clean


# ==============================
# Gradio ì•±
# ==============================
def build_app():
    with gr.Blocks(title="OpenCode Â· DeepSeek-R1 (Local Â· Ollama)") as demo:
        gr.Markdown(
            """
            # ğŸ§  OpenCode Â· DeepSeek-R1 (Local Â· Ollama)
            OpenAI ì˜ˆì œë¥¼ ë¡œì»¬ ì˜¤í”ˆì†ŒìŠ¤ LLMìœ¼ë¡œ ë³€í™˜í•œ ë©€í‹° ë°ëª¨ì…ë‹ˆë‹¤.  
            **API Key ë¶ˆí•„ìš” Â· .env ë¶ˆí•„ìš” Â· ë‚´ë¶€ ì¶”ë¡ (`<think>`) ìë™ ìˆ¨ê¹€**
            """
        )

        with gr.Row():
            model_tag = gr.Textbox(value="deepseek-r1", label="ëª¨ë¸(ë¡œì»¬ íƒœê·¸)", interactive=True)
            temperature = gr.Slider(0.0, 1.5, value=0.7, step=0.05, label="temperature")
            top_p = gr.Slider(0.0, 1.0, value=1.0, step=0.05, label="top_p")
            max_tokens = gr.Slider(16, 2048, value=256, step=1, label="max_tokens")

        # ============ íƒ­ 1: ê¸°ë³¸ ëŒ€í™” ============
        with gr.Tab("ê¸°ë³¸ ëŒ€í™”"):
            sys = gr.Textbox(label="system", value="ë‹¹ì‹ ì€ ì´ì•¼ê¸°ê¾¼ì…ë‹ˆë‹¤.")
            usr = gr.Textbox(label="user", value="ì˜›ë‚  ì˜›ì ì— ")
            stops = gr.Textbox(label="stop (ì‰¼í‘œ êµ¬ë¶„)", value="\\n")
            run = gr.Button("ìƒì„±", variant="primary")
            out = gr.Textbox(label="ì‘ë‹µ", lines=8, show_copy_button=True)
            usage_box = gr.JSON(label="usage")

            def on_basic(m, t, tp, mt, s, u, stops_csv):
                stop_list = [x.strip() for x in stops_csv.split(",") if x.strip()]
                resp = ollama_complete(
                    messages=[{"role": "system", "content": s},
                              {"role": "user", "content": u}],
                    model=m, temperature=t, top_p=tp, max_tokens=int(mt),
                    stop=stop_list or None,
                )
                return resp["message"]["content"], resp["usage"]

            run.click(on_basic, [model_tag, temperature, top_p, max_tokens, sys, usr, stops], [out, usage_box])

        # ============ íƒ­ 2: temperature ë¹„êµ ============
        with gr.Tab("temperature ë¹„êµ"):
            prefix = gr.Textbox(value="ì˜›ë‚  ì˜›ì ì— ", label="í”„ë¡¬í”„íŠ¸(prefix)")
            run_temp = gr.Button("ìƒì„±")
            high = gr.Textbox(label="ë†’ìŒ (2.0)", lines=5, show_copy_button=True)
            mid = gr.Textbox(label="ì¤‘ê°„ (1.0)", lines=5, show_copy_button=True)
            low = gr.Textbox(label="ë‚®ìŒ (0.0)", lines=5, show_copy_button=True)

            def on_temp(m, tp, mt, pfx):
                def gen(temp):
                    r = ollama_complete(
                        messages=[{"role": "system", "content": "ë‹¹ì‹ ì€ ì´ì•¼ê¸°ê¾¼ì…ë‹ˆë‹¤."},
                                  {"role": "user", "content": pfx}],
                        model=m, temperature=temp, top_p=tp, max_tokens=int(mt),
                        stop=["\n"]
                    )["message"]["content"]
                    return pfx + r
                return gen(2.0), gen(1.0), gen(0.0)
            run_temp.click(on_temp, [model_tag, top_p, max_tokens, prefix], [high, mid, low])

        # ============ íƒ­ 3: top_p ë¹„êµ ============
        with gr.Tab("top_p ë¹„êµ"):
            prefix2 = gr.Textbox(value="ì˜›ë‚  ì˜›ì ì— ", label="í”„ë¡¬í”„íŠ¸(prefix)")
            run_topp = gr.Button("ìƒì„±")
            topp1 = gr.Textbox(label="top_p=1.0", lines=5, show_copy_button=True)
            topp05 = gr.Textbox(label="top_p=0.5", lines=5, show_copy_button=True)
            topp01 = gr.Textbox(label="top_p=0.1", lines=5, show_copy_button=True)

            def on_topp(m, t, mt, pfx):
                def gen(tp):
                    r = ollama_complete(
                        messages=[{"role": "system", "content": "ë‹¹ì‹ ì€ ì´ì•¼ê¸°ê¾¼ì…ë‹ˆë‹¤."},
                                  {"role": "user", "content": pfx}],
                        model=m, temperature=t, top_p=tp, max_tokens=int(mt),
                        stop=["\n"]
                    )["message"]["content"]
                    return pfx + r
                return gen(1.0), gen(0.5), gen(0.1)
            run_topp.click(on_topp, [model_tag, temperature, max_tokens, prefix2], [topp1, topp05, topp01])

        # ============ íƒ­ 4: (ìœ ì‚¬) í˜ë„í‹° ============
        with gr.Tab("ë°˜ë³µ í˜ë„í‹°(ê·¼ì‚¬)"):
            gr.Markdown("OpenAIì˜ frequency/presence_penalty ëŒ€ì‹  **repeat_penalty**ë¥¼ ê°€ë³€ ì ìš©í•´ ë°˜ë³µì„ ì–µì œí•©ë‹ˆë‹¤.")
            pfx3 = gr.Textbox(value="ì˜›ë‚  ì˜›ì ì— ", label="í”„ë¡¬í”„íŠ¸(prefix)")
            run_pen = gr.Button("ìƒì„±")
            high_rep = gr.Textbox(label="repeat_penalty=1.8 (ê°•í•œ ì–µì œ)", lines=5, show_copy_button=True)
            low_rep = gr.Textbox(label="repeat_penalty=1.0 (ê¸°ë³¸)", lines=5, show_copy_button=True)

            def on_pen(m, t, tp, mt, pfx):
                def gen(rp):
                    r = ollama_complete(
                        messages=[{"role":"system","content":"ë‹¹ì‹ ì€ ì´ì•¼ê¸°ê¾¼ì…ë‹ˆë‹¤."},
                                  {"role":"user","content":pfx}],
                        model=m, temperature=t, top_p=tp, max_tokens=int(mt),
                        repeat_penalty=rp
                    )["message"]["content"]
                    return pfx + r
                return gen(1.8), gen(1.0)
            run_pen.click(on_pen, [model_tag, temperature, top_p, max_tokens, pfx3], [high_rep, low_rep])

        # ============ íƒ­ 5: nê°œ í›„ë³´ ============
        with gr.Tab("ë‹¤ì¤‘ í›„ë³´(n)"):
            gr.Markdown("í•œ í”„ë¡¬í”„íŠ¸ì— ëŒ€í•´ ì—¬ëŸ¬ í›„ë³´ë¥¼ ì—°ì† í˜¸ì¶œë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
            pfx4 = gr.Textbox(value="ì˜›ë‚  ì˜›ì ì— ", label="í”„ë¡¬í”„íŠ¸(prefix)")
            n_choices = gr.Slider(1, 6, value=2, step=1, label="n")
            run_n = gr.Button("ìƒì„±")
            out_n = gr.Textbox(label="í›„ë³´ë“¤", lines=12, show_copy_button=True)

            def on_n(m, t, tp, mt, pfx, n):
                results = []
                for i in range(int(n)):
                    r = ollama_complete(
                        messages=[{"role":"system","content":"ë‹¹ì‹ ì€ ì´ì•¼ê¸°ê¾¼ì…ë‹ˆë‹¤."},
                                  {"role":"user","content":pfx}],
                        model=m, temperature=t, top_p=tp, max_tokens=int(mt), stop=["\n"]
                    )["message"]["content"]
                    results.append(f"Choice {i}:\n{pfx}{r}")
                return "\n\n".join(results)

            run_n.click(on_n, [model_tag, temperature, top_p, max_tokens, pfx4, n_choices], out_n)

        # ============ íƒ­ 6: ìŠ¤íŠ¸ë¦¬ë° ============
        with gr.Tab("ìŠ¤íŠ¸ë¦¬ë°"):
            pfx5 = gr.Textbox(value="ì˜›ë‚  ì˜›ì ì— ", label="í”„ë¡¬í”„íŠ¸(prefix)")
            btn_stream = gr.Button("ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘")
            stream_box = gr.Textbox(label="ì‹¤ì‹œê°„ ì¶œë ¥", lines=10)

            def do_stream(m, t, tp, mt, pfx):
                stream_box.value = pfx
                msgs = [
                    {"role":"system","content":"ë‹¹ì‹ ì€ ì´ì•¼ê¸°ê¾¼ì…ë‹ˆë‹¤."},
                    {"role":"user","content":pfx}
                ]
                buffer = pfx
                for clean in ollama_stream(msgs, m, int(mt), float(t), float(tp), stop=["\n"]):
                    inc = clean
                    stream_box.value = inc  # ì „ì²´ ëˆ„ì ì„ ê·¸ëŒ€ë¡œ ë³´ì—¬ì¤Œ
                    yield gr.update(value=inc)
                    time.sleep(0.01)

            btn_stream.click(do_stream, [model_tag, temperature, top_p, max_tokens, pfx5], [stream_box])

        # ============ íƒ­ 7: í‚¤ì›Œë“œ/í•´ì‹œíƒœê·¸ íŒŒì´í”„ë¼ì¸ ============
        with gr.Tab("í‚¤ì›Œë“œ/í•´ì‹œíƒœê·¸ íŒŒì´í”„ë¼ì¸"):
            text_kw = gr.Textbox(label="ì…ë ¥ í…ìŠ¤íŠ¸", lines=6, value="í•œ ê¸°ë°œí•œ ë§ˆì„ì—ì„œ...")
            btn_kw = gr.Button("í‚¤ì›Œë“œ ì¶”ì¶œ â†’ í•´ì‹œíƒœê·¸ ìƒì„±")
            kw_out = gr.Textbox(label="í‚¤ì›Œë“œ(ëª¨ë¸ ì‘ë‹µ)", lines=6, show_copy_button=True)
            hash_out = gr.Textbox(label="í•´ì‹œíƒœê·¸ ì œì•ˆ", lines=3, show_copy_button=True)

            def on_kw(m, t, tp, mt, txt):
                # 1) í‚¤ì›Œë“œ
                kw = ollama_complete(
                    messages=[
                        {"role":"system","content":"ì¤‘ìš” í‚¤ì›Œë“œë¥¼ JSON ë°°ì—´ë¡œ ì¶œë ¥í•˜ì„¸ìš”."},
                        {"role":"user","content":txt}
                    ],
                    model=m, temperature=0.1, top_p=tp, max_tokens=200
                )["message"]["content"]
                # 2) í•´ì‹œíƒœê·¸
                hs = ollama_complete(
                    messages=[
                        {"role":"system","content":"ì£¼ì–´ì§„ í‚¤ì›Œë“œì—ì„œ í•´ì‹œíƒœê·¸ 3~5ê°œë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”."},
                        {"role":"user","content":kw}
                    ],
                    model=m, temperature=t, top_p=tp, max_tokens=60, stop=["\n"]
                )["message"]["content"]
                return kw, hs

            btn_kw.click(on_kw, [model_tag, temperature, top_p, max_tokens, text_kw], [kw_out, hash_out])

        # ============ íƒ­ 8: ì•”í˜¸í™”í ì •ë³´ í…œí”Œë¦¿ ============
        with gr.Tab("ì•”í˜¸í™”í ì •ë³´(í…œí”Œë¦¿)"):
            coin = gr.Textbox(label="ì½”ì¸ ì´ë¦„", value="Bitcoin")
            btn_coin = gr.Button("ìƒì„±")
            coin_out = gr.Textbox(label="ì„¤ëª…", lines=12, show_copy_button=True)
            gr.Markdown("> ì£¼ì˜: ê°€ê²©/ìµœê³ ê°€ ë“± ìµœì‹  ìˆ˜ì¹˜ëŠ” ëª¨ë¸ì´ ë¶€ì •í™•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (ë³¸ ì•±ì€ ì¸í„°ë„· ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤)")

            def on_coin(m, t, tp, mt, name):
                msgs = [
                    {"role":"system","content":"ë„ˆëŠ” ìŠ¤ë§ˆíŠ¸ ì–´ì‹œìŠ¤í„´íŠ¸ë‹¤. ì•„ë˜ í˜•ì‹ì„ ë”°ë¥´ë˜, ëª¨í˜¸í•˜ë©´ ì•ˆì „í•˜ê²Œ unknownìœ¼ë¡œ í‘œê¸°í•´ë¼."},
                    {"role":"assistant","content":"- ìƒì„±ì—°ë„: \n- ê³µì‹ ì‚¬ì´íŠ¸: \n- ìµœì‹  ê°€ê²©: \n- ìµœê³ ê°€: \n- ìµœì €ê°€: \n"},
                    {"role":"user","content":name}
                ]
                r = ollama_complete(messages=msgs, model=m, temperature=t, top_p=tp, max_tokens=int(mt))
                return r["message"]["content"]
            btn_coin.click(on_coin, [model_tag, temperature, top_p, max_tokens, coin], coin_out)

        # ============ íƒ­ 9: í•´ì‹œíƒœê·¸ ì¶”ì¶œ â†’ íŠ¸ìœ— ìƒì„± ============
        with gr.Tab("í•´ì‹œíƒœê·¸ â†’ íŠ¸ìœ—"):
            src = gr.Textbox(label="ë³¸ë¬¸", lines=8, value="ì¢‹ì€ ì¹œêµ¬ëŠ” ...")
            btn_tweet = gr.Button("íŠ¸ìœ— ìƒì„±")
            tweet_out = gr.Textbox(label="íŠ¸ìœ—(â‰¤280ì)", lines=6, show_copy_button=True)

            def on_tweet(m, t, tp, mt, body):
                hashtags = ollama_complete(
                    messages=[{"role":"system","content":"í…ìŠ¤íŠ¸ì—ì„œ ê´€ë ¨ í•´ì‹œíƒœê·¸ 3~5ê°œ ì¶”ì¶œ"},
                              {"role":"user","content":body}],
                    model=m, temperature=0.2, top_p=tp, max_tokens=60
                )["message"]["content"]
                tweet = ollama_complete(
                    messages=[{"role":"system","content":"ì£¼ì–´ì§„ í•´ì‹œíƒœê·¸ë¡œ 100~280ì íŠ¸ìœ— ì‘ì„±"},
                              {"role":"user","content":f"{body}\n{hashtags}"}],
                    model=m, temperature=t, top_p=tp, max_tokens=280, stop=["\n"]
                )["message"]["content"]
                # 280ì ì´ë‚´ ë³´ì •
                return tweet[:280]
            btn_tweet.click(on_tweet, [model_tag, temperature, top_p, max_tokens, src], tweet_out)

        # ============ íƒ­ 10: ë‘ ë‹¨ê³„ ë© ê°€ì‚¬ ============
        with gr.Tab("ë‘ ë‹¨ê³„ ë© ê°€ì‚¬"):
            btn_rap = gr.Button("ìƒì„±")
            rap_ctx = gr.Textbox(label="1ë‹¨ê³„ ìš”ì•½/ë°°ê²½", lines=6, show_copy_button=True)
            rap_out = gr.Textbox(label="2ë‹¨ê³„ ì˜¬ë“œìŠ¤ì¿¨ ë© ê°€ì‚¬", lines=12, show_copy_button=True)

            def on_rap(m, t, tp, mt):
                ctx = ollama_complete(
                    messages=[{"role":"system","content":"ì˜¬ë“œìŠ¤ì¿¨ ë©ì˜ ê°€ì‚¬ì  íŠ¹ì§•ê³¼ ì£¼ì œë¥¼ ê°„ê²° ì •ë¦¬"},
                              {"role":"user","content":"í•œ ë‹¨ë½ìœ¼ë¡œ ì •ë¦¬"}],
                    model=m, temperature=0.3, top_p=tp, max_tokens=200
                )["message"]["content"]
                lyric = ollama_complete(
                    messages=[{"role":"system","content":"ë„ˆëŠ” ìœ ëª…í•œ ì˜¬ë“œìŠ¤ì¿¨ ë© ì‘ì‚¬ê°€"},
                              {"role":"user","content":f"ë°°ê²½: {ctx}\n\nì •ì˜ì™€ í‰ë“±ì— ëŒ€í•œ ì˜¬ë“œìŠ¤ì¿¨ ë© ê°€ì‚¬ë¥¼ ì¨ì¤˜."}],
                    model=m, temperature=t, top_p=tp, max_tokens=500
                )["message"]["content"]
                return ctx, lyric
            btn_rap.click(on_rap, [model_tag, temperature, top_p, max_tokens], [rap_ctx, rap_out])

        # ============ íƒ­ 11: í’ˆì‚¬ íŒë³„(ì—¬ëŸ¬ ë¬¸ì¥) ============
        with gr.Tab("í’ˆì‚¬ íŒë³„"):
            prompts = gr.Textbox(label="ë¬¸ì¥ë“¤", lines=10, value=
                "The light is red.\n"
                "This desk is very light.\n"
                "You light up my life.\n"
                "He stepped light on the snow."
            )
            btn_pos = gr.Button("íŒë³„")
            pos_out = gr.Textbox(label="ì‘ë‹µ", lines=10, show_copy_button=True)

            def on_pos(m, t, tp, mt, txt):
                lines = [s.strip() for s in txt.splitlines() if s.strip()]
                results = []
                for s in lines:
                    r = ollama_complete(
                        messages=[{"role":"system","content":"ë‹¨ì–´ 'light'ì˜ í’ˆì‚¬ë¥¼ ì˜ì–´ë¡œ 1~2ë‹¨ì–´ë¡œ ë‹µí•˜ë¼ (noun/adj/verb/adv ë“±)"},
                                  {"role":"user","content":s}],
                        model=m, temperature=0.0, top_p=tp, max_tokens=40
                    )["message"]["content"]
                    results.append(f"{s} -> {r}")
                return "\n".join(results)
            btn_pos.click(on_pos, [model_tag, temperature, top_p, max_tokens, prompts], pos_out)

        # ============ íƒ­ 12: few-shot ë¶„ë¥˜ ============
        with gr.Tab("Few-shot ë¶„ë¥˜ (Apple)"):
            few_in = gr.Dropdown(choices=["í”„ë¡¬í”„íŠ¸ A", "í”„ë¡¬í”„íŠ¸ B"], value="í”„ë¡¬í”„íŠ¸ A", label="ì¼€ì´ìŠ¤")
            btn_fs = gr.Button("ë¶„ë¥˜")
            fs_out = gr.Textbox(label="ì‘ë‹µ", lines=6, show_copy_button=True)

            def on_fs(m, t, tp, mt, which):
                if which == "í”„ë¡¬í”„íŠ¸ A":
                    prompt = (
                        "Huawei:\ncompany\n\nGoogle:\ncompany\n\nMicrosoft:\ncompany\nApple:\n"
                    )
                else:
                    prompt = (
                        "Huawei:\ncompany\n\nGoogle:\ncompany\n\nMicrosoft:\ncompany\n\nApricot:\nFruit\n\nApple:\n"
                    )
                r = ollama_complete(
                    messages=[{"role":"system","content":"ë„ˆëŠ” ìŠ¤ë§ˆíŠ¸í•œ ì–´ì‹œìŠ¤í„´íŠ¸"},
                              {"role":"user","content":prompt}],
                    model=m, temperature=0.0, top_p=tp, max_tokens=40
                )["message"]["content"]
                return r.strip()
            btn_fs.click(on_fs, [model_tag, temperature, top_p, max_tokens, few_in], fs_out)

        # ============ íƒ­ 13: DevOps (Dockerfile/K8s) ============
        with gr.Tab("DevOps ìƒì„±"):
            sel = gr.Dropdown(choices=["Python Dockerfile", "MySQL K8s ë°°í¬"], value="Python Dockerfile")
            btn_dev = gr.Button("ìƒì„±")
            dev_out = gr.Code(label="ê²°ê³¼", language="yaml", lines=18)

            def on_dev(m, t, tp, mt, kind):
                if kind == "Python Dockerfile":
                    prompt = (
                        "# Node.jsìš© Dockerfile:\n"
                        "FROM node:14\nWORKDIR /app\nCOPY . /app\nRUN npm install\n"
                        "EXPOSE 8080\nCMD [\"node\", \"app.js\"]\n"
                        "# Pythonìš© Dockerfile:\n"
                    )
                else:
                    prompt = (
                        "# Redisìš© Kubernetes ë°°í¬:\n"
                        "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-deployment\n"
                        "spec:\n  selector:\n    matchLabels:\n      app: redis\n  template:\n    metadata:\n      labels:\n        app: redis\n"
                        "    spec:\n      containers:\n      - name: redis\n        image: redis\n        ports:\n        - containerPort: 6379\n"
                        "# MySQLìš© Kubernetes ë°°í¬:\n"
                    )
                r = ollama_complete(
                    messages=[{"role":"system","content":"êµ¬ì„± ìŠ¤í¬ë¦½íŠ¸ë¥¼ ê¹”ë”íˆ ì‘ì„±í•˜ëŠ” DevOps ì–´ì‹œìŠ¤í„´íŠ¸"},
                              {"role":"user","content":prompt}],
                    model=m, temperature=0.2, top_p=tp, max_tokens=400
                )["message"]["content"]
                return r.strip()
            btn_dev.click(on_dev, [model_tag, temperature, top_p, max_tokens, sel], dev_out)

        # ============ íƒ­ 14: TODO ìƒì„± (stop ì‹œí€€ìŠ¤) ============
        with gr.Tab("TODO ìƒì„± (stop)"):
            n_tasks = gr.Slider(1, 20, value=5, step=1, label="í•  ì¼ ê°œìˆ˜ n")
            btn_todo = gr.Button("ìƒì„±")
            todo_out = gr.Textbox(label="ê²°ê³¼", lines=12, show_copy_button=True)

            def on_todo(m, t, tp, mt, n):
                n = int(n)
                prompt = (
                    "Please create a todo list for establishing a company in the United States.\n"
                    "Each task should be written in one line.\n"
                    "Task 1: [task 1]\nTask 2: [task 2]\nTask 3: [task 3]\n...\nTask n: [task n]\n"
                )
                stop = [f"Task {n+1}:", "assistant:", "user:"]
                r = ollama_complete(
                    messages=[{"role":"system","content":"You are a smart assistant."},
                              {"role":"user","content":prompt}],
                    model=m, temperature=t, top_p=tp, max_tokens=max(60, n*50), stop=stop
                )["message"]["content"]
                return r
            btn_todo.click(on_todo, [model_tag, temperature, top_p, max_tokens, n_tasks], todo_out)

        # ============ íƒ­ 15: ì…¸ ëª…ë ¹ ì œì•ˆ(ì˜µì…˜ ì‹¤í–‰) ============
        with gr.Tab("ì…¸ ëª…ë ¹ ì œì•ˆ (ì£¼ì˜)"):
            gr.Markdown("**ìœ„í—˜ ê²½ê³ **: ì‹¤ì œ ëª…ë ¹ ì‹¤í–‰ì€ ë¡œì»¬ ì‹œìŠ¤í…œì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ì€ ì‹¤í–‰ ê¸ˆì§€ì…ë‹ˆë‹¤.")
            allow_exec = gr.Checkbox(label="ì‹¤í–‰ í—ˆìš©(ë³¸ì¸ ì±…ì„)", value=False)
            hist = gr.Textbox(label="ëŒ€í™” íˆìŠ¤í† ë¦¬(í”„ë¡¬í”„íŠ¸ ì²´ì¸)", lines=12, value=
                "System: ë‹¹ì‹ ì€ ìŠ¤ë§ˆíŠ¸í•œ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë‹µë³€ì€ ëª…ë ¹ì¤„ì˜ ë‚´ìš©ë§Œ í•´ ì£¼ì„¸ìš”.\n"
                "User: í˜„ì¬ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  íŒŒì¼ì„ ë‚˜ì—´í•´ ì£¼ì„¸ìš”.\n"
                "Assistant: ls -l\n"
                "User: ìˆ¨ê¹€ íŒŒì¼ í¬í•¨í•˜ì—¬ ë‚˜ì—´í•´ ì£¼ì„¸ìš”.\n"
                "Assistant: ls -la\n"
                "User: íŒŒì¼ 'test.txt'ì—ì„œ 'sun' ë‹¨ì–´ ê°œìˆ˜ ì„¸ì–´ ì£¼ì„¸ìš”.\n"
                "Assistant: grep -o 'sun' test.txt | wc -l\n"
            )
            user_q = gr.Textbox(label="ì§ˆë¬¸", value="í˜„ì¬ ë””ë ‰í† ë¦¬ì˜ íŒŒì¼ ìˆ˜ë¥¼ ì„¸ì–´ì£¼ì„¸ìš”.")
            btn_cmd = gr.Button("ëª…ë ¹ ì œì•ˆ â†’ (ì„ íƒ)ì‹¤í–‰")
            cmd_out = gr.Textbox(label="ì œì•ˆëœ ëª…ë ¹", lines=2, show_copy_button=True)
            exec_out = gr.Textbox(label="ì‹¤í–‰ ê²°ê³¼(stdout/stderr)", lines=8, show_copy_button=True)

            def on_cmd(m, t, tp, mt, hx, q, allow):
                # ê°„ë‹¨í•œ few-shot ëª…ë ¹ ìƒì„±
                prompt = f"{hx}\nUser: {q}\nAssistant:"
                r = ollama_complete(
                    messages=[{"role":"system","content":"ë‹µë³€ì€ ì‰˜ í•œ ì¤„ ëª…ë ¹ë§Œ."},
                              {"role":"user","content":prompt}],
                    model=m, temperature=0.0, top_p=tp, max_tokens=60
                )["message"]["content"].strip()
                executed = ""
                if allow:
                    try:
                        res = subprocess.run(r, shell=True, capture_output=True, text=True, timeout=30)
                        executed = res.stdout + ("\n[stderr]\n"+res.stderr if res.stderr else "")
                    except Exception as e:
                        executed = f"[ì‹¤í–‰ ì˜¤ë¥˜] {e}"
                else:
                    executed = "(ì‹¤í–‰ ê¸ˆì§€ë¨)"
                return r, executed

            btn_cmd.click(on_cmd, [model_tag, temperature, top_p, max_tokens, hist, user_q, allow_exec], [cmd_out, exec_out])

        # ============ ëª¨ë¸ ëª©ë¡ ============
        with gr.Tab("ëª¨ë¸ ëª©ë¡"):
            btn_list = gr.Button("ollama list")
            list_out = gr.JSON(label="ì„¤ì¹˜ëœ ëª¨ë¸")

            def on_list():
                return ollama.list()
            btn_list.click(on_list, outputs=list_out)

        gr.Markdown(
            """
            ### ë§¤í•‘ ìš”ì•½ (OpenAI â†’ Ollama)
            - `OpenAI(api_key=...)` â†’ **ë¶ˆí•„ìš”** (ë¡œì»¬ ì¶”ë¡ )
            - `client.chat.completions.create` â†’ `ollama.chat` ë˜í•‘(ìœ„ í•¨ìˆ˜ë“¤)
            - `model="gpt-4o(-mini)"` â†’ `model="deepseek-r1"` (ë¡œì»¬ íƒœê·¸ë¡œ êµì²´)
            - `max_tokens` â†’ `num_predict`
            - `temperature`, `top_p`, `stop` â†’ ë™ì¼ ì˜ë¯¸
            - `n`(ë‹¤ì¤‘ í›„ë³´) â†’ ì—°ì† í˜¸ì¶œë¡œ êµ¬í˜„
            - `frequency/presence_penalty` â†’ **repeat_penalty**ë¡œ ê·¼ì‚¬(ëª¨ë¸ì— ë”°ë¼ ì²´ê° ìƒì´)
            """
        )
    return demo


# ==============================
# ë©”ì¸
# ==============================
if __name__ == "__main__":
    app = build_app()
    # ì™¸ë¶€ ì ‘ì† í•„ìš” ì—†ìœ¼ë©´ server_name ìƒëµ ê°€ëŠ¥
    app.launch(server_name="0.0.0.0", server_port=7860, show_error=True)

