#45
"""
LlamaIndex + Ollama (DeepSeek-R1) RAG ë°ëª¨ - í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰(BM25+ë²¡í„°) + Reranker + ì„¹ì…˜/ëª©ì°¨ ê¸°ë°˜ ì²­í‚¹ + ì¶œì²˜ ë…¸ì¶œ

ìš”ì•½
- ë¡œì»¬ LLM/ìž„ë² ë”©: Ollama(DeepSeek-R1, nomic-embed-text)
- í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰: ë²¡í„° ê²€ìƒ‰ + BM25(ì „í†µì  ë‹¨ì–´ê¸°ë°˜) ê²°ê³¼ë¥¼ ê°€ì¤‘ ë³‘í•©
- Reranker: Sentence-Transformers êµì°¨ ì¸ì½”ë”ë¡œ ìƒìœ„ ë¬¸ì„œ ìž¬ëž­í‚¹
- ì²­í‚¹ ìµœì í™”: Markdown/ì„¹ì…˜(í—¤ë”©) ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆˆ ë’¤ ë¬¸ìž¥ ë‹¨ìœ„ ì„¸ë¶„í™”(ì¤‘ì²©/ì˜¤ë²„ëž©)
- ì¶œì²˜ ë…¸ì¶œ: ì‘ë‹µ í•˜ë‹¨ì— ì†ŒìŠ¤ ë…¸ë“œ(íŒŒì¼/URL/ì„¹ì…˜/ìŠ¤ì½”ì–´) ë¦¬ìŠ¤íŠ¸ í‘œì‹œ
- Gradio UI í¬í•¨

ì‚¬ì „ ì¤€ë¹„
1) Ollama ì„¤ì¹˜ ë° ì‹¤í–‰ (http://localhost:11434)
   - ì–¸ì–´ëª¨ë¸:  `ollama pull deepseek-r1`
   - ìž„ë² ë”©:    `ollama pull nomic-embed-text`
2) íŒ¨í‚¤ì§€ ì„¤ì¹˜
   pip install "llama-index>=0.10.0" llama-index-llms-ollama llama-index-embeddings-ollama \
               gradio requests pypdf beautifulsoup4 sentence-transformers

ì‹¤í–‰
  python app.py
"""
from __future__ import annotations
import math
from pathlib import Path
from typing import List, Optional, Dict, Iterable

import gradio as gr

# LlamaIndex í•µì‹¬
from llama_index.core import (
    VectorStoreIndex,
    SimpleDirectoryReader,
    StorageContext,
    load_index_from_storage,
    Settings,
)
from llama_index.core.base.base_retriever import BaseRetriever
from llama_index.core.schema import NodeWithScore, QueryBundle
from llama_index.core.retrievers import RecursiveRetriever
from llama_index.core.response_synthesizers import get_response_synthesizer

# LLM/ìž„ë² ë”©: Ollama
from llama_index.llms.ollama import Ollama
from llama_index.embeddings.ollama import OllamaEmbedding

# ë¡œë”
from llama_index.readers.web import BeautifulSoupWebReader

# ì²­í‚¹/íŒŒì„œ
from llama_index.core.node_parser import MarkdownNodeParser, SentenceSplitter

# Reranker (ë²„ì „ í˜¸í™˜ ìž„í¬íŠ¸)
SentenceTransformerRerank = None
try:
    from llama_index.postprocessor.sentence_transformer_rerank import SentenceTransformerRerank  # type: ignore
except Exception:
    try:
        from llama_index.postprocessor.sbert_rerank import SentenceTransformerRerank  # type: ignore
    except Exception:
        SentenceTransformerRerank = None

# BM25 (ë²„ì „ í˜¸í™˜ ìž„í¬íŠ¸)
BM25RetrieverClass = None
try:
    from llama_index.core.retrievers import BM25Retriever as _BM25
    BM25RetrieverClass = _BM25
except Exception:
    try:
        from llama_index.retrievers.bm25 import BM25Retriever as _BM25  # type: ignore
        BM25RetrieverClass = _BM25
    except Exception:
        BM25RetrieverClass = None

# ---------------------------
# ì „ì—­ ì„¤ì •: ë¡œì»¬ LLM + ìž„ë² ë”©
# ---------------------------
DEFAULT_LLM_MODEL = "deepseek-r1"
DEFAULT_EMBED_MODEL = "nomic-embed-text"


def configure_llamaindex(
    llm_model: str = DEFAULT_LLM_MODEL,
    embed_model: str = DEFAULT_EMBED_MODEL,
    temperature: float = 0.2,
    top_p: float = 0.95,
):
    """LlamaIndex ì „ì—­ Settingsì— ë¡œì»¬ Ollama LLM/ìž„ë² ë”© ë“±ë¡."""
    Settings.llm = Ollama(
        model=llm_model,
        request_timeout=120,
        temperature=temperature,
        top_p=top_p,
    )
    Settings.embed_model = OllamaEmbedding(model_name=embed_model)


# ---------------------------
# ì²­í‚¹(ì„¹ì…˜/ë¬¸ìž¥ ê¸°ë°˜) íŒŒì´í”„ë¼ì¸
# ---------------------------

def build_chunking_pipeline(
    use_markdown_section: bool = True,
    sentence_chunk_size: int = 800,
    sentence_chunk_overlap: int = 160,
):
    """Markdown ì„¹ì…˜ íŒŒì„œ + ë¬¸ìž¥ ë¶„í• ê¸° êµ¬ì„±.
    - MarkdownNodeParser: í—¤ë”©(##, ###, â€¦) ê¸°ì¤€ ì„¹ì…˜ ë…¸ë“œ ìƒì„±
    - SentenceSplitter: ì„¹ì…˜ ë‚´ë¶€ë¥¼ ë¬¸ìž¥ ë‹¨ìœ„ë¡œ ì„¸ë¶„í™” (ì¤‘ì²©/ì˜¤ë²„ëž© ì§€ì›)
    """
    parsers: List = []
    if use_markdown_section:
        # ì„¹ì…˜(í—¤ë”©) ë‹¨ìœ„ ë…¸ë“œ ìƒì„±: ë©”íƒ€ë°ì´í„°ì— ì„¹ì…˜ ì œëª©, ê²½ë¡œê°€ í¬í•¨ë¨
        parsers.append(MarkdownNodeParser())
    # ë¬¸ìž¥ ë¶„í• ê¸°: ê¸¸ì´ ê¸°ë°˜ ì²­í‚¹, ì˜¤ë²„ëž©ìœ¼ë¡œ ë¬¸ë§¥ ë³´ì¡´
    parsers.append(
        SentenceSplitter(
            chunk_size=sentence_chunk_size,
            chunk_overlap=sentence_chunk_overlap,
        )
    )
    return parsers


# ---------------------------
# ìƒ‰ì¸ ì €ìž¥/ë¶ˆëŸ¬ì˜¤ê¸°
# ---------------------------

def build_index_from_documents(documents, persist_dir: str, transformations: Optional[List] = None) -> VectorStoreIndex:
    index = VectorStoreIndex.from_documents(documents, transformations=transformations)
    index.storage_context.persist(persist_dir=persist_dir)
    return index


def load_index(persist_dir: str) -> VectorStoreIndex:
    storage_context = StorageContext.from_defaults(persist_dir=persist_dir)
    return load_index_from_storage(storage_context)


# ---------------------------
# ë°ì´í„° ì†ŒìŠ¤ë³„ ìƒ‰ì¸ ìƒì„±
# ---------------------------

def index_local_folder(folder_path: str, persist_dir: str,
                       use_md_section: bool, chunk_size: int, chunk_overlap: int) -> str:
    folder = Path(folder_path)
    if not folder.exists() or not folder.is_dir():
        return f"[ì˜¤ë¥˜] í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {folder_path}"

    documents = SimpleDirectoryReader(folder_path).load_data()
    transformations = build_chunking_pipeline(use_md_section, chunk_size, chunk_overlap)
    index = build_index_from_documents(documents, persist_dir, transformations)
    _ = index.as_query_engine()  # ì—”ì§„ ìƒì„±ìœ¼ë¡œ ê°„ë‹¨ ê²€ì¦
    return f"[ì™„ë£Œ] í´ë” ìƒ‰ì¸ ìƒì„± ë° ì €ìž¥: {persist_dir} (ë¬¸ì„œ ìˆ˜: {len(documents)})"


def index_web_urls(urls_text: str, persist_dir: str,
                   use_md_section: bool, chunk_size: int, chunk_overlap: int) -> str:
    urls: List[str] = []
    for token in urls_text.replace(",", "\n").splitlines():
        u = token.strip()
        if u:
            urls.append(u)
    if not urls:
        return "[ì˜¤ë¥˜] ìœ íš¨í•œ URLì´ ì—†ìŠµë‹ˆë‹¤."

    loader = BeautifulSoupWebReader()
    documents = loader.load_data(urls=urls)
    transformations = build_chunking_pipeline(use_md_section, chunk_size, chunk_overlap)
    index = build_index_from_documents(documents, persist_dir, transformations)
    _ = index.as_query_engine()
    return f"[ì™„ë£Œ] ì›¹ ë¬¸ì„œ ìƒ‰ì¸ ìƒì„± ë° ì €ìž¥: {persist_dir} (ë¬¸ì„œ ìˆ˜: {len(documents)})"


def index_pdf_files(file_paths_text: str, persist_dir: str,
                    use_md_section: bool, chunk_size: int, chunk_overlap: int) -> str:
    paths: List[Path] = []
    for token in file_paths_text.replace(",", "\n").splitlines():
        p = Path(token.strip())
        if p.exists() and p.is_file() and p.suffix.lower() == ".pdf":
            paths.append(p)
    if not paths:
        return "[ì˜¤ë¥˜] ìœ íš¨í•œ PDF íŒŒì¼ ê²½ë¡œê°€ ì—†ìŠµë‹ˆë‹¤."

    documents = SimpleDirectoryReader(input_files=paths).load_data()
    transformations = build_chunking_pipeline(use_md_section, chunk_size, chunk_overlap)
    index = build_index_from_documents(documents, persist_dir, transformations)
    _ = index.as_query_engine()
    return f"[ì™„ë£Œ] PDF ìƒ‰ì¸ ìƒì„± ë° ì €ìž¥: {persist_dir} (ë¬¸ì„œ ìˆ˜: {len(documents)})"


# ---------------------------
# í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ë²¡í„° + BM25) + Reranker
# ---------------------------
class HybridRetriever(BaseRetriever):
    """ë²¡í„° ê²€ìƒ‰ê³¼ BM25 ê²°ê³¼ë¥¼ ê°€ì¤‘ ë³‘í•©í•˜ëŠ” ì»¤ìŠ¤í…€ ë¦¬íŠ¸ë¦¬ë²„.

    - vector_weight(alpha): 0~1 ì‚¬ì´. ìµœì¢… ìŠ¤ì½”ì–´ = alpha*vector + (1-alpha)*bm25
    - ê° ê²°ê³¼ì˜ ì ìˆ˜ëŠ” 0~1ë¡œ ì •ê·œí™”(min-max)í•˜ì—¬ ê²°í•©
    - BM25RetrieverëŠ” LlamaIndex ë‚´ìž¥ í´ëž˜ìŠ¤ë¥¼ ì‚¬ìš©(ë²„ì „ í˜¸í™˜ ì²˜ë¦¬)
    """

    def __init__(
        self,
        index: VectorStoreIndex,
        vector_top_k: int = 8,
        bm25_top_k: int = 16,
        alpha: float = 0.5,
    ) -> None:
        super().__init__()
        self.index = index
        self.vector_top_k = vector_top_k
        self.bm25_top_k = bm25_top_k
        self.alpha = max(0.0, min(1.0, alpha))

        # ë²¡í„° ë¦¬íŠ¸ë¦¬ë²„
        self.vector_retriever = index.as_retriever(similarity_top_k=self.vector_top_k)

        # BM25 ë¦¬íŠ¸ë¦¬ë²„ (ê°€ëŠ¥í•œ ê²½ìš° ì¸ë±ìŠ¤ì—ì„œ ì§ì ‘ ìƒì„±)
        self.bm25_retriever = None
        if BM25RetrieverClass is not None:
            try:
                # ì¼ë¶€ ë²„ì „ì€ from_defaults(index=index) ì§€ì›
                self.bm25_retriever = BM25RetrieverClass.from_defaults(
                    index=index, similarity_top_k=self.bm25_top_k
                )
            except Exception:
                try:
                    # ëŒ€ì•ˆ: ëª¨ë“  ë…¸ë“œë¥¼ ìˆ˜ì§‘í•´ì„œ êµ¬ì„±(ì €ìž¥ì†Œì—ì„œ ë…¸ë“œ ë¡œë“œ)
                    nodes = list(index.docstore.get_all_nodes().values())
                    self.bm25_retriever = BM25RetrieverClass.from_defaults(
                        nodes=nodes, similarity_top_k=self.bm25_top_k
                    )
                except Exception:
                    self.bm25_retriever = None

    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:
        # 1) ë²¡í„° ê²€ìƒ‰
        v_nodes = self.vector_retriever.retrieve(query_bundle)

        # 2) BM25 ê²€ìƒ‰ (ê°€ëŠ¥í•œ ê²½ìš°)
        b_nodes: List[NodeWithScore] = []
        if self.bm25_retriever is not None:
            b_nodes = self.bm25_retriever.retrieve(query_bundle)

        # 3) ì ìˆ˜ ì •ê·œí™” + ë³‘í•©
        def normalize(nodes: List[NodeWithScore]) -> Dict[str, float]:
            if not nodes:
                return {}
            scores = [n.score for n in nodes if n.score is not None]
            if not scores:
                return {}
            s_min, s_max = min(scores), max(scores)
            denom = (s_max - s_min) if (s_max - s_min) > 1e-9 else 1.0
            norm = {}
            for n in nodes:
                s = n.score if n.score is not None else s_min
                norm[str(n.node.node_id)] = (s - s_min) / denom
            return norm

        v_map = normalize(v_nodes)
        b_map = normalize(b_nodes)

        # id -> NodeWithScore (ì›ë³¸ ë…¸ë“œ ë³´ê´€)
        id2node: Dict[str, NodeWithScore] = {}
        for n in v_nodes + b_nodes:
            id2node[str(n.node.node_id)] = n

        # ê²°í•© ìŠ¤ì½”ì–´ ê³„ì‚°
        combined: List[NodeWithScore] = []
        keys = set(v_map.keys()) | set(b_map.keys())
        for k in keys:
            v = v_map.get(k, 0.0)
            b = b_map.get(k, 0.0)
            score = self.alpha * v + (1.0 - self.alpha) * b
            node = id2node[k]
            combined.append(NodeWithScore(node=node.node, score=score))

        # ìŠ¤ì½”ì–´ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        combined.sort(key=lambda x: x.score or 0.0, reverse=True)
        return combined


def make_query_engine(
    index: VectorStoreIndex,
    response_mode: str = "default",
    vector_top_k: int = 8,
    bm25_top_k: int = 16,
    alpha: float = 0.5,
    use_reranker: bool = True,
    rerank_model: str = "cross-encoder/ms-marco-MiniLM-L-6-v2",
    rerank_top_n: int = 4,
):
    # í•˜ì´ë¸Œë¦¬ë“œ ë¦¬íŠ¸ë¦¬ë²„
    retriever = HybridRetriever(
        index=index,
        vector_top_k=vector_top_k,
        bm25_top_k=bm25_top_k,
        alpha=alpha,
    )

    # ë…¸ë“œ í›„ì²˜ë¦¬ê¸° (Reranker)
    node_postprocessors = []
    if use_reranker:
        if SentenceTransformerRerank is None:
            raise RuntimeError(
                "SentenceTransformerRerank ìž„í¬íŠ¸ ì‹¤íŒ¨. 'sentence-transformers' ë° LlamaIndex ë²„ì „ì„ í™•ì¸í•˜ì„¸ìš”."
            )
        node_postprocessors.append(
            SentenceTransformerRerank(top_n=rerank_top_n, model=rerank_model)
        )

    # ì‘ë‹µ í•©ì„±ê¸°
    synthesizer = get_response_synthesizer(response_mode=response_mode)

    # Query Engine ì¡°ë¦½(RecursiveRetriever ëž˜í¼ ì‚¬ìš©í•´ë„ ë˜ì§€ë§Œ ì—¬ê¸°ì„  ì§ì ‘ ì²˜ë¦¬)
    class _QE:
        def __init__(self, retriever: BaseRetriever):
            self.retriever = retriever
        def query(self, q: str):
            qb = QueryBundle(q)
            nodes = self.retriever.retrieve(qb)
            # í›„ì²˜ë¦¬ ì ìš©
            processed = nodes
            for pp in node_postprocessors:
                processed = pp.postprocess_nodes(processed, qb)
            # ì‘ë‹µ ìƒì„±
            resp = synthesizer.synthesize(
                query=qb,
                nodes=processed,
            )
            # ì†ŒìŠ¤ ë…¸ë“œë¥¼ ë¶™ì—¬ì„œ ë°˜í™˜(ë¬¸ìžì—´ + ì¶œì²˜ ëª©ë¡)
            lines = [str(resp)]
            lines.append("\n\n---\n### ðŸ”Ž Sources")
            for i, sn in enumerate(resp.source_nodes, 1):
                meta = sn.node.metadata or {}
                file = meta.get("file_name") or meta.get("filename") or meta.get("source") or "-"
                url = meta.get("url") or meta.get("URL") or ""
                sect = meta.get("section") or meta.get("Category") or meta.get("title") or ""
                score = sn.score if sn.score is not None else 0.0
                item = f"{i}. score={score:.3f} | file={file}"
                if sect:
                    item += f" | section={sect}"
                if url:
                    item += f" | url={url}"
                lines.append(item)
            return "\n".join(lines)

    return _QE(retriever)


# ---------------------------
# ì§ˆì˜ í•¸ë“¤ëŸ¬ (UI)
# ---------------------------

def query_index(
    persist_dir: str,
    user_query: str,
    response_mode: str,
    vector_top_k: int,
    bm25_top_k: int,
    alpha: float,
    use_reranker: bool,
    rerank_model: str,
    rerank_top_n: int,
) -> str:
    if not user_query.strip():
        return "[ì˜¤ë¥˜] ì§ˆì˜ ë‚´ìš©ì„ ìž…ë ¥í•˜ì„¸ìš”."
    try:
        index = load_index(persist_dir)
    except Exception as e:
        return f"[ì˜¤ë¥˜] ìƒ‰ì¸ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ìƒ‰ì¸ì„ ìƒì„±í•˜ì„¸ìš”.\n- persist_dir: {persist_dir}\n- ìƒì„¸: {e}"

    try:
        qe = make_query_engine(
            index=index,
            response_mode=response_mode,
            vector_top_k=int(vector_top_k),
            bm25_top_k=int(bm25_top_k),
            alpha=float(alpha),
            use_reranker=bool(use_reranker),
            rerank_model=rerank_model.strip(),
            rerank_top_n=int(rerank_top_n),
        )
    except Exception as e:
        return f"[ì˜¤ë¥˜] Query Engine ìƒì„± ì‹¤íŒ¨: {e}"

    return qe.query(user_query)


# ---------------------------
# Gradio UI
# ---------------------------
with gr.Blocks(title="LlamaIndex + Ollama (RAG í•˜ì´ë¸Œë¦¬ë“œ/Rerank/ì„¹ì…˜ ì²­í‚¹/ì¶œì²˜)") as demo:
    gr.Markdown("## ðŸ”Ž LlamaIndex + Ollama RAG â€” í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰(BM25+ë²¡í„°) + Reranker + ì„¹ì…˜ ì²­í‚¹ + ì¶œì²˜ ë…¸ì¶œ")
    gr.Markdown(
        "- ë¡œì»¬ **Ollama LLM/ìž„ë² ë”©**\n"
        "- **ì„¹ì…˜ ê¸°ë°˜ ì²­í‚¹**(Markdown í—¤ë”©) + ë¬¸ìž¥ ë‹¨ìœ„ ì²­í‚¹ìœ¼ë¡œ ë¬¸ë§¥ ë³´ì¡´\n"
        "- **í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰**: ë²¡í„° + BM25 ê°€ì¤‘ ë³‘í•©(alphaë¡œ ì¡°ì ˆ)\n"
        "- **Reranker**: êµì°¨ ì¸ì½”ë”ë¡œ ìƒìœ„ ë¬¸ì„œ ìž¬ì •ë ¬\n"
        "- ì‘ë‹µ í•˜ë‹¨ì— **ì¶œì²˜(Source Nodes)** ìžë™ í‘œì‹œ"
    )

    # ì „ì—­ ëª¨ë¸ ì„¤ì •
    with gr.Accordion("âš™ï¸ ì „ì—­ LLM/ìž„ë² ë”© ì„¤ì • (Ollama)", open=True):
        with gr.Row():
            llm_model = gr.Textbox(label="LLM ëª¨ë¸", value=DEFAULT_LLM_MODEL, scale=2)
            embed_model = gr.Textbox(label="ìž„ë² ë”© ëª¨ë¸", value=DEFAULT_EMBED_MODEL, scale=2)
        with gr.Row():
            temperature = gr.Slider(0.0, 1.5, value=0.2, step=0.05, label="temperature", scale=1)
            top_p = gr.Slider(0.1, 1.0, value=0.95, step=0.05, label="top_p", scale=1)
        apply_btn = gr.Button("ëª¨ë¸ ì„¤ì • ì ìš©", variant="primary")
        apply_status = gr.Markdown(visible=True)

    # ìƒ‰ì¸ ìƒì„± íƒ­
    with gr.Tabs():
        with gr.Tab("ðŸ“ í´ë” ìƒ‰ì¸"):
            with gr.Row():
                folder_path = gr.Textbox(label="í´ë” ê²½ë¡œ", value="data", scale=3)
                persist_dir_folder = gr.Textbox(label="ì €ìž¥ ê²½ë¡œ(persist_dir)", value="storage_folder", scale=2)
            with gr.Row():
                use_md_section = gr.Checkbox(label="Markdown ì„¹ì…˜(í—¤ë”©) ê¸°ë°˜ ë¶„í•´ ì‚¬ìš©", value=True)
                chunk_size = gr.Slider(200, 2000, value=800, step=50, label="ë¬¸ìž¥ ì²­í‚¹ í¬ê¸°")
                chunk_overlap = gr.Slider(0, 400, value=160, step=10, label="ì˜¤ë²„ëž©")
            build_folder_btn = gr.Button("í´ë” ìƒ‰ì¸ ìƒì„±/ì €ìž¥", variant="primary")
            folder_status = gr.Textbox(label="ìƒíƒœ", lines=3)

        with gr.Tab("ðŸŒ ì›¹ URL ìƒ‰ì¸"):
            urls_text = gr.Textbox(
                label="URL ëª©ë¡ (ì‰¼í‘œ/ì¤„ë°”ê¿ˆ êµ¬ë¶„)",
                placeholder="ì˜ˆ) https://www.law.go.kr/ë²•ë ¹/ëŒ€í•œë¯¼êµ­í—Œë²•",
                lines=3,
            )
            with gr.Row():
                persist_dir_web = gr.Textbox(label="ì €ìž¥ ê²½ë¡œ(persist_dir)", value="storage_web")
                use_md_section_w = gr.Checkbox(label="Markdown ì„¹ì…˜ ë¶„í•´", value=True)
                chunk_size_w = gr.Slider(200, 2000, value=800, step=50, label="ë¬¸ìž¥ ì²­í‚¹ í¬ê¸°")
                chunk_overlap_w = gr.Slider(0, 400, value=160, step=10, label="ì˜¤ë²„ëž©")
            build_web_btn = gr.Button("ì›¹ ìƒ‰ì¸ ìƒì„±/ì €ìž¥", variant="primary")
            web_status = gr.Textbox(label="ìƒíƒœ", lines=3)

        with gr.Tab("ðŸ“„ PDF ìƒ‰ì¸"):
            pdf_paths_text = gr.Textbox(
                label="PDF ê²½ë¡œ ëª©ë¡ (ì‰¼í‘œ/ì¤„ë°”ê¿ˆ êµ¬ë¶„)",
                placeholder="ì˜ˆ) ./attention.pdf",
                lines=3,
            )
            with gr.Row():
                persist_dir_pdf = gr.Textbox(label="ì €ìž¥ ê²½ë¡œ(persist_dir)", value="storage_pdf")
                use_md_section_p = gr.Checkbox(label="Markdown ì„¹ì…˜ ë¶„í•´", value=True)
                chunk_size_p = gr.Slider(200, 2000, value=800, step=50, label="ë¬¸ìž¥ ì²­í‚¹ í¬ê¸°")
                chunk_overlap_p = gr.Slider(0, 400, value=160, step=10, label="ì˜¤ë²„ëž©")
            build_pdf_btn = gr.Button("PDF ìƒ‰ì¸ ìƒì„±/ì €ìž¥", variant="primary")
            pdf_status = gr.Textbox(label="ìƒíƒœ", lines=3)

    # ì§ˆì˜ + í•˜ì´ë¸Œë¦¬ë“œ/Rerank ì„¤ì •
    gr.Markdown("---")
    gr.Markdown("### ðŸ’¬ ì§ˆì˜í•˜ê¸° (ì €ìž¥ëœ ìƒ‰ì¸ ëŒ€ìƒ) â€” í•˜ì´ë¸Œë¦¬ë“œ & Reranker")
    with gr.Row():
        persist_dir_query = gr.Textbox(label="ëŒ€ìƒ persist_dir", value="storage_folder", scale=2)
        response_mode = gr.Dropdown(
            ["default", "compact", "tree_summarize", "accumulate"],
            value="default",
            label="response_mode",
            scale=1,
        )
    with gr.Row():
        vector_top_k = gr.Slider(1, 50, value=8, step=1, label="ë²¡í„° Top-K")
        bm25_top_k = gr.Slider(1, 100, value=16, step=1, label="BM25 Top-K")
        alpha = gr.Slider(0.0, 1.0, value=0.5, step=0.05, label="ê°€ì¤‘ì¹˜ Î± (ë²¡í„° ë¹„ì¤‘)")
    with gr.Row():
        use_reranker = gr.Checkbox(label="Reranker ì‚¬ìš©", value=True)
        rerank_model = gr.Textbox(
            label="Reranker ëª¨ë¸ (Sentence-Transformers)",
            value="cross-encoder/ms-marco-MiniLM-L-6-v2",
            placeholder="ì˜ˆ) cross-encoder/ms-marco-MiniLM-L-6-v2, BAAI/bge-reranker-base",
        )
        rerank_top_n = gr.Slider(1, 50, value=4, step=1, label="Rerank Top-N")

    user_query = gr.Textbox(
        label="ì§ˆì˜",
        placeholder="ì˜ˆ) What did the author do growing up? / í•œêµ­ì–´ë¡œ ìš”ì•½í•´ì¤˜",
        lines=2,
    )
    ask_btn = gr.Button("ì§ˆì˜ ì‹¤í–‰ ðŸš€", variant="primary")
    answer_box = gr.Textbox(label="ì‘ë‹µ + ì¶œì²˜", lines=14)

    # ì´ë²¤íŠ¸ ë°”ì¸ë”©
    def _apply_models(llm, emb, temp, tp):
        configure_llamaindex(llm, emb, temp, tp)
        return gr.update(value=f"âœ… ì ìš©ë¨: LLM={llm}, EMB={emb}, temperature={temp}, top_p={tp}")

    apply_btn.click(
        fn=_apply_models,
        inputs=[llm_model, embed_model, temperature, top_p],
        outputs=[apply_status],
    )

    build_folder_btn.click(
        fn=index_local_folder,
        inputs=[folder_path, persist_dir_folder, use_md_section, chunk_size, chunk_overlap],
        outputs=[folder_status],
    )

    build_web_btn.click(
        fn=index_web_urls,
        inputs=[urls_text, persist_dir_web, use_md_section_w, chunk_size_w, chunk_overlap_w],
        outputs=[web_status],
    )

    build_pdf_btn.click(
        fn=index_pdf_files,
        inputs=[pdf_paths_text, persist_dir_pdf, use_md_section_p, chunk_size_p, chunk_overlap_p],
        outputs=[pdf_status],
    )

    ask_btn.click(
        fn=query_index,
        inputs=[
            persist_dir_query,
            user_query,
            response_mode,
            vector_top_k,
            bm25_top_k,
            alpha,
            use_reranker,
            rerank_model,
            rerank_top_n,
        ],
        outputs=[answer_box],
    )

# ìµœì´ˆ 1íšŒ ì „ì—­ ëª¨ë¸ ì„¤ì •
configure_llamaindex()

if __name__ == "__main__":
    demo.launch()






# ollama pull deepseek-r1
# ollama pull nomic-embed-text
# pip install "llama-index>=0.10.0" llama-index-llms-ollama
# llama-index-embeddings-ollama \ gradio requests pypdf beautifulsoup4 
# sentence-transformers


