#93
# app.py
# ------------------------------------------------------------
# OpenAI ì „ìš© ì½”ë“œë¥¼ ë¡œì»¬ ì˜¤í”ˆì†ŒìŠ¤ LLM(Ollama + DeepSeek-R1)ë¡œ ì „í™˜í•œ ì˜¬ì¸ì› ì˜ˆì‹œ
# - ë¡œì»¬ REST: http://localhost:11434
# - ì±„íŒ…/ì¶”ë¡ : deepseek-r1:7b
# - ì„ë² ë”©: nomic-embed-text (ë˜ëŠ” ì„¤ì¹˜ëœ ì„ë² ë”© ëª¨ë¸ëª…ìœ¼ë¡œ êµì²´)
# - GUI: Gradio íƒ­ ì¸í„°í˜ì´ìŠ¤
# ------------------------------------------------------------

import os
import json
import time
import re
import ast
import requests
import numpy as np
import pandas as pd
import gradio as gr

# -------------------------------
# í™˜ê²½ ì„¤ì • (í•„ìš”ì‹œ ëª¨ë¸ëª… ë³€ê²½)
# -------------------------------
OLLAMA_HOST = os.environ.get("OLLAMA_HOST", "http://localhost:11434")
LLM_MODEL = os.environ.get("LLM_MODEL", "deepseek-r1:7b")
EMBED_MODEL = os.environ.get("EMBED_MODEL", "nomic-embed-text")  # ì„ë² ë”© ì „ìš© ëª¨ë¸

# -------------------------------
# Ollama REST wrapper
# -------------------------------
def ollama_generate(model: str, prompt: str, system: str = None, temperature: float = 0.2, max_tokens: int = 512):
    """
    DeepSeek-R1 ê°™ì€ ì¼ë°˜ LLMì— ëŒ€í•´ í…ìŠ¤íŠ¸ ìƒì„±(ëŒ€í™”) ìš”ì²­.
    - /api/generate ì—”ë“œí¬ì¸íŠ¸ ì‚¬ìš©
    - OllamaëŠ” ìŠ¤íŠ¸ë¦¬ë°ì„ ê¸°ë³¸ìœ¼ë¡œ í•˜ë¯€ë¡œ stream=False ì‚¬ìš©
    """
    url = f"{OLLAMA_HOST}/api/generate"
    payload = {
        "model": model,
        "prompt": prompt,
        "stream": False,
        "options": {
            "temperature": temperature,
            "num_predict": max_tokens
        }
    }
    # system í”„ë¡¬í”„íŠ¸ë¥¼ ëª¨ë¸ì— ì£¼ê³ ì‹¶ì„ ë•Œ, prompt ì•ìª½ì— prepend
    if system:
        payload["prompt"] = f"<<SYS>>\n{system}\n<</SYS>>\n\n{prompt}"

    try:
        resp = requests.post(url, json=payload, timeout=120)
        resp.raise_for_status()
        data = resp.json()
        return data.get("response", "").strip()
    except Exception as e:
        return f"[ì˜¤ë¥˜] Ollama generate ì‹¤íŒ¨: {e}"

def ollama_embed(model: str, texts):
    """
    ì„ë² ë”© ì „ìš© ëª¨ë¸ì— ëŒ€í•´ /api/embeddings í˜¸ì¶œ
    - texts: str ë˜ëŠ” str ë¦¬ìŠ¤íŠ¸
    - ë°˜í™˜: (N, D) numpy array
    """
    url = f"{OLLAMA_HOST}/api/embeddings"
    # ë‹¨ì¼ ë¬¸ìì—´ë„ ë¦¬ìŠ¤íŠ¸í™”
    is_single = isinstance(texts, str)
    if is_single:
        texts = [texts]

    vectors = []
    try:
        for t in texts:
            payload = {
                "model": model,
                "prompt": t
            }
            resp = requests.post(url, json=payload, timeout=120)
            resp.raise_for_status()
            emb = resp.json().get("embedding", None)
            if emb is None:
                raise RuntimeError("ì„ë² ë”© ì‘ë‹µì— 'embedding' í•„ë“œ ì—†ìŒ")
            vectors.append(emb)
        arr = np.array(vectors, dtype=np.float32)
        return (arr[0] if is_single else arr)
    except Exception as e:
        raise RuntimeError(f"[ì˜¤ë¥˜] Ollama embeddings ì‹¤íŒ¨: {e}")

# -------------------------------
# ë²¡í„°/ì „ì²˜ë¦¬ ìœ í‹¸
# -------------------------------
def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:
    a = np.asarray(a, dtype=np.float32)
    b = np.asarray(b, dtype=np.float32)
    denom = (np.linalg.norm(a) * np.linalg.norm(b))
    if denom == 0:
        return 0.0
    return float(np.dot(a, b) / denom)

def preprocess_text_simple(text: str) -> str:
    """
    ê°„ë‹¨ ì „ì²˜ë¦¬: ì†Œë¬¸ìí™” + ì•ŒíŒŒë²³/í•œê¸€/ìˆ«ì/ê³µë°±ë§Œ ìœ ì§€
    """
    text = text.lower()
    text = re.sub(r"[^0-9a-zA-Zã„±-ã…ê°€-í£\s]", " ", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text

# -------------------------------
# íƒ­ 1) ì„ë² ë”© ë°ëª¨
# -------------------------------
def embed_demo(input_text: str):
    if not input_text.strip():
        return "[ê²½ê³ ] ì…ë ¥ í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.", ""
    text = input_text.replace("\n", " ")
    try:
        emb = ollama_embed(EMBED_MODEL, text)
        return "ì„±ê³µ: ì„ë² ë”© ê¸¸ì´ = {}".format(len(emb)), str(list(map(float, emb[:8]))) + " ... (truncated)"
    except Exception as e:
        return "ì‹¤íŒ¨", str(e)

# -------------------------------
# íƒ­ 2) ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (ì„¸ ê°€ì§€ ë°©ì‹ê³¼ ë™ì¼ ê²°ê³¼)
# -------------------------------
def cosine_demo(vec_a_str: str, vec_b_str: str):
    """
    ì…ë ¥ ì˜ˆ: "2,3,5,2,6,7,9,2,3,4"
    """
    try:
        A = np.array([float(x) for x in vec_a_str.split(",")], dtype=np.float32)
        B = np.array([float(x) for x in vec_b_str.split(",")], dtype=np.float32)
        # Numpy
        cos_np = float(np.dot(A, B) / (np.linalg.norm(A) * np.linalg.norm(B)))
        # SciPyì™€ sklearn ì—†ì´ ë™ì¼ ìˆ˜ì‹ (ìœ„ì™€ ë™ì¼)
        cos_custom = cosine_similarity(A, B)
        return f"NumPy: {cos_np:.6f}\nCustom(ë™ì¼ì‹): {cos_custom:.6f}"
    except Exception as e:
        return f"[ì˜¤ë¥˜] ë²¡í„° íŒŒì‹±/ê³„ì‚° ì‹¤íŒ¨: {e}"

# -------------------------------
# íƒ­ 3) CSV ì„ë² ë”© + ê²€ìƒ‰
# -------------------------------
def csv_embed_and_search(csv_file, text_col: str, query: str, topk: int):
    """
    csv_fileì˜ text_colì„ ì„ë² ë”© -> embeddings.csvë¡œ ì €ì¥ -> queryì™€ ìœ ì‚¬ë„ TopK ë°˜í™˜
    """
    try:
        if csv_file is None:
            return "[ê²½ê³ ] CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.", None
        df = pd.read_csv(csv_file.name)
        if text_col not in df.columns:
            return f"[ì˜¤ë¥˜] '{text_col}' ì»¬ëŸ¼ì´ CSVì— ì—†ìŠµë‹ˆë‹¤. ì»¬ëŸ¼ë“¤: {list(df.columns)}", None
        # ì „ì²˜ë¦¬ + ì„ë² ë”©
        texts = [preprocess_text_simple(str(t)) for t in df[text_col].tolist()]
        embs = []
        for t in texts:
            embs.append(ollama_embed(EMBED_MODEL, t))
        df["embedding"] = [json.dumps(list(map(float, e))) for e in embs]

        # ì¿¼ë¦¬ ì„ë² ë”© ë° ìœ ì‚¬ë„
        q = preprocess_text_simple(query) if query else ""
        if q:
            q_emb = ollama_embed(EMBED_MODEL, q)
            sims = []
            for e in embs:
                sims.append(cosine_similarity(e, q_emb))
            df["similarity"] = sims
            df_sorted = df.sort_values(by="similarity", ascending=False).head(topk).reset_index(drop=True)
        else:
            df_sorted = df.head(topk).reset_index(drop=True)

        # ë¡œì»¬ì— ì €ì¥ (ë™ì¼ í´ë”)
        out_path = os.path.join(os.getcwd(), "embeddings.csv")
        df.to_csv(out_path, index=False)
        return f"ì™„ë£Œ! embeddings.csv ì €ì¥ ê²½ë¡œ: {out_path}", df_sorted
    except Exception as e:
        return f"[ì˜¤ë¥˜] CSV ì²˜ë¦¬ ì‹¤íŒ¨: {e}", None

# -------------------------------
# íƒ­ 4) í…ìŠ¤íŠ¸ ë¶„ë¥˜ (ë¼ë²¨ ì„ë² ë”©ê³¼ ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
# -------------------------------
DEFAULT_CATEGORIES = [
    "U.S. NEWS",
    "COMEDY",
    "PARENTING",
    "WORLD NEWS",
    "CULTURE & ARTS",
    "TECH",
    "SPORTS"
]

def classify_sentence(sentence: str, categories_text: str):
    """
    categories_text: ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„ëœ ì¹´í…Œê³ ë¦¬ ëª©ë¡
    ë¬¸ì¥ì„ ì„ë² ë”©í•˜ì—¬ ê° ì¹´í…Œê³ ë¦¬ ë¼ë²¨ ì„ë² ë”©ê³¼ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë¹„êµ
    """
    if not sentence.strip():
        return "[ê²½ê³ ] ë¬¸ì¥ì„ ì…ë ¥í•˜ì„¸ìš”."
    cats = [c.strip() for c in categories_text.split("\n") if c.strip()]
    if not cats:
        return "[ê²½ê³ ] ì¹´í…Œê³ ë¦¬ë¥¼ í•œ ì¤„ì— í•˜ë‚˜ì”© ì…ë ¥í•˜ì„¸ìš”."

    s_emb = ollama_embed(EMBED_MODEL, preprocess_text_simple(sentence))
    best_c, best_sim = None, -1.0
    details = []
    for c in cats:
        c_emb = ollama_embed(EMBED_MODEL, c.lower())
        sim = cosine_similarity(s_emb, c_emb)
        details.append((c, sim))
        if sim > best_sim:
            best_c, best_sim = c, sim
    details.sort(key=lambda x: x[1], reverse=True)
    lines = [f"[Top] {best_c}  (ìœ ì‚¬ë„: {best_sim:.4f})", "---- ì„¸ë¶€ ì ìˆ˜ ----"]
    lines += [f"{c}: {sim:.4f}" for c, sim in details]
    return "\n".join(lines)

# -------------------------------
# íƒ­ 5) â€œìŠ¤ë§ˆíŠ¸ í™ˆâ€ ì±„íŒ… (íŒŒì¸íŠœë‹ ëŒ€ì²´, ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ê³ ì •)
# -------------------------------
SMART_HOME_SYSTEM = (
    "ë‹¹ì‹ ì€ ìŠ¤ë§ˆíŠ¸ í™ˆ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ìš”ì²­ì„ ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ìˆ˜í–‰í•˜ì„¸ìš”. "
    "ìŠ¤ë§ˆíŠ¸ ì¡°ëª…/ì—ì–´ì»¨/ìŠ¤í”¼ì»¤/ì¼ì • ë“± í™ˆ ê´€ë ¨ ëª…ë ¹ì„ ì´í•´í•˜ê³  ì•ˆì „í•˜ê²Œ ë‹µí•˜ì„¸ìš”."
)

def smart_home_chat(history, user_msg, temperature=0.2):
    """
    Gradio ChatInterfaceìš© í•¨ìˆ˜
    history: [(user, bot), ...]
    user_msg: str
    """
    # ê°„ë‹¨í•œ ëŒ€í™” ì´ë ¥ í”„ë¡¬í”„íŠ¸ êµ¬ì„± (few-shot ìŠ¤íƒ€ì¼)
    convo = []
    for u, b in history:
        if u:
            convo.append(f"User: {u}")
        if b:
            convo.append(f"Assistant: {b}")
    convo.append(f"User: {user_msg}\nAssistant:")
    prompt = "\n".join(convo)
    reply = ollama_generate(LLM_MODEL, prompt=prompt, system=SMART_HOME_SYSTEM, temperature=float(temperature), max_tokens=512)
    return reply

# -------------------------------
# íƒ­ 6) ë‰´ìŠ¤ í—¤ë“œë¼ì¸ ë¶„ë¥˜ ì •ë°€ë„ í‰ê°€ (ì˜µì…˜ íŒŒì¼: data/news.json)
# -------------------------------
def evaluate_precision(json_path, sample_size=20):
    """
    data/news.json (HuffPost ìŠ¤íƒ€ì¼)ì—ì„œ 'headline','category' í•„ë“œ ì‚¬ìš©
    ê°„ë‹¨ ì„ë² ë”©-ë¼ë²¨ ìœ ì‚¬ë„ ë¶„ë¥˜ê¸°ë¡œ ì •ë°€ë„ ì¸¡ì • (micro)
    """
    try:
        if json_path is None:
            return "[ê²½ê³ ] data/news.json íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.", ""
        df = pd.read_json(json_path.name, lines=True)
        if "headline" not in df.columns or "category" not in df.columns:
            return "[ì˜¤ë¥˜] JSONì— headline/category í•„ë“œê°€ í•„ìš”í•©ë‹ˆë‹¤.", ""
        df = df.head(int(sample_size)).copy()
        cats = sorted(list(set(df["category"].tolist())))
        # ë¼ë²¨ ì„ë² ë”© ìºì‹œ
        cat2emb = {c: ollama_embed(EMBED_MODEL, c.lower()) for c in cats}

        y_true, y_pred = [], []
        wrong, right = [], []
        for _, row in df.iterrows():
            sent = preprocess_text_simple(str(row["headline"]))
            s_emb = ollama_embed(EMBED_MODEL, sent)
            # ìµœëŒ“ê°’ ì„ íƒ
            best_c, best_sim = None, -1.0
            for c in cats:
                sim = cosine_similarity(s_emb, cat2emb[c])
                if sim > best_sim:
                    best_c, best_sim = c, sim
            y_true.append(row["category"])
            y_pred.append(best_c)
            if best_c == row["category"]:
                right.append(f"[OK] {row['headline'][:60]} ... -> {best_c}")
            else:
                wrong.append(f"[X]  {row['headline'][:60]} ... -> pred={best_c} / gold={row['category']}")

        # micro-precision == accuracy (ë‹¨ì¼ ë¼ë²¨)
        acc = float(np.mean([1.0 if a == b else 0.0 for a, b in zip(y_true, y_pred)]))
        report = "\n".join(wrong[:10] + right[:10])  # ë„ˆë¬´ ê¸¸ì–´ì§ˆ ìˆ˜ ìˆìœ¼ë‹ˆ ê° 10ê°œë§Œ
        return f"ì •ë°€ë„(=ì •í™•ë„, micro): {acc:.4f}", report
    except Exception as e:
        return f"[ì˜¤ë¥˜] í‰ê°€ ì‹¤íŒ¨: {e}", ""

# -------------------------------
# íƒ­ 7) JSONL ë°ì´í„°ì…‹ í¬ë§· ê²€ì¦ê¸° (ë©”ì‹œì§€/ì—­í•  êµ¬ì¡° ì²´í¬)
# -------------------------------
def validate_jsonl(jsonl_file):
    """
    OpenAI íŠœë‹ í¬ë§· ìœ ì‚¬ ê·œì¹™ì„ ë¡œì»¬ì—ì„œ ê²€ì¦
    messages: [{role, content, ...}]
    """
    try:
        if jsonl_file is None:
            return "[ê²½ê³ ] JSONL íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”."
        with open(jsonl_file.name, "r", encoding="utf-8") as f:
            dataset = [json.loads(line) for line in f]

        if len(dataset) < 10:
            return "ì˜¤ë¥˜: ìµœì†Œ 10ê°œ ì´ìƒ ì˜ˆì œê°€ í•„ìš”í•©ë‹ˆë‹¤."

        format_errors = {}
        def inc(k): format_errors[k] = format_errors.get(k, 0) + 1

        for line in dataset:
            if not isinstance(line, dict):
                inc("data_type")
                continue
            messages = line.get("messages", None)
            if not messages or not isinstance(messages, list):
                inc("missing_messages_list")
                continue

            has_assistant = False
            for m in messages:
                if "role" not in m or "content" not in m:
                    inc("message_missing_key")
                valid_keys = {"role", "content", "name", "function_call"}
                if any(k not in valid_keys for k in m.keys()):
                    inc("message_unrecognized_key")
                if m.get("role") not in {"system", "user", "assistant", "function"}:
                    inc("unrecognized_role")
                content = m.get("content", None)
                function_call = m.get("function_call", None)
                if (not content and not function_call) or not isinstance(content, (str, type(None))):
                    inc("missing_content")
                if m.get("role") == "assistant":
                    has_assistant = True
            if not has_assistant:
                inc("example_missing_assistant_message")

        if format_errors:
            lines = ["Found errors:"]
            for k, v in format_errors.items():
                lines.append(f"{k}: {v}")
            lines.append("=> The dataset contains errors.")
            return "\n".join(lines)
        return "í†µê³¼: í˜•ì‹ ì˜¤ë¥˜ê°€ ì—†ìŠµë‹ˆë‹¤."
    except Exception as e:
        return f"[ì˜¤ë¥˜] ê²€ì¦ ì‹¤íŒ¨: {e}"

# -------------------------------
# Gradio UI
# -------------------------------
with gr.Blocks(title="OpenCode: Ollama + DeepSeek-R1 Playground", theme=gr.themes.Soft()) as demo:
    gr.Markdown(
        """
        # ğŸ§© OpenCode Â· Ollama + DeepSeek-R1 (ë¡œì»¬)
        - OpenAI ì˜ì¡´ ì½”ë“œë¥¼ **ë¡œì»¬ ì˜¤í”ˆì†ŒìŠ¤ í™˜ê²½**ìœ¼ë¡œ ì „í™˜í•œ ì˜ˆì‹œì…ë‹ˆë‹¤.
        - **ì„ë² ë”©**: `nomic-embed-text` Â· **LLM**: `deepseek-r1:7b`
        - ì¢Œì¸¡ íƒ­ì—ì„œ ê¸°ëŠ¥ì„ ì²´í—˜í•˜ì„¸ìš”.
        """
    )

    with gr.Tab("1) ì„ë² ë”© ë°ëª¨"):
        gr.Markdown("ë‹¨ì¼ ë¬¸ì¥ ì„ë² ë”© ìƒì„±")
        t_in = gr.Textbox(label="ì…ë ¥ í…ìŠ¤íŠ¸", value="I am a programmer")
        btn = gr.Button("ì„ë² ë”© ìƒì„±")
        status = gr.Textbox(label="ìƒíƒœ", interactive=False)
        head = gr.Textbox(label="ì„ë² ë”© ì•ë¶€ë¶„ ë¯¸ë¦¬ë³´ê¸°", interactive=False)
        btn.click(embed_demo, inputs=[t_in], outputs=[status, head])

    with gr.Tab("2) ì½”ì‚¬ì¸ ìœ ì‚¬ë„"):
        gr.Markdown("ë‘ ë²¡í„° ì½”ì‚¬ì¸ ìœ ì‚¬ë„(NumPy/ë™ì¼ì‹)")
        a = gr.Textbox(label="ë²¡í„° A (ì‰¼í‘œ êµ¬ë¶„)", value="2,3,5,2,6,7,9,2,3,4")
        b = gr.Textbox(label="ë²¡í„° B (ì‰¼í‘œ êµ¬ë¶„)", value="3,6,3,1,0,9,2,3,4,5")
        btn2 = gr.Button("ìœ ì‚¬ë„ ê³„ì‚°")
        out2 = gr.Textbox(label="ê²°ê³¼")
        btn2.click(cosine_demo, inputs=[a, b], outputs=[out2])

    with gr.Tab("3) CSV ì„ë² ë”© + ê²€ìƒ‰"):
        gr.Markdown("CSVì˜ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ì„ ì„ë² ë”©í•˜ê³ , ì¿¼ë¦¬ì™€ ìœ ì‚¬í•œ ìƒìœ„ í•­ëª©ì„ ì°¾ìŠµë‹ˆë‹¤.")
        f_csv = gr.File(label="CSV ì—…ë¡œë“œ (ì˜ˆ: words.csv)")
        col = gr.Textbox(label="í…ìŠ¤íŠ¸ ì»¬ëŸ¼ëª…", value="text")
        q = gr.Textbox(label="ê²€ìƒ‰ì–´(ì˜µì…˜)", value="The black cat sat on the mat")
        k = gr.Slider(1, 50, 10, step=1, label="TopK")
        btn3 = gr.Button("ì„ë² ë”© + ê²€ìƒ‰ ì‹¤í–‰")
        msg3 = gr.Textbox(label="ë©”ì‹œì§€")
        table3 = gr.Dataframe(label="ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°")
        btn3.click(csv_embed_and_search, inputs=[f_csv, col, q, k], outputs=[msg3, table3])

    with gr.Tab("4) í…ìŠ¤íŠ¸ ë¶„ë¥˜(ë¼ë²¨ ì„ë² ë”© ê¸°ë°˜)"):
        gr.Markdown("ë¬¸ì¥ì„ ë¼ë²¨ ì„ë² ë”©ê³¼ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ìµœëŒ€ê°’ìœ¼ë¡œ ë¶„ë¥˜")
        sent = gr.Textbox(label="ë¬¸ì¥ ì…ë ¥", value="ìƒˆë¡œìš´ ì•„ì´í°ì´ ì¶œì‹œë˜ì—ˆìŠµë‹ˆë‹¤.")
        cats = gr.Textbox(label="ì¹´í…Œê³ ë¦¬ ëª©ë¡(ì¤„ë°”ê¿ˆ êµ¬ë¶„)", value="\n".join(DEFAULT_CATEGORIES), lines=8)
        btn4 = gr.Button("ë¶„ë¥˜í•˜ê¸°")
        out4 = gr.Textbox(label="ê²°ê³¼")
        btn4.click(classify_sentence, inputs=[sent, cats], outputs=[out4])

    with gr.Tab("5) ìŠ¤ë§ˆíŠ¸ í™ˆ ì±„íŒ… (few-shot ëŒ€ì²´)"):
        gr.Markdown("íŒŒì¸íŠœë‹ ì—†ì´ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¡œ ì—­í• ì„ ê³ ì •í•œ ë¡œì»¬ ì±„íŒ…")
        chat = gr.ChatInterface(
            fn=lambda history, message, temperature: smart_home_chat(history, message, temperature),
            additional_inputs=[gr.Slider(0.0, 1.0, 0.2, step=0.05, label="Temperature")],
            title="ìŠ¤ë§ˆíŠ¸ í™ˆ ì–´ì‹œìŠ¤í„´íŠ¸",
            description="DeepSeek-R1 ë¡œì»¬ ì±„íŒ…",
            retry_btn=None, undo_btn=None, clear_btn="ëŒ€í™” ì´ˆê¸°í™”"
        )

    with gr.Tab("6) ë‰´ìŠ¤ ì •ë°€ë„ í‰ê°€(ì˜µì…˜ íŒŒì¼)"):
        gr.Markdown("HuffPost ìœ ì‚¬ ë°ì´í„°(data/news.json)ë¥¼ ì—…ë¡œë“œí•˜ë©´ ê°„ë‹¨ ì •í™•ë„ ì¸¡ì •")
        jf = gr.File(label="data/news.json ì—…ë¡œë“œ")
        n = gr.Slider(5, 200, 20, step=5, label="í‰ê°€ ìƒ˜í”Œ í¬ê¸°")
        btn6 = gr.Button("í‰ê°€ ì‹¤í–‰")
        score = gr.Textbox(label="ì •ë°€ë„(=ì •í™•ë„)")
        report = gr.Textbox(label="ìƒ˜í”Œ ê²°ê³¼(ì¼ë¶€)", lines=12)
        btn6.click(evaluate_precision, inputs=[jf, n], outputs=[score, report])

    with gr.Tab("7) JSONL í¬ë§· ê²€ì¦"):
        gr.Markdown("ë©”ì‹œì§€ í¬ë§·/ì—­í•  êµ¬ì¡° ê²€ì¦ê¸° (ê°„ë‹¨ ë£°)")
        jlf = gr.File(label="dataset.jsonl ì—…ë¡œë“œ")
        btn7 = gr.Button("ê²€ì¦")
        out7 = gr.Textbox(label="ê²€ì¦ ê²°ê³¼", lines=12)
        btn7.click(validate_jsonl, inputs=[jlf], outputs=[out7])

if __name__ == "__main__":
    # ë¡œì»¬ ì‹¤í–‰
    demo.launch(server_name="0.0.0.0", server_port=7860)


