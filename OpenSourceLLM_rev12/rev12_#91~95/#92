#92
"""
OpenCode ë³€í™˜ë³¸: OpenAI Images API â†’ Diffusers(Stable Diffusion XL) + Gradio (ë¡œì»¬)
-----------------------------------------------------------------------
ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” OpenAIì˜ client.images.generate / edit / variations ì˜ˆì œë¥¼
**ì™„ì „ ë¡œì»¬ íŒŒì´í”„ë¼ì¸**ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.
- Textâ†’Image: StableDiffusionXLPipeline
- Imageâ†’Image(ìŠ¤íƒ€ì¼ ë³€í™˜): StableDiffusionXLImg2ImgPipeline
- Inpaint(ë§ˆìŠ¤í¬ í¸ì§‘): StableDiffusionXLInpaintPipeline
- (ì„ íƒ) Ollama + deepseek-r1ë¡œ í”„ë¡¬í”„íŠ¸ í´ë¦¬ì‹œ(ê³ ë„í™”)
- Gradio GUI + ë°°ì¹˜ ìƒì„±, ì‹œë“œ ê³ ì •, í•´ìƒë„/ìŠ¤í…/CFG ì¡°ì ˆ

ì„¤ì¹˜ ì•ˆë‚´
  pip install -U diffusers transformers accelerate safetensors torch torchvision --index-url https://download.pytorch.org/whl/cu121
  pip install -U gradio pillow numpy
  # (ì„ íƒ) ë” ë¹ ë¥´ê²Œ: xformers
  pip install -U xformers --index-url https://download.pytorch.org/whl/cu121

ëª¨ë¸ ì¤€ë¹„(ìµœì´ˆ 1íšŒ ìë™ ë‹¤ìš´ë¡œë“œ)
  - base:     stabilityai/stable-diffusion-xl-base-1.0
  - refiner:  stabilityai/stable-diffusion-xl-refiner-1.0 (ì„ íƒ)

ì‹¤í–‰
  python imggen_diffusers_gradio.py

í´ë”
  - outputs/ ì— ìƒì„± ì´ë¯¸ì§€ ìë™ ì €ì¥
"""
from __future__ import annotations
import os, time, math, random
from typing import List, Tuple, Optional
from dataclasses import dataclass

import torch
from PIL import Image
import numpy as np
import gradio as gr

from diffusers import (
    StableDiffusionXLPipeline,
    StableDiffusionXLImg2ImgPipeline,
    StableDiffusionXLInpaintPipeline,
)

# ===============================
# í™˜ê²½ì„¤ì •
# ===============================
MODEL_BASE = os.environ.get("SDXL_BASE", "stabilityai/stable-diffusion-xl-base-1.0")
MODEL_REFINER = os.environ.get("SDXL_REFINER", "stabilityai/stable-diffusion-xl-refiner-1.0")
USE_REFINER = os.environ.get("USE_REFINER", "0") == "1"  # ì„±ëŠ¥ ì—¬ìœ  ìˆì„ ë•Œë§Œ

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
DTYPE = torch.float16 if (DEVICE == "cuda") else torch.float32
OUT_DIR = os.path.abspath("outputs")
os.makedirs(OUT_DIR, exist_ok=True)

# (ì„ íƒ) Ollamaë¡œ í”„ë¡¬í”„íŠ¸ í´ë¦¬ì‹œ
OLLAMA_HOST = os.environ.get("OLLAMA_HOST", "http://127.0.0.1:11434")
OLLAMA_MODEL = os.environ.get("OLLAMA_MODEL", "deepseek-r1:7b")

# ===============================
# ëª¨ë¸ ë¡œë“œ (ì§€ì—° ë¡œë“œ)
# ===============================
@dataclass
class Pipelines:
    txt2img: Optional[StableDiffusionXLPipeline] = None
    img2img: Optional[StableDiffusionXLImg2ImgPipeline] = None
    inpaint: Optional[StableDiffusionXLInpaintPipeline] = None


PIPES = Pipelines()


def _common_opt(pipe):
    pipe.to(DEVICE, dtype=DTYPE)
    pipe.safety_checker = None  # í•„ìš” ì‹œ ìì²´ í•„í„° ì ìš©
    pipe.enable_attention_slicing()
    if DEVICE == "cuda":
        try:
            pipe.enable_xformers_memory_efficient_attention()
        except Exception:
            pass
    return pipe


def get_txt2img():
    if PIPES.txt2img is None:
        PIPES.txt2img = _common_opt(StableDiffusionXLPipeline.from_pretrained(MODEL_BASE, use_safetensors=True))
    return PIPES.txt2img


def get_img2img():
    if PIPES.img2img is None:
        PIPES.img2img = _common_opt(StableDiffusionXLImg2ImgPipeline.from_pretrained(MODEL_BASE, use_safetensors=True))
    return PIPES.img2img


def get_inpaint():
    if PIPES.inpaint is None:
        PIPES.inpaint = _common_opt(StableDiffusionXLInpaintPipeline.from_pretrained(MODEL_BASE, use_safetensors=True))
    return PIPES.inpaint

# (ì„ íƒ) Refiner í˜¸ì¶œ

def run_refiner(images: List[Image.Image], steps: int = 20, cfg: float = 7.0) -> List[Image.Image]:
    if not USE_REFINER:
        return images
    ref = _common_opt(StableDiffusionXLPipeline.from_pretrained(MODEL_REFINER, use_safetensors=True))
    refined = []
    for im in images:
        out = ref(prompt="highly detailed, photo quality", image=im, num_inference_steps=steps, guidance_scale=cfg)
        refined.append(out.images[0])
    del ref
    return refined

# ===============================
# í”„ë¡¬í”„íŠ¸ í´ë¦¬ì‹œ (Ollama, ì„ íƒ)
# ===============================
import requests

def polish_prompt(prompt: str, negative: str = "") -> str:
    """Ollama(DeepSeek-R1)ë¡œ ê°„ë‹¨ í”„ë¡¬í”„íŠ¸ ê³ ë„í™”. ë„¤íŠ¸ì›Œí¬ ì‹¤íŒ¨ ì‹œ ì›ë¬¸ ë°˜í™˜"""
    try:
        sys = (
            "You are a prompt engineer. Improve the user's prompt for Stable Diffusion XL. "
            "Keep the original content and add helpful style/lighting/lens details in English. "
            "Return a single line under 120 words."
        )
        payload = {
            "model": OLLAMA_MODEL,
            "messages": [
                {"role": "system", "content": sys},
                {"role": "user", "content": f"Prompt: {prompt}\nNegative: {negative}"},
            ],
            "stream": False,
            "options": {"temperature": 0.2},
        }
        r = requests.post(f"{OLLAMA_HOST.rstrip('/')}/api/chat", json=payload, timeout=30)
        r.raise_for_status()
        txt = r.json().get("message", {}).get("content", "").strip()
        # DeepSeek-R1ì€ <think>ë¥¼ í¬í•¨í•  ìˆ˜ ìˆìŒ â†’ ì œê±°
        import re
        txt = re.sub(r"<think>.*?</think>", "", txt, flags=re.DOTALL).strip()
        return txt or prompt
    except Exception:
        return prompt

# ===============================
# ê³µí†µ ë„ìš°ë¯¸
# ===============================

def parse_size(size_label: str) -> Tuple[int, int]:
    # "512x512" â†’ (512, 512)
    try:
        w, h = size_label.lower().split("x")
        return int(w), int(h)
    except Exception:
        return 1024, 1024  # SDXL ê¸°ë³¸ í•´ìƒë„


def save_images(imgs: List[Image.Image], prefix: str) -> List[str]:
    ts = time.strftime("%Y%m%d_%H%M%S")
    paths = []
    for i, im in enumerate(imgs):
        path = os.path.join(OUT_DIR, f"{prefix}_{ts}_{i:02d}.png")
        im.save(path)
        paths.append(path)
    return paths

# ===============================
# ìƒì„± í•¨ìˆ˜: Textâ†’Image
# ===============================

def generate_text2img(prompt: str, negative: str, size: str, steps: int, cfg: float, n: int, seed: int, use_ollama: bool):
    if not prompt.strip():
        raise gr.Error("í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    w, h = parse_size(size)
    if use_ollama:
        prompt = polish_prompt(prompt, negative)
    pipe = get_txt2img()
    g = torch.Generator(device=DEVICE).manual_seed(seed) if seed >= 0 else None
    out = pipe(
        prompt=prompt,
        negative_prompt=(negative or None),
        width=w, height=h,
        num_inference_steps=steps,
        guidance_scale=cfg,
        num_images_per_prompt=n,
        generator=g,
    )
    imgs = out.images
    imgs = run_refiner(imgs)
    paths = save_images(imgs, prefix="txt2img")
    return imgs, "\n".join(paths)

# ===============================
# ìƒì„± í•¨ìˆ˜: Imageâ†’Image (Variations ìŠ¤íƒ€ì¼)
# ===============================

def generate_img2img(init_img: Image.Image, prompt: str, strength: float, negative: str, steps: int, cfg: float, n: int, seed: int, use_ollama: bool):
    if init_img is None:
        raise gr.Error("ì›ë³¸ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.")
    if use_ollama and prompt.strip():
        prompt = polish_prompt(prompt, negative)
    pipe = get_img2img()
    g = torch.Generator(device=DEVICE).manual_seed(seed) if seed >= 0 else None
    out = pipe(
        prompt=(prompt or ""),
        image=init_img,
        strength=float(strength),  # 0.0~1.0, 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì›ë³¸ ë³´ì¡´
        negative_prompt=(negative or None),
        guidance_scale=cfg,
        num_inference_steps=steps,
        num_images_per_prompt=n,
        generator=g,
    )
    imgs = out.images
    imgs = run_refiner(imgs)
    paths = save_images(imgs, prefix="img2img")
    return imgs, "\n".join(paths)

# ===============================
# ìƒì„± í•¨ìˆ˜: Inpaint (Edit)
# ===============================

def generate_inpaint(init_img: Image.Image, mask_img: Image.Image, prompt: str, negative: str, steps: int, cfg: float, n: int, seed: int, use_ollama: bool):
    if init_img is None or mask_img is None:
        raise gr.Error("ì›ë³¸/ë§ˆìŠ¤í¬ ì´ë¯¸ì§€ë¥¼ ëª¨ë‘ ì—…ë¡œë“œí•˜ì„¸ìš”.")
    # SDXL InpaintëŠ” í°ìƒ‰(255) ë¶€ë¶„ì„ ìˆ˜ì • ëŒ€ìƒìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    if use_ollama and prompt.strip():
        prompt = polish_prompt(prompt, negative)
    pipe = get_inpaint()
    g = torch.Generator(device=DEVICE).manual_seed(seed) if seed >= 0 else None
    out = pipe(
        prompt=(prompt or ""),
        image=init_img,
        mask_image=mask_img,
        negative_prompt=(negative or None),
        guidance_scale=cfg,
        num_inference_steps=steps,
        num_images_per_prompt=n,
        generator=g,
    )
    imgs = out.images
    imgs = run_refiner(imgs)
    paths = save_images(imgs, prefix="inpaint")
    return imgs, "\n".join(paths)

# ===============================
# Gradio UI
# ===============================
size_choices = ["512x512", "768x768", "1024x1024", "1152x896", "896x1152"]

with gr.Blocks(title="Stable Diffusion XL â€” OpenCode", theme=gr.themes.Soft()) as demo:
    gr.Markdown("""
    # ğŸ–Œï¸ Stable Diffusion XL (ë¡œì»¬) â€” OpenCode
    OpenAI Images API ì—†ì´ **ì™„ì „ ë¡œì»¬**ë¡œ í…ìŠ¤íŠ¸ìƒì„±/ì´ë¯¸ì§€í¸ì§‘/ë³€í˜•ì„ ì œê³µí•©ë‹ˆë‹¤.
    """)

    with gr.Tabs():
        with gr.TabItem("Text â†’ Image"):
            t_prompt = gr.Textbox(label="Prompt", value="A beautiful landscape, golden hour, ultrarealistic")
            t_negative = gr.Textbox(label="Negative Prompt", value="low quality, blurry, distorted")
            with gr.Row():
                t_size = gr.Dropdown(size_choices, value="1024x1024", label="Size")
                t_steps = gr.Slider(10, 80, value=30, step=1, label="Steps")
                t_cfg = gr.Slider(1.0, 12.0, value=7.0, step=0.5, label="CFG")
            with gr.Row():
                t_n = gr.Slider(1, 6, value=1, step=1, label="Batch (n)")
                t_seed = gr.Number(value=-1, label="Seed(-1=random)")
                t_polish = gr.Checkbox(value=False, label="Ollamaë¡œ í”„ë¡¬í”„íŠ¸ ê³ ë„í™”")
            t_btn = gr.Button("Generate", variant="primary")
            t_gallery = gr.Gallery(label="Results", columns=3, height=512)
            t_paths = gr.Textbox(label="Saved Files")
            t_btn.click(generate_text2img, inputs=[t_prompt, t_negative, t_size, t_steps, t_cfg, t_n, t_seed, t_polish], outputs=[t_gallery, t_paths])

        with gr.TabItem("Image â†’ Image (Variations)"):
            i_img = gr.Image(label="Init Image", type="pil")
            i_prompt = gr.Textbox(label="Prompt (ì„ íƒ)", value="")
            i_strength = gr.Slider(0.05, 1.0, value=0.35, step=0.05, label="Denoise Strength")
            i_negative = gr.Textbox(label="Negative Prompt", value="low quality, blurry, distorted")
            with gr.Row():
                i_steps = gr.Slider(10, 80, value=30, step=1, label="Steps")
                i_cfg = gr.Slider(1.0, 12.0, value=7.0, step=0.5, label="CFG")
            with gr.Row():
                i_n = gr.Slider(1, 6, value=2, step=1, label="Batch (n)")
                i_seed = gr.Number(value=-1, label="Seed(-1=random)")
                i_polish = gr.Checkbox(value=False, label="Ollama í”„ë¡¬í”„íŠ¸")
            i_btn = gr.Button("Generate", variant="primary")
            i_gallery = gr.Gallery(label="Results", columns=3, height=512)
            i_paths = gr.Textbox(label="Saved Files")
            i_btn.click(generate_img2img, inputs=[i_img, i_prompt, i_strength, i_negative, i_steps, i_cfg, i_n, i_seed, i_polish], outputs=[i_gallery, i_paths])

        with gr.TabItem("Inpaint (Edit)"):
            p_img = gr.Image(label="Init Image", type="pil")
            p_mask = gr.Image(label="Mask (í°ìƒ‰=ìˆ˜ì •)" , type="pil")
            p_prompt = gr.Textbox(label="Prompt", value="A group of people hiking in green forest between trees")
            p_negative = gr.Textbox(label="Negative Prompt", value="low quality, artifacts")
            with gr.Row():
                p_steps = gr.Slider(10, 80, value=30, step=1, label="Steps")
                p_cfg = gr.Slider(1.0, 12.0, value=7.0, step=0.5, label="CFG")
            with gr.Row():
                p_n = gr.Slider(1, 4, value=1, step=1, label="Batch (n)")
                p_seed = gr.Number(value=-1, label="Seed(-1=random)")
                p_polish = gr.Checkbox(value=False, label="Ollama í”„ë¡¬í”„íŠ¸")
            p_btn = gr.Button("Generate", variant="primary")
            p_gallery = gr.Gallery(label="Results", columns=3, height=512)
            p_paths = gr.Textbox(label="Saved Files")
            p_btn.click(generate_inpaint, inputs=[p_img, p_mask, p_prompt, p_negative, p_steps, p_cfg, p_n, p_seed, p_polish], outputs=[p_gallery, p_paths])

    gr.Markdown("""
    ### ì‚¬ìš© íŒ
    - **Variations**ëŠ” Imageâ†’Image íƒ­ì—ì„œ `strengthâ‰ˆ0.2~0.4`ë¡œ ì›ë³¸ ëŠë‚Œì„ ìœ ì§€í•˜ë©° ë³€ì£¼í•©ë‹ˆë‹¤.
    - **Inpaint**ëŠ” ë§ˆìŠ¤í¬ ì´ë¯¸ì§€ì—ì„œ **í°ìƒ‰(255)** ë¶€ë¶„ì´ ìˆ˜ì • ëŒ€ìƒì…ë‹ˆë‹¤. (ê²€ì •=ë³´ì¡´)
    - VRAMì´ ë¶€ì¡±í•˜ë©´ í•´ìƒë„ë¥¼ ë‚®ì¶”ê±°ë‚˜ Stepsë¥¼ ì¤„ì´ì„¸ìš”.
    - ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ì›í•˜ë©´ **Seed**ë¥¼ 0 ì´ìƒì˜ ì •ìˆ˜ë¡œ ê³ ì •í•˜ì„¸ìš”.
    - Refinerë¥¼ ì“°ë ¤ë©´ `USE_REFINER=1` í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”.
    """)

if __name__ == "__main__":
    demo.launch()


# pip install -U diffusers transformers accelerate safetensors torch torchvision --index-url https://download.pytorch.org/whl/cu121
# pip install -U gradio pillow numpy
# (ì„ íƒ) ì†ë„ â†‘
# pip install -U xformers --index-url https://download.pytorch.org/whl/cu121
