#95
# OpenSource_Image_Gen_and_Analysis.py
# -----------------------------------------------------------------------------
# âœ… ëª©ì 
#   - OpenAI ì „ìš© ì´ë¯¸ì§€ ìƒì„±/ë¶„ì„ ì½”ë“œë¥¼ **ë¡œì»¬ ì˜¤í”ˆì†ŒìŠ¤ ìŠ¤íƒ**ìœ¼ë¡œ ë³€í™˜
#   - API Key ì—†ì´ ë™ì‘ (ë¡œì»¬ **Ollama** + **Diffusers/Stable Diffusion**)
#   - ê°„í¸í•œ **Gradio GUI** ì œê³µ (í…ìŠ¤íŠ¸â†’ì´ë¯¸ì§€ / ë‹¨ì¼ ì´ë¯¸ì§€ ì„¤ëª… / ë‹¤ì¤‘ ì´ë¯¸ì§€ ì„¤ëª…)
#
# âœ… êµ¬ì„±
#   1) í…ìŠ¤íŠ¸â†’ì´ë¯¸ì§€: Hugging Face Diffusersì˜ SDXL Turbo(ê¸°ë³¸) ë˜ëŠ” Stable Diffusion ì‚¬ìš©
#   2) ì´ë¯¸ì§€â†’í…ìŠ¤íŠ¸(ì„¤ëª…): Ollama ë¹„ì „ ëª¨ë¸(ì˜ˆ: "llama3.2-vision" ë˜ëŠ” "llava:13b")
#   3) ì—¬ëŸ¬ ì´ë¯¸ì§€ ë™ì‹œ ë¶„ì„: ë¹„ì „ ëª¨ë¸ì— ë‹¤ì¤‘ ì´ë¯¸ì§€ ì…ë ¥
#
# âœ… ì¤€ë¹„ë¬¼
#   - Ollama ì‹¤í–‰: `ollama serve`
#   - (ìµœì´ˆ 1íšŒ) ëª¨ë¸ ë‹¤ìš´ë¡œë“œ:
#       * ë¹„ì „ LLM: `ollama pull llama3.2-vision`  (ë˜ëŠ” `ollava:13b`)
#       * í…ìŠ¤íŠ¸2ì´ë¯¸ì§€(ë¡œì»¬): íŒŒì´ì¬ì—ì„œ ìë™ ë‹¤ìš´ë¡œë“œ (sdxl-turbo ê¶Œì¥)
#         - GPU ê¶Œì¥, CPUë„ ê°€ëŠ¥(ëŠë¦´ ìˆ˜ ìˆìŒ)
#
# âœ… ì‹¤í–‰
#   - `python OpenSource_Image_Gen_and_Analysis.py`
#   - ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:7860 ì ‘ì†
# -----------------------------------------------------------------------------

import os
import io
import re
import base64
import requests
from typing import List, Tuple, Optional

import gradio as gr
from PIL import Image

# Diffusers (Stable Diffusion / SDXL)
import torch
from diffusers import AutoPipelineForText2Image

# -----------------------------
# í™˜ê²½ ë³€ìˆ˜/ìƒìˆ˜
# -----------------------------
OLLAMA_HOST = os.environ.get("OLLAMA_HOST", "http://localhost:11434")
VISION_MODEL = os.environ.get("VISION_MODEL", "llama3.2-vision")  # ëŒ€ì•ˆ: "llava:13b"
TI_MODEL_REPO = os.environ.get("TI_MODEL_REPO", "stabilityai/sdxl-turbo")  # í…ìŠ¤íŠ¸â†’ì´ë¯¸ì§€ ê¸°ë³¸

# -----------------------------
# ê³µìš© ìœ í‹¸
# -----------------------------

def fetch_image_from_url(url: str) -> Image.Image:
    """URLì—ì„œ ì´ë¯¸ì§€ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ì—¬ PIL Imageë¡œ ë°˜í™˜."""
    resp = requests.get(url, timeout=60)
    resp.raise_for_status()
    return Image.open(io.BytesIO(resp.content)).convert("RGB")


def pil_to_base64(img: Image.Image, fmt: str = "PNG") -> str:
    """PIL ì´ë¯¸ì§€ë¥¼ base64 ë¬¸ìì—´ë¡œ ë³€í™˜ (Ollama visionì— ì „ë‹¬ìš©)."""
    buf = io.BytesIO()
    img.save(buf, format=fmt)
    return base64.b64encode(buf.getvalue()).decode("utf-8")


# -----------------------------
# Ollama Vision í˜¸ì¶œ ë˜í¼ (ì´ë¯¸ì§€ ì„¤ëª…)
# -----------------------------

def ollama_vision_describe(prompt: str, images: List[Image.Image], model: str = VISION_MODEL,
                            temperature: float = 0.2, max_tokens: int = 512) -> str:
    """Ollama ë¹„ì „ ëª¨ë¸ì— ì´ë¯¸ì§€ ë°°ì—´ê³¼ í”„ë¡¬í”„íŠ¸ë¥¼ ì „ë‹¬í•´ ì„¤ëª… ìƒì„±."""
    url = f"{OLLAMA_HOST}/api/generate"
    b64_list = [pil_to_base64(im) for im in images]
    payload = {
        "model": model,
        "prompt": prompt,
        "images": b64_list,       # OllamaëŠ” base64 ì´ë¯¸ì§€ ë°°ì—´ì„ ì§€ì›
        "stream": False,
        "options": {
            "temperature": float(temperature),
            "num_predict": int(max_tokens),
        },
    }
    try:
        r = requests.post(url, json=payload, timeout=180)
        r.raise_for_status()
        return r.json().get("response", "").strip()
    except Exception as e:
        return f"[ì˜¤ë¥˜] Ollama vision í˜¸ì¶œ ì‹¤íŒ¨: {e}"


# -----------------------------
# Diffusers: í…ìŠ¤íŠ¸ â†’ ì´ë¯¸ì§€ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” (ì§€ì—° ë¡œë”©)
# -----------------------------
_t2i_pipe: Optional[AutoPipelineForText2Image] = None

def get_t2i_pipeline(model_repo: str = TI_MODEL_REPO) -> AutoPipelineForText2Image:
    global _t2i_pipe
    if _t2i_pipe is not None:
        return _t2i_pipe

    device = "cuda" if torch.cuda.is_available() else "cpu"
    dtype = torch.float16 if torch.cuda.is_available() else torch.float32
    pipe = AutoPipelineForText2Image.from_pretrained(
        model_repo,
        torch_dtype=dtype,
        variant="fp16" if torch.cuda.is_available() else None,
    )
    pipe = pipe.to(device)

    # SDXL TurboëŠ” guidance_scale 0.0~1.5, steps 1~4 ê¶Œì¥
    # ì¼ë°˜ SDXL/SD1.5ëŠ” 20~30 steps + guidance 7~9 ê¶Œì¥
    _t2i_pipe = pipe
    return _t2i_pipe


def txt2img(prompt: str, width: int = 768, height: int = 768,
            steps: int = 4, guidance: float = 0.0,
            model_repo: str = TI_MODEL_REPO, seed: Optional[int] = None,
            num_images: int = 1) -> List[Image.Image]:
    """Diffusers ë¡œì»¬ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ í…ìŠ¤íŠ¸ì—ì„œ ì´ë¯¸ì§€ ìƒì„±."""
    if not prompt.strip():
        raise ValueError("í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")

    pipe = get_t2i_pipeline(model_repo)
    generator = None
    if seed is not None:
        generator = torch.Generator(device=pipe.device).manual_seed(int(seed))

    images = []
    for _ in range(int(num_images)):
        out = pipe(
            prompt=prompt,
            width=int(width), height=int(height),
            num_inference_steps=int(steps), guidance_scale=float(guidance),
            generator=generator,
        )
        images.append(out.images[0])
    return images


# -----------------------------
# Gradio í•¸ë“¤ëŸ¬ë“¤
# -----------------------------

# 1) í…ìŠ¤íŠ¸ â†’ ì´ë¯¸ì§€

def ui_txt2img(prompt: str, size: str, steps: int, guidance: float,
               model_repo: str, seed: str, n: int):
    try:
        w, h = map(int, size.lower().split("x"))
        seed_val = int(seed) if seed and seed.strip() else None
        imgs = txt2img(prompt, width=w, height=h, steps=steps, guidance=guidance,
                       model_repo=model_repo, seed=seed_val, num_images=n)
        return imgs, f"ìƒì„± ì™„ë£Œ: {len(imgs)}ì¥"
    except Exception as e:
        return None, f"[ì˜¤ë¥˜] í…ìŠ¤íŠ¸â†’ì´ë¯¸ì§€ ì‹¤íŒ¨: {e}"


# 2) ë‹¨ì¼ ì´ë¯¸ì§€ ì„¤ëª… (ì—…ë¡œë“œ ë˜ëŠ” URL)

def ui_describe_single(image: Image.Image, url: str, prompt: str, model: str, temperature: float, max_tokens: int):
    try:
        imgs = []
        if image is not None:
            imgs.append(image)
        if url and url.strip():
            imgs.append(fetch_image_from_url(url.strip()))
        if not imgs:
            return "[ê²½ê³ ] ì´ë¯¸ì§€ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê±°ë‚˜ URLì„ ì…ë ¥í•˜ì„¸ìš”."
        p = prompt.strip() or "ì´ ì´ë¯¸ì§€ì˜ í•µì‹¬ ìš”ì†Œë¥¼ ê°„ê²°íˆ ì„¤ëª…í•´ ì£¼ì„¸ìš”."
        ans = ollama_vision_describe(p, imgs, model=model, temperature=temperature, max_tokens=max_tokens)
        return ans
    except Exception as e:
        return f"[ì˜¤ë¥˜] ë‹¨ì¼ ì´ë¯¸ì§€ ì„¤ëª… ì‹¤íŒ¨: {e}"


# 3) ë‹¤ì¤‘ ì´ë¯¸ì§€ ì„¤ëª… (ì—…ë¡œë“œ ì—¬ëŸ¬ ì¥ ë˜ëŠ” URL ì—¬ëŸ¬ ì¤„)

def ui_describe_multi(images: list, urls_text: str, prompt: str, model: str, temperature: float, max_tokens: int):
    try:
        imgs: List[Image.Image] = []
        if images:
            for im in images:
                if im is not None:
                    imgs.append(im)
        if urls_text and urls_text.strip():
            for line in urls_text.splitlines():
                u = line.strip()
                if not u:
                    continue
                imgs.append(fetch_image_from_url(u))
        if not imgs:
            return "[ê²½ê³ ] ì´ë¯¸ì§€(ë“¤)ë¥¼ ì—…ë¡œë“œí•˜ê±°ë‚˜ URL ëª©ë¡ì„ ì…ë ¥í•˜ì„¸ìš”."
        p = prompt.strip() or "ë‹¤ìŒ ì´ë¯¸ì§€ë“¤ì˜ ê³µí†µì /ì°¨ì´ì /ì£¼ìš” íŠ¹ì§•ì„ ì´ˆë³´ìì—ê²Œ ì„¤ëª…í•´ ì£¼ì„¸ìš”."
        ans = ollama_vision_describe(p, imgs, model=model, temperature=temperature, max_tokens=max_tokens)
        return ans
    except Exception as e:
        return f"[ì˜¤ë¥˜] ë‹¤ì¤‘ ì´ë¯¸ì§€ ì„¤ëª… ì‹¤íŒ¨: {e}"


# -----------------------------
# Gradio UI êµ¬ì„±
# -----------------------------
with gr.Blocks(title="OpenCode Â· ë¡œì»¬ ì´ë¯¸ì§€ ìƒì„±/ë¶„ì„", theme=gr.themes.Soft()) as demo:
    gr.Markdown(
        """
        # ğŸ§© OpenCode Â· ë¡œì»¬ ì´ë¯¸ì§€ ìƒì„±/ë¶„ì„ (Ollama + Diffusers)
        - OpenAI API ì—†ì´ **ë¡œì»¬**ì—ì„œ ì´ë¯¸ì§€ ìƒì„±/ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        - í…ìŠ¤íŠ¸â†’ì´ë¯¸ì§€: Diffusers (ê¸°ë³¸: SDXL Turbo)
        - ì´ë¯¸ì§€ ì„¤ëª…: Ollama ë¹„ì „ ëª¨ë¸ (ê¸°ë³¸: `llama3.2-vision`)
        """
    )

    # TAB 1: í…ìŠ¤íŠ¸ â†’ ì´ë¯¸ì§€
    with gr.Tab("1) í…ìŠ¤íŠ¸ â†’ ì´ë¯¸ì§€"):
        gr.Markdown("SDXL Turboë¥¼ ê¸°ë³¸ìœ¼ë¡œ ë¹ ë¥´ê²Œ ìƒì„± (ê¶Œì¥: steps 1~4, guidance 0.0~1.5)")
        with gr.Row():
            prompt = gr.Textbox(label="í”„ë¡¬í”„íŠ¸", value="A beautiful futuristic city at sunset with neon lights and flying cars")
        with gr.Row():
            size = gr.Dropdown(choices=["512x512", "768x768", "1024x1024"], value="768x768", label="ì´ë¯¸ì§€ í¬ê¸°")
            steps = gr.Slider(1, 50, value=4, step=1, label="Steps")
            guidance = gr.Slider(0.0, 12.0, value=0.0, step=0.1, label="Guidance Scale")
        with gr.Row():
            model_repo = gr.Textbox(label="Diffusers ëª¨ë¸ ë ˆí¬ (ë³€ê²½ ê°€ëŠ¥)", value=TI_MODEL_REPO)
            seed = gr.Textbox(label="Seed (ì˜µì…˜)")
            nimg = gr.Slider(1, 4, value=1, step=1, label="ìƒì„± ê°œìˆ˜")
        run_btn = gr.Button("ì´ë¯¸ì§€ ìƒì„±")
        gallery = gr.Gallery(label="ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°").style(grid=(2,))
        status1 = gr.Textbox(label="ìƒíƒœ ë©”ì‹œì§€", interactive=False)
        run_btn.click(ui_txt2img, inputs=[prompt, size, steps, guidance, model_repo, seed, nimg], outputs=[gallery, status1])

    # TAB 2: ë‹¨ì¼ ì´ë¯¸ì§€ ì„¤ëª…
    with gr.Tab("2) ë‹¨ì¼ ì´ë¯¸ì§€ ì„¤ëª…"):
        gr.Markdown("ì—…ë¡œë“œ ë˜ëŠ” URLë¡œ 1~2ì¥ì˜ ì´ë¯¸ì§€ë¥¼ ì…ë ¥í•´ ì„¤ëª…ì„ ìƒì„±")
        with gr.Row():
            up = gr.Image(type="pil", label="ì´ë¯¸ì§€ ì—…ë¡œë“œ (ì„ íƒ)")
            url = gr.Textbox(label="ì´ë¯¸ì§€ URL (ì„ íƒ)", value="https://ldjwj.github.io/SOURCE_VideoSoraGen3Etc/img/cat_computer.png")
        with gr.Row():
            vprompt = gr.Textbox(label="ì„¤ëª… í”„ë¡¬í”„íŠ¸", value="ì´ ì´ë¯¸ì§€ì— ë¬´ì—‡ì´ ìˆëŠ”ì§€ ìì„¸íˆ ì„¤ëª…í•´ ì£¼ì„¸ìš”.")
        with gr.Row():
            vmodel = gr.Textbox(label="Ollama ë¹„ì „ ëª¨ë¸", value=VISION_MODEL)
            vtemp = gr.Slider(0.0, 1.0, value=0.2, step=0.05, label="Temperature")
            vmax = gr.Slider(64, 1024, value=512, step=16, label="Max Tokens")
        run2 = gr.Button("ì„¤ëª… ìƒì„±")
        out2 = gr.Textbox(label="ì„¤ëª… ê²°ê³¼", lines=10)
        run2.click(ui_describe_single, inputs=[up, url, vprompt, vmodel, vtemp, vmax], outputs=[out2])

    # TAB 3: ë‹¤ì¤‘ ì´ë¯¸ì§€ ì„¤ëª…
    with gr.Tab("3) ë‹¤ì¤‘ ì´ë¯¸ì§€ ì„¤ëª…"):
        gr.Markdown("ì—¬ëŸ¬ ì¥ì„ ì—…ë¡œë“œí•˜ê±°ë‚˜, URLì„ ì¤„ë°”ê¿ˆìœ¼ë¡œ ì—¬ëŸ¬ ê°œ ì…ë ¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        imgs = gr.File(file_count="multiple", type="filepath", label="ì—¬ëŸ¬ ì´ë¯¸ì§€ ì—…ë¡œë“œ(ì„ íƒ)")  # ë‚´ë¶€ ë³€í™˜ìš© ëŒ€ì²´
        # Gradioì˜ ë‹¤ì¤‘ Image ì»´í¬ë„ŒíŠ¸ê°€ ìƒì„±ë¬¼ ì €ì¥ ì´ìŠˆê°€ ìˆì–´, ì•„ë˜ì™€ ê°™ì´ ì—…ë¡œë“œ í›„ PILë¡œ ì¬ë¡œë“œ
        up_multi = gr.Image(type="pil", sources=["upload"], label="(ì˜µì…˜) ì¶”ê°€ ì—…ë¡œë“œ: ë§ˆì§€ë§‰ 1ì¥", visible=False)
        urls_text = gr.Textbox(label="ì´ë¯¸ì§€ URL ëª©ë¡(ì¤„ë°”ê¿ˆ êµ¬ë¶„)", lines=5,
                               value="\n".join([
                                   "https://ldjwj.github.io/SOURCE_VideoSoraGen3Etc/img/bird_Gen4.png",
                                   "https://ldjwj.github.io/SOURCE_VideoSoraGen3Etc/img/cat_computer.png",
                                   "https://ldjwj.github.io/SOURCE_VideoSoraGen3Etc/img/edu_webtoon.png",
                                   "https://ldjwj.github.io/SOURCE_VideoSoraGen3Etc/img/truth_01.png",
                               ]))
        mprompt = gr.Textbox(label="ì„¤ëª… í”„ë¡¬í”„íŠ¸", value="ì´ ë„¤ ì´ë¯¸ì§€ë¥¼ ì´ˆë³´ìì—ê²Œ ë¶„ì„í•´ì„œ ì„¤ëª…í•´ ì£¼ì„¸ìš”")
        with gr.Row():
            mmodel = gr.Textbox(label="Ollama ë¹„ì „ ëª¨ë¸", value=VISION_MODEL)
            mtemp = gr.Slider(0.0, 1.0, value=0.2, step=0.05, label="Temperature")
            mmax = gr.Slider(64, 2048, value=800, step=16, label="Max Tokens")
        run3 = gr.Button("ë‹¤ì¤‘ ì„¤ëª… ìƒì„±")
        out3 = gr.Textbox(label="ì„¤ëª… ê²°ê³¼", lines=14)

        def _load_many_and_call(files, prompt, urls, model, temperature, max_tokens):
            # files: íŒŒì¼ ê²½ë¡œ ëª©ë¡ â†’ PIL Image ë¡œë“œ
            pil_list = []
            try:
                if files:
                    for p in files:
                        try:
                            with open(p, "rb") as f:
                                pil_list.append(Image.open(f).convert("RGB"))
                        except Exception:
                            pass
                if urls and urls.strip():
                    for line in urls.splitlines():
                        u = line.strip()
                        if not u:
                            continue
                        pil_list.append(fetch_image_from_url(u))
                if not pil_list:
                    return "[ê²½ê³ ] ì´ë¯¸ì§€(ë“¤) ë˜ëŠ” URL ëª©ë¡ì„ ì…ë ¥í•˜ì„¸ìš”."
                return ollama_vision_describe(prompt or "ì´ë¯¸ì§€ë“¤ì„ ì„¤ëª…í•´ ì£¼ì„¸ìš”.", pil_list, model=model,
                                              temperature=temperature, max_tokens=max_tokens)
            except Exception as e:
                return f"[ì˜¤ë¥˜] ì²˜ë¦¬ ì‹¤íŒ¨: {e}"

        run3.click(_load_many_and_call, inputs=[imgs, mprompt, urls_text, mmodel, mtemp, mmax], outputs=[out3])

if __name__ == "__main__":
    # ê³µìœ /ì™¸ë¶€ ì ‘ì† í•„ìš”ì‹œ server_name="0.0.0.0" ë¡œ ë³€ê²½
    demo.launch(server_name="127.0.0.1", server_port=7860)
