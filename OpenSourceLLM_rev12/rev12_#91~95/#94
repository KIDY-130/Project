#94
# open_agents_app.py
# -----------------------------------------------------------------------------
# OpenAI/AutoGen ì˜ì¡´ ì½”ë“œë¥¼ "ë¡œì»¬ ì˜¤í”ˆì†ŒìŠ¤ LLM (Ollama + DeepSeek-R1)"ë¡œ ë³€í™˜í•œ ì˜ˆì‹œ
# - API Key ë¶ˆí•„ìš” (ë¡œì»¬ Ollama REST API ì‚¬ìš©)
# - ëª¨ë¸:
#     * ì±„íŒ…/ì¶”ë¡ : deepseek-r1:7b  (ì›í•˜ë©´ 8b/14b ë“±ìœ¼ë¡œ êµì²´)
# - GUI: Gradio (íƒ­ 3ê°œ: ë‹¨ì¼ ì–´ì‹œìŠ¤í„´íŠ¸, 2-ì—ì´ì „íŠ¸ ëŒ€í™”, ë§ˆì¼€íŒ… 3-ì—ì´ì „íŠ¸ íŒŒì´í”„ë¼ì¸)
#
# ì¤€ë¹„:
#   1) Ollama ì„¤ì¹˜ í›„ ì‹¤í–‰:  ollama serve
#   2) ëª¨ë¸ ë‹¤ìš´ë¡œë“œ:
#        ollama pull deepseek-r1:7b
#   3) ì´ íŒŒì¼ ì‹¤í–‰: python open_agents_app.py
#   4) ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:7860 ì ‘ì†
# -----------------------------------------------------------------------------

import os
import re
import time
import requests
import gradio as gr

# -----------------------------
# ë¡œì»¬ Ollama ì„¤ì •
# -----------------------------
OLLAMA_HOST = os.environ.get("OLLAMA_HOST", "http://localhost:11434")
LLM_MODEL   = os.environ.get("LLM_MODEL", "deepseek-r1:7b")

# -----------------------------
# Ollama: /api/generate ë˜í¼
# -----------------------------
def ollama_generate(
    prompt: str,
    model: str = LLM_MODEL,
    system: str | None = None,
    temperature: float = 0.7,
    max_tokens: int = 800,
    stop: list[str] | None = None,
) -> str:
    """
    DeepSeek-R1 ë“± ì¼ë°˜ LLMì— ëŒ€í•´ í…ìŠ¤íŠ¸ ìƒì„±(ë¹„ìŠ¤íŠ¸ë¦¬ë°) ìš”ì²­.
    - OpenAI API Key ë¶ˆí•„ìš”.
    - stop í† í°ì´ í•„ìš”í•˜ë©´ ë¦¬ìŠ¤íŠ¸ë¡œ ì „ë‹¬.
    """
    url = f"{OLLAMA_HOST}/api/generate"
    payload = {
        "model": model,
        "prompt": prompt if not system else f"<<SYS>>\n{system}\n<</SYS>>\n\n{prompt}",
        "stream": False,
        "options": {
            "temperature": float(temperature),
            "num_predict": int(max_tokens),
        },
    }
    if stop:
        payload["stop"] = stop

    try:
        resp = requests.post(url, json=payload, timeout=120)
        resp.raise_for_status()
        text = resp.json().get("response", "")
        return text.strip()
    except Exception as e:
        return f"[ì˜¤ë¥˜] Ollama generate ì‹¤íŒ¨: {e}"

# -----------------------------
# ê³µí†µ: ì‹œìŠ¤í…œ/ëŒ€í™” í”„ë¡¬í”„íŠ¸ ë¹Œë”
# -----------------------------
def build_dialog_prompt(history: list[tuple[str, str]], next_speaker: str) -> str:
    """
    history: [(speaker, content), ...]
    next_speaker: ë‹¤ìŒ ë°œí™”ìì˜ ì´ë¦„(ì˜ˆ: "Assistant" / "Alex(Trainer)" ë“±)
    """
    lines = []
    for who, msg in history:
        lines.append(f"{who}: {msg}")
    lines.append(f"{next_speaker}:")
    return "\n".join(lines)

# =====================================================================
# íƒ­ 1) ë‹¨ì¼ ì–´ì‹œìŠ¤í„´íŠ¸ (AutoGenì˜ simple initiate_chat ëŒ€ì²´)
# =====================================================================
ASSISTANT_SYSTEM = (
    "ë‹¹ì‹ ì€ ì§€ì‹ì´ í’ë¶€í•˜ê³  ì¹œì ˆí•œ ì¡°ë ¥ìì…ë‹ˆë‹¤. "
    "ì‚¬ì‹¤ì— ê·¼ê±°í•˜ì—¬ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ í•œêµ­ì–´ë¡œ ë‹µí•˜ì„¸ìš”."
)

def single_assistant_chat(history, user_msg, temperature=0.7, max_tokens=800):
    """
    Gradio ChatInterfaceìš© í•¸ë“¤ëŸ¬.
    - history í˜•ì‹: [(user, bot), ...]
    - user_msg: ì‚¬ìš©ìì˜ ìµœì‹  ì…ë ¥
    """
    # ëŒ€í™” ì´ë ¥ -> (ë°œí™”ì, ë‚´ìš©) ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    conv = []
    for u, b in history:
        if u: conv.append(("User", u))
        if b: conv.append(("Assistant", b))

    # ì‚¬ìš©ì ë°œí™” ì¶”ê°€
    conv.append(("User", user_msg))

    # í”„ë¡¬í”„íŠ¸ êµ¬ì„± ë° ì‘ë‹µ ìƒì„±
    prompt = build_dialog_prompt(conv, next_speaker="Assistant")
    reply = ollama_generate(
        prompt=prompt,
        system=ASSISTANT_SYSTEM,
        temperature=temperature,
        max_tokens=int(max_tokens),
        stop=["\nUser:", "\nassistant:", "\nAssistant:"],
    )
    return reply

# =====================================================================
# íƒ­ 2) 2-ì—ì´ì „íŠ¸ ëŒ€í™” (í”¼íŠ¸ë‹ˆìŠ¤ íŠ¸ë ˆì´ë„ˆ â†” ìš´ë™ ì˜í•™ ì „ë¬¸ê°€)
#       - AutoGenì˜ ConversableAgent ê°„ ëŒ€í™” íë¦„ì„ ë¡œì»¬ì—ì„œ ëª¨ì‚¬
# =====================================================================
TRAINER_SYS = (
    "ë‹¹ì‹ ì˜ ì´ë¦„ì€ Alexì´ë©°, ê·¼ë ¥/ì‹¬í˜ˆê´€/ìœ ì—°ì„± í›ˆë ¨ì— ì „ë¬¸ì„±ì„ ê°€ì§„ í”¼íŠ¸ë‹ˆìŠ¤ íŠ¸ë ˆì´ë„ˆì…ë‹ˆë‹¤. "
    "ì¹œì ˆí•˜ê²Œ, ê·¼ê±°ë¥¼ ê°„ë‹¨íˆ ì œì‹œí•˜ë©° í•œêµ­ì–´ë¡œ ë‹µí•˜ì„¸ìš”."
)
DOCTOR_SYS = (
    "ë‹¹ì‹ ì˜ ì´ë¦„ì€ Dr. Leeì´ë©°, ë¶€ìƒ ì˜ˆë°©ê³¼ ì¬í™œ, ì‹ ì²´ ê±´ê°• ìµœì í™”ì— íŠ¹í™”ëœ ìš´ë™ ìƒë¦¬í•™ìì…ë‹ˆë‹¤. "
    "ì•ˆì „ ìˆ˜ì¹™ì„ ê°•ì¡°í•˜ë©° í•œêµ­ì–´ë¡œ ë‹µí•˜ì„¸ìš”."
)

def two_agent_chat(
    seed_message="Dr. Lee, ì˜¤ëŠ˜ ìš´ë™ì„ ì‹œì‘í•˜ë ¤ëŠ” ì‚¬ëŒì—ê²Œ ì–´ë–¤ ìš´ë™ì„ ì¶”ì²œí• ê¹Œìš”?",
    turns=3,
    temperature_trainer=0.9,
    temperature_doctor=0.7,
    max_tokens=300,
):
    """
    íŠ¸ë ˆì´ë„ˆ(Alex)ì™€ ë‹¥í„°(Dr. Lee)ê°€ ë²ˆê°ˆì•„ê°€ë©° ëŒ€í™”í•˜ëŠ” ì‹œë®¬ë ˆì´ì…˜.
    - seed_message: íŠ¸ë ˆì´ë„ˆê°€ ë‹¥í„°ì—ê²Œ ë¨¼ì € ë³´ë‚´ëŠ” ë©”ì‹œì§€
    - turns: ì´ ë°œí™” íšŸìˆ˜(ì™•ë³µ ê¸°ì¤€ì´ ì•„ë‹ˆë¼ 'ì „ì²´ ë°œí™” ìˆ˜'ë¡œ ì´í•´)
    """
    # ëŒ€í™” ì´ë ¥: (í™”ì, ë‚´ìš©)
    history = [("Alex(Trainer)", seed_message)]
    speaker = "Dr. Lee"

    for t in range(int(turns) - 1):
        prompt = build_dialog_prompt(history, next_speaker=speaker)
        if speaker == "Dr. Lee":
            reply = ollama_generate(
                prompt=prompt,
                system=DOCTOR_SYS,
                temperature=temperature_doctor,
                max_tokens=int(max_tokens),
                stop=["\nAlex(Trainer):", "\nDr. Lee:"],
            )
            history.append(("Dr. Lee", reply))
            speaker = "Alex(Trainer)"
        else:
            reply = ollama_generate(
                prompt=prompt,
                system=TRAINER_SYS,
                temperature=temperature_trainer,
                max_tokens=int(max_tokens),
                stop=["\nAlex(Trainer):", "\nDr. Lee:"],
            )
            history.append(("Alex(Trainer)", reply))
            speaker = "Dr. Lee"

    # ë³´ê¸° ì¢‹ê²Œ í¬ë§·íŒ…
    transcript = []
    for who, msg in history:
        transcript.append(f"**{who}**: {msg}")
    return "\n\n".join(transcript)

# =====================================================================
# íƒ­ 3) ë§ˆì¼€íŒ… 3-ì—ì´ì „íŠ¸ íŒŒì´í”„ë¼ì¸
#       (ì‹œì¥ ì¡°ì‚¬ â†’ ê³ ê° ì„¸ë¶„í™” â†’ í†µí•© ë§ˆì¼€íŒ… ì „ëµ)
# =====================================================================
RESEARCHER_SYS = (
    "ë‹¹ì‹ ì€ ì „ë¬¸ ì‹œì¥ ì¡°ì‚¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œí’ˆ/ì„œë¹„ìŠ¤ì— ëŒ€í•´ ë°ì´í„° ê¸°ë°˜ì˜ "
    "ì‹œì¥ ë™í–¥, ê²½ìŸ êµ¬ë„, íƒ€ê¹ƒ ë‹ˆì¦ˆë¥¼ ê°„ê²°íˆ ì •ë¦¬í•˜ì„¸ìš”. í•œêµ­ì–´ë¡œ 8~12ë¬¸ì¥."
)
SEGMENT_SYS = (
    "ë‹¹ì‹ ì€ ê³ ê° ì„¸ë¶„í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì‹œì¥ ì¡°ì‚¬ ê²°ê³¼ë¥¼ í† ëŒ€ë¡œ "
    "ì„¸ë¶„ ì§‘ë‹¨(3~5ê°œ)ì„ ì •ì˜í•˜ê³ , í˜ë¥´ì†Œë‚˜/í•µì‹¬ ë‹ˆì¦ˆ/ì ‘ì /ë©”ì‹œì§€ë¥¼ ìš”ì•½í•˜ì„¸ìš”. í•œêµ­ì–´."
)
STRATEGIST_SYS = (
    "ë‹¹ì‹ ì€ ë§ˆì¼€íŒ… ì „ëµê°€ì…ë‹ˆë‹¤. ì‹œì¥ ì¡°ì‚¬ì™€ ê³ ê° ì„¸ë¶„í™” ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ "
    "í¬ì§€ì…”ë‹, í•µì‹¬ ê°€ì¹˜ì œì•ˆ, ë©”ì‹œì§€ í”„ë ˆì„, ì±„ë„/ìº í˜ì¸ ì•„ì´ë””ì–´, KPIë¥¼ ì œì‹œí•˜ì„¸ìš”. í•œêµ­ì–´."
)

def generate_marketing_report(product_concept: str, temperature=0.7, max_tokens=700):
    """
    ì„¸ ëª…ì˜ 'ê°€ìƒ' ì—ì´ì „íŠ¸ê°€ ìˆœì°¨ë¡œ ì‘ì—…í•˜ëŠ” íŒŒì´í”„ë¼ì¸.
    - ê° ë‹¨ê³„ëŠ” ë¡œì»¬ LLM í˜¸ì¶œ.
    """
    product_concept = product_concept.strip()
    if not product_concept:
        return ("[ê²½ê³ ] ì œí’ˆ ê°œë…ì„ ì…ë ¥í•˜ì„¸ìš”.", "", "")

    # 1) ì‹œì¥ ì¡°ì‚¬
    prompt_research = (
        f"ì œí’ˆ ê°œë…: {product_concept}\n\n"
        "ìœ„ ì œí’ˆì— ëŒ€í•œ 'ì‹œì¥ ì¡°ì‚¬ ë³´ê³ ì„œ'ë¥¼ ì‘ì„±í•˜ì„¸ìš”."
    )
    market_research = ollama_generate(
        prompt=prompt_research,
        system=RESEARCHER_SYS,
        temperature=temperature,
        max_tokens=int(max_tokens),
    )

    # 2) ê³ ê° ì„¸ë¶„í™”
    prompt_seg = (
        "ë‹¤ìŒ ì‹œì¥ ì¡°ì‚¬ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê³ ê° ì„¸ë¶„í™” ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”.\n\n"
        f"[ì‹œì¥ ì¡°ì‚¬]\n{market_research}"
    )
    segmentation = ollama_generate(
        prompt=prompt_seg,
        system=SEGMENT_SYS,
        temperature=temperature,
        max_tokens=int(max_tokens),
    )

    # 3) ë§ˆì¼€íŒ… ì „ëµ
    prompt_strat = (
        "ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¢…í•©ì ì¸ ë§ˆì¼€íŒ… ì „ëµì„ ìˆ˜ë¦½í•˜ì„¸ìš”.\n\n"
        f"[ì‹œì¥ ì¡°ì‚¬]\n{market_research}\n\n"
        f"[ê³ ê° ì„¸ë¶„í™”]\n{segmentation}"
    )
    strategy = ollama_generate(
        prompt=prompt_strat,
        system=STRATEGIST_SYS,
        temperature=temperature,
        max_tokens=int(max_tokens),
    )

    return (market_research, segmentation, strategy)

# =============================================================================
# Gradio UI
# =============================================================================
with gr.Blocks(title="OpenCode Â· Local Multi-Agent (Ollama + DeepSeek-R1)", theme=gr.themes.Soft()) as demo:
    gr.Markdown(
        """
        # ğŸ§© OpenCode Â· Local Multi-Agent (Ollama + DeepSeek-R1)
        - OpenAI/autogen ì½”ë“œ ì—†ì´ **ë¡œì»¬ Ollama + DeepSeek-R1**ë¡œ ì—ì´ì „íŠ¸/ëŒ€í™” ì‹œë®¬ë ˆì´ì…˜
        - API Key ë¶ˆí•„ìš” Â· ì¸í„°ë„· ì—°ê²° ë¶ˆí•„ìš”(ëª¨ë¸ë§Œ ì„¤ì¹˜ë˜ì–´ ìˆìœ¼ë©´ ë¨)
        """
    )

    # íƒ­ 1) ë‹¨ì¼ ì–´ì‹œìŠ¤í„´íŠ¸
    with gr.Tab("1) ë‹¨ì¼ ì–´ì‹œìŠ¤í„´íŠ¸"):
        gr.Markdown("ê°„ë‹¨í•œ Q&A ë˜ëŠ” í”„ë¡¬í”„íŠ¸ ëŒ€í™” (AutoGen initiate_chat ëŒ€ì²´)")
        chat = gr.ChatInterface(
            fn=lambda history, msg, t, m: single_assistant_chat(history, msg, t, m),
            additional_inputs=[
                gr.Slider(0.0, 1.0, value=0.7, step=0.05, label="Temperature"),
                gr.Slider(64, 2000, value=800, step=32, label="Max Tokens"),
            ],
            title="Assistant",
            description="ì¹œì ˆí•œ ì¡°ë ¥ì (í•œêµ­ì–´)",
            retry_btn=None, undo_btn=None, clear_btn="ëŒ€í™” ì´ˆê¸°í™”"
        )

    # íƒ­ 2) 2-ì—ì´ì „íŠ¸ ëŒ€í™”
    with gr.Tab("2) 2-ì—ì´ì „íŠ¸ ëŒ€í™” (Trainer â†” Doctor)"):
        gr.Markdown("í”¼íŠ¸ë‹ˆìŠ¤ íŠ¸ë ˆì´ë„ˆì™€ ìš´ë™ ì˜í•™ ì „ë¬¸ê°€ì˜ ì™•ë³µ ëŒ€í™” ì‹œë®¬ë ˆì´ì…˜")
        seed = gr.Textbox(label="íŠ¸ë ˆì´ë„ˆì˜ ì²« ë©”ì‹œì§€", value="Dr. Lee, ì˜¤ëŠ˜ ìš´ë™ì„ ì‹œì‘í•˜ë ¤ëŠ” ì‚¬ëŒì—ê²Œ ì–´ë–¤ ìš´ë™ì„ ì¶”ì²œí• ê¹Œìš”?")
        turns = gr.Slider(2, 12, value=3, step=1, label="ì´ ë°œí™” ìˆ˜")
        t_tr = gr.Slider(0.0, 1.0, value=0.9, step=0.05, label="Trainer Temperature")
        t_dr = gr.Slider(0.0, 1.0, value=0.7, step=0.05, label="Doctor Temperature")
        mx = gr.Slider(64, 1000, value=300, step=16, label="ê° í„´ Max Tokens")
        btn = gr.Button("ëŒ€í™” ì‹¤í–‰")
        out = gr.Markdown()
        btn.click(
            two_agent_chat,
            inputs=[seed, turns, t_tr, t_dr, mx],
            outputs=[out]
        )

    # íƒ­ 3) ë§ˆì¼€íŒ… 3-ì—ì´ì „íŠ¸ íŒŒì´í”„ë¼ì¸
    with gr.Tab("3) ë§ˆì¼€íŒ… ì—ì´ì „íŠ¸ íŒŒì´í”„ë¼ì¸"):
        gr.Markdown("ì‹œì¥ ì¡°ì‚¬ â†’ ê³ ê° ì„¸ë¶„í™” â†’ ë§ˆì¼€íŒ… ì „ëµì„ ìˆœì°¨ ìƒì„±")
        prod = gr.Textbox(
            label="ì œí’ˆ ê°œë…",
            value="AI ê¸°ë°˜ ê°œì¸ ë§ì¶¤í˜• ê±´ê°• ê´€ë¦¬ ì›¨ì–´ëŸ¬ë¸” ë””ë°”ì´ìŠ¤",
            lines=3
        )
        temp = gr.Slider(0.0, 1.0, value=0.7, step=0.05, label="Temperature")
        mxt  = gr.Slider(128, 2000, value=700, step=32, label="ê° ë‹¨ê³„ Max Tokens")
        run3 = gr.Button("ë³´ê³ ì„œ ìƒì„±")
        mrk  = gr.Markdown(label="2. ì‹œì¥ ì¡°ì‚¬")
        seg  = gr.Markdown(label="3. ê³ ê° ì„¸ë¶„í™”")
        strat= gr.Markdown(label="4. ë§ˆì¼€íŒ… ì „ëµ")
        run3.click(
            generate_marketing_report,
            inputs=[prod, temp, mxt],
            outputs=[mrk, seg, strat]
        )

if __name__ == "__main__":
    # 0.0.0.0 ë¡œ ë°”ì¸ë”© â†’ ë‹¤ë¥¸ ê¸°ê¸°ì—ì„œë„ ì ‘ì† ê°€ëŠ¥ (ë™ì¼ ë„¤íŠ¸ì›Œí¬)
    demo.launch(server_name="0.0.0.0", server_port=7860)
