#91
"""
OpenCode: CLIP ì œë¡œìƒ· ë¶„ë¥˜ + ì´ë¯¸ì§€ ìº¡ì…˜(Clip-Interrogator) + Gradio
------------------------------------------------------------------
- Closed LLM ì˜ì¡´ ì—†ìŒ, ëª¨ë‘ ì˜¤í”ˆì†ŒìŠ¤ ë¡œì»¬ ëª¨ë¸(Hugging Face) ì‚¬ìš©
- ê¸°ëŠ¥
  1) ì´ë¯¸ì§€ ì—…ë¡œë“œ/URL ì…ë ¥
  2) ì‚¬ìš©ì ì •ì˜ ë¼ë²¨ë¡œ CLIP ì œë¡œìƒ· ë¶„ë¥˜ (ViT-B/32)
  3) ì´ë¯¸ì§€ ê·¸ë¦¬ë“œ ë¯¸ë¦¬ë³´ê¸° + ê° ì´ë¯¸ì§€ë³„ í™•ë¥  ë§‰ëŒ€ê·¸ë˜í”„
  4) (ì„ íƒ) Clip-Interrogatorë¡œ ìì—°ì–´ ìº¡ì…˜/í”„ë¡¬í”„íŠ¸ ì¶”ì •
- GPUê°€ ìˆìœ¼ë©´ ìë™ìœ¼ë¡œ CUDA ì‚¬ìš©, ì—†ìœ¼ë©´ CPU

ì„¤ì¹˜
  pip install -U torch torchvision pillow requests gradio matplotlib
  pip install -U transformers
  pip install -U clip-interrogator==0.6.0  # (ì„ íƒ) ìº¡ì…˜ ê¸°ëŠ¥ ì›í•˜ë©´

ì‹¤í–‰
  python clip_zeroshot_gradio.py
"""
from __future__ import annotations
import io
import os
import math
import requests
from dataclasses import dataclass
from typing import List, Tuple, Optional

import torch
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt
import gradio as gr

from transformers import CLIPProcessor, CLIPModel

try:
    from clip_interrogator import Config, Interrogator
    HAVE_CI = True
except Exception:
    HAVE_CI = False

# =====================================
# ì¥ì¹˜ ì„¤ì • (GPU ìš°ì„ )
# =====================================
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# =====================================
# ëª¨ë¸/í”„ë¡œì„¸ì„œ ë¡œë“œ (ìºì‹±)
# =====================================
@dataclass
class ClipBundle:
    model: CLIPModel
    processor: CLIPProcessor

_CLIP_CACHE: Optional[ClipBundle] = None


def load_clip(model_id: str = "openai/clip-vit-base-patch32") -> ClipBundle:
    global _CLIP_CACHE
    if _CLIP_CACHE is not None:
        return _CLIP_CACHE
    model = CLIPModel.from_pretrained(model_id).to(DEVICE)
    processor = CLIPProcessor.from_pretrained(model_id)
    _CLIP_CACHE = ClipBundle(model=model, processor=processor)
    return _CLIP_CACHE

# =====================================
# ìœ í‹¸: ì´ë¯¸ì§€ ë¡œë”©/ê·¸ë¦¬ë“œ/í”Œë¡¯
# =====================================

def load_image_from_url(url: str) -> Image.Image:
    resp = requests.get(url, stream=True, timeout=20)
    resp.raise_for_status()
    return Image.open(resp.raw).convert("RGB")


def image_grid(imgs: List[Image.Image], cols: int = 2) -> Image.Image:
    if not imgs:
        raise ValueError("ì´ë¯¸ì§€ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
    w, h = imgs[0].size
    rows = math.ceil(len(imgs) / cols)
    grid = Image.new("RGB", size=(cols * w, rows * h), color=(255, 255, 255))
    for i, img in enumerate(imgs):
        grid.paste(img, box=((i % cols) * w, (i // cols) * h))
    return grid


def barplot_image(probs: np.ndarray, labels: List[str]) -> Image.Image:
    # probs: (C,) ì‹¤ìˆ˜ ë°°ì—´
    fig, ax = plt.subplots(figsize=(4.2, 2.6), dpi=150)
    y = np.arange(len(labels))
    ax.barh(y, probs, color="#4F46E5")
    ax.set_yticks(y, labels)
    ax.set_xlim(0, 1.0)
    ax.invert_yaxis()
    for i, p in enumerate(probs):
        ax.text(min(0.98, p + 0.02), i, f"{p:.2f}", va="center")
    ax.set_xlabel("Probability")
    ax.grid(axis="x", linestyle=":", alpha=0.4)
    fig.tight_layout()
    buf = io.BytesIO()
    fig.savefig(buf, format="png", bbox_inches="tight")
    plt.close(fig)
    buf.seek(0)
    return Image.open(buf)

# =====================================
# ì œë¡œìƒ· ë¶„ë¥˜
# =====================================

def zeroshot_classify(images: List[Image.Image], labels: List[str]) -> Tuple[List[np.ndarray], List[int]]:
    bundle = load_clip()
    model, processor = bundle.model, bundle.processor
    with torch.no_grad():
        inputs = processor(text=labels, images=images, return_tensors="pt", padding=True)
        # CPU/GPU ì¥ì¹˜ ë°°ì¹˜
        inputs = {k: v.to(DEVICE) if hasattr(v, "to") else v for k, v in inputs.items()}
        outputs = model(**inputs)
        logits_per_image = outputs.logits_per_image  # (N, C)
        probs = logits_per_image.softmax(dim=1).detach().cpu().numpy()
        pred_idx = probs.argmax(axis=1).tolist()
    return [p for p in probs], pred_idx

# =====================================
# Clip-Interrogator ìº¡ì…˜
# =====================================
_CI: Optional[Interrogator] = None

def load_interrogator(clip_model_name: str = "ViT-L-14/openai") -> Optional[Interrogator]:
    global _CI
    if not HAVE_CI:
        return None
    if _CI is not None:
        return _CI
    cfg = Config(clip_model_name=clip_model_name)
    _CI = Interrogator(cfg)
    return _CI


def generate_captions(images: List[Image.Image]) -> List[str]:
    ci = load_interrogator()
    if ci is None:
        return ["(clip-interrogator ë¯¸ì„¤ì¹˜)"] * len(images)
    caps = []
    for img in images:
        caps.append(ci.interrogate(img))
    return caps

# =====================================
# Gradio í•¸ë“¤ëŸ¬
# =====================================

def run_pipeline(
    upload_imgs: List[np.ndarray],
    url_text: str,
    labels_text: str,
    cols: int,
    want_captions: bool,
) -> Tuple[Image.Image, List[Image.Image], List[str]]:
    # 1) ì´ë¯¸ì§€ ìˆ˜ì§‘ (ì—…ë¡œë“œ + URL)
    imgs: List[Image.Image] = []
    if upload_imgs:
        for arr in upload_imgs:
            imgs.append(Image.fromarray(arr.astype(np.uint8)).convert("RGB"))
    urls = [u.strip() for u in (url_text or "").splitlines() if u.strip()]
    for u in urls:
        try:
            imgs.append(load_image_from_url(u))
        except Exception as e:
            print(f"[URL ë¡œë“œ ì‹¤íŒ¨] {u}: {e}")
    if not imgs:
        raise gr.Error("ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ê±°ë‚˜ URLì„ ì…ë ¥í•˜ì„¸ìš”.")

    # 2) ë¼ë²¨ íŒŒì‹±
    labels = [x.strip() for x in labels_text.split(',') if x.strip()]
    if not labels:
        raise gr.Error("ë¼ë²¨ì„ ì½¤ë§ˆ(,)ë¡œ êµ¬ë¶„í•´ ì…ë ¥í•˜ì„¸ìš”. ì˜ˆ) giraffe,zebra,elephant,person,toy")

    # 3) ì œë¡œìƒ· ë¶„ë¥˜
    probs_list, pred_idx = zeroshot_classify(imgs, labels)

    # 4) ê·¸ë¦¬ë“œ ì¸ë„¤ì¼
    grid = image_grid([im.resize((384, 384)) for im in imgs], cols=max(1, int(cols)))

    # 5) ê° ì´ë¯¸ì§€ë³„ ë§‰ëŒ€ê·¸ë˜í”„ + (ì„ íƒ) ìº¡ì…˜
    caps = generate_captions(imgs) if want_captions else [""] * len(imgs)

    viz_rows: List[Image.Image] = []
    txt_rows: List[str] = []
    for i, (probs, pidx) in enumerate(zip(probs_list, pred_idx)):
        bar = barplot_image(probs, labels)
        viz_rows.append(bar)
        label = labels[pidx]
        caption = caps[i]
        txt = f"Top-1: {label} ({probs[pidx]:.2f})"
        if want_captions:
            txt += f"\nCaption: {caption}"
        txt_rows.append(txt)

    return grid, viz_rows, txt_rows

# =====================================
# Gradio UI
# =====================================
with gr.Blocks(title="CLIP ì œë¡œìƒ· ë¶„ë¥˜ + ìº¡ì…˜", theme=gr.themes.Soft()) as demo:
    gr.Markdown(
        """
        # ğŸ–¼ï¸ CLIP ì œë¡œìƒ· ë¶„ë¥˜ + ìº¡ì…˜ (ë¡œì»¬)
        - ëª¨ë¸: `openai/clip-vit-base-patch32` (Hugging Face, ë¡œì»¬ ìºì‹œ)
        - ìº¡ì…˜: `clip-interrogator` (ì„¤ì¹˜í•œ ê²½ìš°ì—ë§Œ)
        """
    )

    with gr.Row():
        img_uploader = gr.Gallery(label="ì´ë¯¸ì§€ ì—…ë¡œë“œ", columns=4, allow_preview=True).style(grid=[4])

    with gr.Row():
        url_box = gr.Textbox(label="ì´ë¯¸ì§€ URL (ì¤„ë°”ê¿ˆìœ¼ë¡œ ì—¬ëŸ¬ ê°œ)")
    labels_box = gr.Textbox(label="ë¼ë²¨(í´ë˜ìŠ¤), ì½¤ë§ˆë¡œ êµ¬ë¶„", value="giraffe,zebra,elephant,person,toy")
    cols_slider = gr.Slider(1, 4, value=2, step=1, label="ê·¸ë¦¬ë“œ ì—´ ìˆ˜")
    cap_ck = gr.Checkbox(value=False, label="Clip-Interrogator ìº¡ì…˜ ìƒì„±(ì„ íƒ)")

    run_btn = gr.Button("ì‹¤í–‰", variant="primary")

    grid_out = gr.Image(label="ê·¸ë¦¬ë“œ ë¯¸ë¦¬ë³´ê¸°")
    bars_out = gr.Gallery(label="ê° ì´ë¯¸ì§€ í™•ë¥  ë§‰ëŒ€ê·¸ë˜í”„", columns=2)
    txt_out = gr.HighlightedText(label="Top-1/ìº¡ì…˜ í…ìŠ¤íŠ¸", combine_adjacent=True)

    def _gallery_to_arrays(gal_items):
        # Gradio GalleryëŠ” ì—…ë¡œë“œ ì‹œ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì „ë‹¬ë¨.
        # ê° í•­ëª©ì€ dict ë˜ëŠ” ndarrayì¼ ìˆ˜ ìˆìŒ. ndarrayë§Œ ì¶”ë ¤ ë°˜í™˜.
        if not gal_items:
            return []
        arrs = []
        for it in gal_items:
            if isinstance(it, dict) and "name" in it and "data" in it:
                arrs.append(it["data"])  # ndarray
            elif isinstance(it, np.ndarray):
                arrs.append(it)
        return arrs

    run_btn.click(
        lambda gal, url, labels, cols, cap: run_pipeline(_gallery_to_arrays(gal), url, labels, cols, cap),
        inputs=[img_uploader, url_box, labels_box, cols_slider, cap_ck],
        outputs=[grid_out, bars_out, txt_out]
    )

    gr.Markdown(
        """
        ### íŒ
        - ë¼ë²¨ ë¬¸êµ¬ëŠ” ììœ ë¡­ê²Œ ë°”ê¿€ ìˆ˜ ìˆìŠµë‹ˆë‹¤. (ì˜ˆ: "cat,dog,car,airplane")
        - ìº¡ì…˜ ê¸°ëŠ¥ì€ `clip-interrogator` ì„¤ì¹˜ ì‹œ í™œì„±í™”ë©ë‹ˆë‹¤. ì²˜ìŒ ì‹¤í–‰ ë•Œ ê°€ì¤‘ì¹˜ ë‹¤ìš´ë¡œë“œë¡œ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        - ëŒ€ëŸ‰ ì´ë¯¸ì§€ëŠ” GPUë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤. CPUì—ì„œë„ ë™ì‘í•˜ì§€ë§Œ ëŠë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        """
    )

if __name__ == "__main__":
    # ì‚¬ì „ ë¡œë”©(ì„ íƒ): ì²« í˜¸ì¶œ ì§€ì—°ì„ ì¤„ì´ê³  ì‹¶ìœ¼ë©´ ë¯¸ë¦¬ ë¡œë“œ
    _ = load_clip()
    demo.launch()


# pip install -U torch torchvision pillow requests gradio matplotlib transformers
# (ì„ íƒ) ìº¡ì…˜ ê¸°ëŠ¥:
# pip install â€“U clip-interrogator==0.6.0
