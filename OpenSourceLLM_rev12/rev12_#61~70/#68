#68
import os
import re
import base64
from typing import List, Dict, Any

# GUI: Gradio
import gradio as gr

# ë°ì´í„°/ì´ë¯¸ì§€
from datasets import load_dataset
from PIL import Image

# ë²¡í„°DB: Chroma + OpenCLIP ì„ë² ë”© (ë¡œì»¬)
import chromadb
from chromadb.utils.embedding_functions import OpenCLIPEmbeddingFunction
from chromadb.utils.data_loaders import ImageLoader

# ì´ë¯¸ì§€ ìº¡ì…”ë‹(ì˜¤í”ˆì†ŒìŠ¤): BLIP (transformers)
# - ì‹œê° ëª¨ë¸ì´ ì—†ëŠ” DeepSeek-R1 ë³´ì™„ìš©: ì´ë¯¸ì§€ â†’ í…ìŠ¤íŠ¸ ìº¡ì…˜ ìƒì„± í›„ R1ì— ì „ë‹¬
# - ìµœì´ˆ ì‹¤í–‰ ì‹œ ëª¨ë¸ì´ ìë™ ë‹¤ìš´ë¡œë“œë©ë‹ˆë‹¤.
from transformers import BlipProcessor, BlipForConditionalGeneration
import torch

# HTTP í´ë¼ì´ì–¸íŠ¸: Ollama ë¡œì»¬ API í˜¸ì¶œ (http://localhost:11434)
import requests

############################################################
# í™˜ê²½ ì„¤ì •
############################################################
# ë¡œì»¬ PCì— Ollama ë° deepseek-r1 ëª¨ë¸ì´ ì„¤ì¹˜ë˜ì–´ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
# ëª¨ë¸ ì´ë¦„ì€ í™˜ê²½ë³€ìˆ˜ OLLAMA_MODEL ë¡œ ë°”ê¿€ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
OLLAMA_URL = os.getenv("OLLAMA_URL", "http://localhost:11434")
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "deepseek-r1:latest")  # ì˜ˆ: deepseek-r1:7b, deepseek-r1:32b ë“±

# ìŠ¤í¬ë¦½íŠ¸ ê²½ë¡œ
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))

# ë°ì´í„°/ì €ì¥ ê²½ë¡œ
DATASET_NAME = "detection-datasets/fashionpedia"
DATASET_DIR = os.path.join(SCRIPT_DIR, "fashion_dataset")
VDB_DIR = os.path.join(SCRIPT_DIR, "img_vdb")

# ì €ì¥í•  ì´ë¯¸ì§€ ê°œìˆ˜(ë„ˆë¬´ í¬ê²Œ ì¡ìœ¼ë©´ ìµœì´ˆ ì„¸íŒ… ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤)
NUM_IMAGES_TO_SAVE = int(os.getenv("NUM_IMAGES", "300"))

############################################################
# Ollama Chat Helper (DeepSeek-R1)
############################################################

def ollama_chat(messages: List[Dict[str, str]], model: str = OLLAMA_MODEL, **options) -> str:
    """
    Ollama Chat APIë¡œ ë¡œì»¬ LLM(DeepSeek-R1)ì„ í˜¸ì¶œí•©ë‹ˆë‹¤.
    messages: [{"role": "system|user|assistant", "content": "..."}, ...]
    options: í…œí¼ëŸ¬ì²˜ ë“± ì˜µì…˜. ì˜ˆ) temperature=0.2
    ë°˜í™˜: ëª¨ë¸ì˜ í…ìŠ¤íŠ¸ ì‘ë‹µ(str)
    """
    url = f"{OLLAMA_URL}/api/chat"
    payload: Dict[str, Any] = {
        "model": model,
        "messages": messages,
        "stream": False,
        "options": options or {"temperature": 0.2},
    }
    resp = requests.post(url, json=payload, timeout=600)
    resp.raise_for_status()
    data = resp.json()
    text = data.get("message", {}).get("content", "")
    # DeepSeek-R1ì€ <think> ë‚´ë¶€ ì‚¬ê³  í”ì ì„ ë‚´ë³´ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. UI ë…¸ì´ì¦ˆ ì œê±°ìš©ìœ¼ë¡œ ìˆ¨ê¹ë‹ˆë‹¤.
    text = re.sub(r"<think>[\s\S]*?</think>", "", text, flags=re.MULTILINE)
    return text.strip()

############################################################
# ë²ˆì—­ ìœ í‹¸(DeepSeek-R1 ì‚¬ìš©) â€” íì‡„í˜• API ì œê±°
############################################################

def translate(text: str, target_lang: str) -> str:
    """DeepSeek-R1ì„ ì‚¬ìš©í•œ ê°„ë‹¨ ë²ˆì—­ê¸°."""
    sys = {
        "role": "system",
        "content": (
            "You are a professional translator. Translate the user's text faithfully, "
            "preserving meaning and tone. Output only the translation without comments."
        ),
    }
    usr = {"role": "user", "content": f"Translate to {target_lang}:\n{text}"}
    return ollama_chat([sys, usr], temperature=0.0)

############################################################
# ë°ì´í„°ì…‹ ì¤€ë¹„ (FashionPedia)
############################################################

def setup_dataset():
    """íŒ¨ì…˜ ê´€ë ¨ ê³µê°œ ë°ì´í„°ì…‹(FashionPedia) ë‹¤ìš´ë¡œë“œ ë° í´ë” ì¤€ë¹„."""
    dataset = load_dataset(DATASET_NAME)
    os.makedirs(DATASET_DIR, exist_ok=True)
    return dataset, DATASET_DIR


def save_images(dataset, dataset_folder: str, num_images: int = NUM_IMAGES_TO_SAVE):
    """ë°ì´í„°ì…‹ì—ì„œ ì§€ì • ê°œìˆ˜ë§Œí¼ ì´ë¯¸ì§€ë¥¼ PNGë¡œ ì €ì¥."""
    count = min(num_images, len(dataset["train"]))
    for i in range(count):
        image = dataset["train"][i]["image"]  # PIL Image
        out_path = os.path.join(dataset_folder, f"image_{i+1}.png")
        image.save(out_path)
    print(f"{count}ê°œì˜ ì´ë¯¸ì§€ë¥¼ {dataset_folder}ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

############################################################
# Chroma ë²¡í„°DB ì¤€ë¹„ (ì´ë¯¸ì§€ ì„ë² ë”©/ê²€ìƒ‰)
############################################################

def setup_chroma_db():
    os.makedirs(VDB_DIR, exist_ok=True)
    chroma_client = chromadb.PersistentClient(path=VDB_DIR)
    image_loader = ImageLoader()
    clip = OpenCLIPEmbeddingFunction()  # ë¡œì»¬ OpenCLIP ì‚¬ìš©
    image_vdb = chroma_client.get_or_create_collection(
        name="image", embedding_function=clip, data_loader=image_loader
    )
    return image_vdb


def get_existing_ids(image_vdb, dataset_folder: str):
    existing_ids = set()
    try:
        # vdbì— ë“¤ì–´ìˆëŠ” ëª¨ë“  idë¥¼ ìµœëŒ€í•œ ê°€ì ¸ì˜¤ë˜, íŒŒì¼ ê°œìˆ˜ ë§Œí¼ë§Œ ìš”ì²­
        num_images = len([n for n in os.listdir(dataset_folder) if n.endswith('.png')])
        records = image_vdb.query(query_texts=[""], n_results=num_images, include=["ids"])
        for record in records.get("ids", []):
            for _id in record:
                existing_ids.add(_id)
        print(f"ë²¡í„°DBì— ì¡´ì¬í•˜ëŠ” ID ìˆ˜: {len(existing_ids)}")
    except Exception as e:
        print(f"ê¸°ì¡´ ID ì¡°íšŒ ì¤‘ ì˜ˆì™¸ê°€ ë°œìƒí–ˆì§€ë§Œ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤: {e}")
    return existing_ids


def add_images_to_db(image_vdb, dataset_folder: str):
    existing_ids = get_existing_ids(image_vdb, dataset_folder)
    ids, uris = [], []
    for i, filename in enumerate(sorted(os.listdir(dataset_folder))):
        if not filename.endswith('.png'):
            continue
        img_id = str(i)
        if img_id not in existing_ids:
            file_path = os.path.join(dataset_folder, filename)
            ids.append(img_id)
            uris.append(file_path)
    if ids:
        image_vdb.add(ids=ids, uris=uris)
        print(f"ìƒˆë¡œìš´ {len(ids)}ê°œ ì´ë¯¸ì§€ë¥¼ ë²¡í„°DBì— ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")
    else:
        print("ì¶”ê°€í•  ìƒˆë¡œìš´ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")


def query_db(image_vdb, query: str, results: int = 2):
    return image_vdb.query(
        query_texts=[query],
        n_results=results,
        include=["uris", "distances", "ids"],
    )

############################################################
# ì´ë¯¸ì§€ ìº¡ì…”ë‹(BLIP) â€” ì‹œê° ì •ë³´ â†’ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
############################################################

_BLIP_PROCESSOR = None
_BLIP_MODEL = None

def _load_blip():
    global _BLIP_PROCESSOR, _BLIP_MODEL
    if _BLIP_PROCESSOR is None or _BLIP_MODEL is None:
        device = "cuda" if torch.cuda.is_available() else "cpu"
        _BLIP_PROCESSOR = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-large")
        _BLIP_MODEL = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-large").to(device)
    return _BLIP_PROCESSOR, _BLIP_MODEL


def caption_image(image_path: str) -> str:
    """ì´ë¯¸ì§€ 1ì¥ì„ BLIPìœ¼ë¡œ ìº¡ì…˜ ìƒì„±."""
    processor, model = _load_blip()
    device = next(model.parameters()).device
    image = Image.open(image_path).convert("RGB")
    inputs = processor(images=image, return_tensors="pt").to(device)
    with torch.no_grad():
        out = model.generate(**inputs, max_new_tokens=40)
    caption = processor.decode(out[0], skip_special_tokens=True)
    return caption

############################################################
# íŒ¨ì…˜ ì¡°ì–¸ í”„ë¡¬í”„íŒ… (DeepSeek-R1)
############################################################

def fashion_advice_with_captions(user_query_ko: str, image_paths: List[str]) -> str:
    """
    ì‚¬ìš©ì í•œêµ­ì–´ ì§ˆë¬¸ê³¼, ê´€ë ¨ ì´ë¯¸ì§€ë“¤ì˜ BLIP ìº¡ì…˜ì„ DeepSeek-R1ì— ì „ë‹¬í•˜ì—¬
    ìŠ¤íƒ€ì¼ë§ ì¡°ì–¸(í•œêµ­ì–´)ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    # ì´ë¯¸ì§€ë“¤ í…ìŠ¤íŠ¸í™”
    captions = []
    for p in image_paths:
        try:
            captions.append({"path": p, "caption": caption_image(p)})
        except Exception as e:
            captions.append({"path": p, "caption": f"(ìº¡ì…˜ ìƒì„± ì‹¤íŒ¨: {e})"})

    # ì‹œìŠ¤í…œ/ìœ ì € í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    sys = {
        "role": "system",
        "content": (
            "You are a helpful fashion & styling assistant. Use the provided image captions "
            "as visual context. Give concise, conversational advice in Korean, and refer to "
            "specific described elements (colors, garments, textures) where helpful. Use Markdown for emphasis."
        ),
    }

    # ê°„ë‹¨í•œ í¬ë§·ìœ¼ë¡œ ì´ë¯¸ì§€ ìº¡ì…˜ì„ í…ìŠ¤íŠ¸ ì»¨í…ìŠ¤íŠ¸ë¡œ ì œê³µ
    caption_block = "\n".join([
        f"- Image {i+1} ({os.path.basename(c['path'])}): {c['caption']}" for i, c in enumerate(captions)
    ])

    user = {
        "role": "user",
        "content": (
            f"ì§ˆë¬¸: {user_query_ko}\n\n"
            f"ì•„ë˜ëŠ” ê´€ë ¨ ì´ë¯¸ì§€ì˜ ìë™ ìº¡ì…˜ì…ë‹ˆë‹¤. ì´ ì„¤ëª…ì„ ì‹œê° ë‹¨ì„œë¡œ í™œìš©í•´ì„œ ë‹µë³€í•˜ì„¸ìš”:\n{caption_block}\n\n"
            f"ìš”ì²­ì‚¬í•­:\n"
            f"1) í•µì‹¬ ì•„ì´ë””ì–´ 3~6ê°œ\n"
            f"2) ì´ìœ /ê·¼ê±°ë¥¼ ê°„ë‹¨íˆ\n"
            f"3) ìƒí™©/ê³„ì ˆ/ì˜ˆì‚°ì— ë”°ë¥¸ ë³€í˜• íŒì´ ìˆìœ¼ë©´ ì¶”ê°€\n"
        ),
    }

    answer = ollama_chat([sys, user], temperature=0.3)
    return answer

############################################################
# ìœ í‹¸: ì´ë¯¸ì§€ Base64 (Gradio Galleryì—” í•„ìš” ì—†ìŒ, ì°¸ê³ ìš©)
############################################################

def load_image_as_base64(image_path: str) -> str:
    with open(image_path, "rb") as img_file:
        return base64.b64encode(img_file.read()).decode("utf-8")

############################################################
# íŒŒì´í”„ë¼ì¸ (Gradioì—ì„œ í˜¸ì¶œ)
############################################################

def pipeline(user_query_ko: str):
    """Gradio ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬: í•œêµ­ì–´ ì§ˆë¬¸ ì…ë ¥ â†’ ê²€ìƒ‰ â†’ ìº¡ì…˜ â†’ DeepSeek-R1 ì¡°ì–¸."""
    if not user_query_ko or not user_query_ko.strip():
        return (
            gr.update(value="ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”."),  # ì‘ë‹µ í…ìŠ¤íŠ¸
            [],  # ì´ë¯¸ì§€ ê°¤ëŸ¬ë¦¬
        )

    # 1) ë°ì´í„°/DB ì¤€ë¹„(ìµœì´ˆ 1íšŒì„± ì‘ì—…ì€ ì•± ì‹œì‘ ì‹œ ì™„ë£Œë˜ë„ë¡ mainì—ì„œ ì²˜ë¦¬)
    image_vdb = setup_chroma_db()

    # 2) ì˜ì–´ë¡œ ë³€í™˜(ê²€ìƒ‰ ì„±ëŠ¥ ë³´ì™„; OpenCLIPëŠ” ì˜ì–´ ì¿¼ë¦¬ì— ê°•í•¨)
    try:
        query_en = translate(user_query_ko, "English")
    except Exception:
        # ë²ˆì—­ ì‹¤íŒ¨ ì‹œ í•œêµ­ì–´ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ê°•ê±´ì„±)
        query_en = user_query_ko

    # 3) ìœ ì‚¬ ì´ë¯¸ì§€ ê²€ìƒ‰
    results = query_db(image_vdb, query_en, results=2)
    uris = results.get("uris", [[]])[0]

    # 4) íŒ¨ì…˜ ì¡°ì–¸ ìƒì„±(ì´ë¯¸ì§€ ìº¡ì…˜ â†’ DeepSeek-R1)
    advice = fashion_advice_with_captions(user_query_ko, uris)

    # 5) Gradioìš© ê°¤ëŸ¬ë¦¬ ë°ì´í„°: [(image_path, caption), ...] í˜•ì‹ë„ ê°€ëŠ¥í•˜ë‚˜ ì—¬ê¸°ì„  ê²½ë¡œë§Œ
    gallery_items = [(u, os.path.basename(u)) for u in uris]

    return advice, gallery_items

############################################################
# ì´ˆê¸° ì„¸íŒ…(ì•± ì‹œì‘ ì‹œ 1íšŒ)
############################################################

def initial_setup():
    # ë°ì´í„°ì…‹/ì´ë¯¸ì§€ ì¤€ë¹„
    if not os.path.exists(DATASET_DIR) or not any(f.endswith('.png') for f in os.listdir(DATASET_DIR)):
        dataset, _ = setup_dataset()
        save_images(dataset, DATASET_DIR, NUM_IMAGES_TO_SAVE)
    else:
        print("ë°ì´í„°ì…‹ ì´ë¯¸ì§€ê°€ ì´ë¯¸ ì¤€ë¹„ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")

    # ë²¡í„°DB êµ¬ì¶•
    image_vdb = setup_chroma_db()
    if not os.path.exists(VDB_DIR) or not os.listdir(VDB_DIR):
        # í´ë”ê°€ ë¹„ì–´ìˆìœ¼ë©´ ì¶”ê°€
        add_images_to_db(image_vdb, DATASET_DIR)
    else:
        # ê¸°ì¡´ DBê°€ ìˆì–´ë„ ìƒˆ ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ ì¶”ê°€
        add_images_to_db(image_vdb, DATASET_DIR)

############################################################
# Gradio UI
############################################################

def build_ui():
    with gr.Blocks(theme=gr.themes.Soft()) as demo:
        gr.Markdown("""
        # ğŸ‘— **FashionRAG (ì˜¤í”ˆì†ŒìŠ¤/ë¡œì»¬ ì „í™˜ë³¸)**
        - **LLM**: DeepSeek-R1 (Ollama, ë¡œì»¬)
        - **ì„ë² ë”©/ê²€ìƒ‰**: Chroma + OpenCLIP (ë¡œì»¬)
        - **ì‹œê° ë³´ì™„**: BLIP ì´ë¯¸ì§€ ìº¡ì…”ë‹(ì˜¤í”ˆì†ŒìŠ¤)
        - **UI**: Gradio

        > ë¡œì»¬ í™˜ê²½ë§Œìœ¼ë¡œ íì‡„í˜• API ì—†ì´ íŒ¨ì…˜ ìŠ¤íƒ€ì¼ë§ ì¡°ì–¸ì„ ì œê³µí•©ë‹ˆë‹¤.
        """)

        with gr.Row():
            q = gr.Textbox(label="ìŠ¤íƒ€ì¼ë§ ì§ˆë¬¸(í•œêµ­ì–´)", placeholder="ì˜ˆ) í•˜ì´ì›¨ì´ìŠ¤íŠ¸ ë°ë‹˜ ìŠ¤ì»¤íŠ¸ì— ì˜ ì–´ìš¸ë¦¬ëŠ” ìƒì˜?", lines=2)
        with gr.Row():
            run_btn = gr.Button("ê²€ìƒ‰í•˜ê³  ì¡°ì–¸ ë°›ê¸°", variant="primary")
        with gr.Row():
            out_md = gr.Markdown(label="FashionRAGì˜ ì‘ë‹µ")
        with gr.Row():
            gallery = gr.Gallery(label="ê²€ìƒ‰ëœ ì´ë¯¸ì§€", show_label=True, columns=2, height=360)

        run_btn.click(fn=pipeline, inputs=[q], outputs=[out_md, gallery])

    return demo

############################################################
# ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸
############################################################
if __name__ == "__main__":
    # ìµœì´ˆ 1íšŒ ì´ˆê¸° ì„¸íŒ… (ë°ì´í„° ë‚´ë ¤ë°›ê¸°/ì¸ë±ì‹±)
    initial_setup()
    # UI êµ¬ë™
    app = build_ui()
    # share=True í•„ìš” ì‹œ ì™¸ë¶€ ì ‘ì† ê³µê°œ
    app.launch(server_name="0.0.0.0", server_port=7860, share=False)

# í•„ìˆ˜ íŒ¨í‚¤ì§€
# pip install gradio datasets pillow chromadb transformers timm torch torchvision # requests

# (ì„ íƒ) GPU ì‚¬ìš© ì‹œ PyTorch CUDA ë²„ì „ ì„¤ì¹˜ ê¶Œì¥
# conda/venv í™˜ê²½ì—ì„œ ì§„í–‰ ì¶”ì²œ

# Ollamaì™€ ëª¨ë¸ ì¤€ë¹„
# https://ollama.com/ ì„¤ì¹˜ í›„
# ollama pull deepseek-r1:latest
# (ì´ë¯¸ ì„¤ì¹˜ëœ ëª¨ë¸ëª…ì´ ë‹¤ë¥´ë©´ ì½”ë“œ ìƒë‹¨ OLLAMA_MODEL í™˜ê²½ë³€ìˆ˜ë¡œ ì§€ì • ê°€ëŠ¥)
