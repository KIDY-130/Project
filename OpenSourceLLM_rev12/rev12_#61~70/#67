#67
"""
OpenCode ë³€í™˜ë³¸: (Closed LLM + Jupyter ìŠ¤í¬ë¦½íŠ¸) â†’ (Gradio + Ollama DeepSeek-R1 ë¡œì»¬ ë©€í‹°ëª¨ë‹¬ RAG)

í•µì‹¬ ë³€ê²½ì 
- âœ… OpenAI/ChatGPT ì˜ì¡´ ì œê±°, API Key ë¶ˆí•„ìš”
- âœ… Gradio GUI ì œê³µ (ì—…ë¡œë“œâ†’ì¸ë±ì‹±â†’ê²€ìƒ‰/ë‹µë³€)
- âœ… í…ìŠ¤íŠ¸ LLM: DeepSeek-R1 (Ollama)
- âœ… ë¹„ì „ LLM: LLaVA (Ollama, ì´ë¯¸ì§€ ìš”ì•½/ë©€í‹°ëª¨ë‹¬ ë‹µë³€)
- âœ… PDF íŒŒì‹±: unstructured.partition.pdf (ì´ë¯¸ì§€/í‘œ/í…ìŠ¤íŠ¸ ì¶”ì¶œ)
- âœ… ì„ë² ë”©/ë²¡í„°DB: OpenCLIPEmbeddings + Chroma
- âœ… Multi-Vector RAG: ìš”ì•½ë¬¸(í…ìŠ¤íŠ¸/í‘œ/ì´ë¯¸ì§€)ì„ ì¸ë±ì‹±í•˜ê³  ì›ë¬¸/ì›ì´ë¯¸ì§€ ë°˜í™˜
- âœ… í•œêµ­ì–´ ìµœì í™”: ëª¨ë“  í”„ë¡¬í”„íŠ¸/ì¶œë ¥ í•œê¸€í™”

ì‚¬ì „ ì¤€ë¹„ (í„°ë¯¸ë„ 1íšŒ ì‹¤í–‰)
1) Ollama ì„¤ì¹˜ í›„ ì‹¤í–‰: https://ollama.com
2) í•„ìš”í•œ ë¡œì»¬ ëª¨ë¸ í’€
   - ollama pull deepseek-r1
   - ollama pull llava:13b      # ë˜ëŠ” llava:7b
   - ollama pull nomic-embed-text
3) íŒŒì´ì¬ íŒ¨í‚¤ì§€
   - pip install -U gradio langchain langchain-community langchain-experimental langchain-text-splitters chromadb unstructured[pdf] pillow tiktoken requests pydantic lxml
   - (ì„ íƒ) poppler, tesseract ë“± unstructured ë°±ì—”ë“œ ì˜ì¡´ì„± OS ì„¤ì¹˜ í•„ìš”í•  ìˆ˜ ìˆìŒ

ì‹¤í–‰
- python app.py
"""

import os
import io
import re
import json
import base64
import uuid
import tempfile
import typing as T
import requests
import gradio as gr

from PIL import Image

# LangChain core
from langchain_core.documents import Document
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser
from langchain.prompts import ChatPromptTemplate

# LangChain community / experimental
from langchain_community.chat_models import ChatOllama
from langchain_community.vectorstores import Chroma
from langchain_experimental.open_clip import OpenCLIPEmbeddings
from langchain.storage import InMemoryStore
from langchain.retrievers.multi_vector import MultiVectorRetriever
from langchain_text_splitters import CharacterTextSplitter

# Unstructured (PDF íŒŒì‹±)
from unstructured.partition.pdf import partition_pdf


# =========================
# ì„¤ì •ê°’ (í•„ìš”ì‹œ ë³€ê²½)
# =========================
DEEPSEEK_MODEL = "deepseek-r1"     # í…ìŠ¤íŠ¸ LLM (Ollama)
LLAVA_MODEL    = "llava:13b"       # ë¹„ì „ LLM (Ollama) - ì´ë¯¸ì§€ ìš”ì•½/ë©€í‹°ëª¨ë‹¬ ë‹µë³€
EMBED_MODEL    = "nomic-embed-text"  # í…ìŠ¤íŠ¸ ì„ë² ë”© (Ollamaìš©, ë‹¨ ì—¬ê¸°ì„œëŠ” OpenCLIPìœ¼ë¡œ ì¸ë±ì‹±)
# ì£¼ì˜: ì•„ë˜ ë²¡í„°DB ì„ë² ë”©ì€ OpenCLIPEmbeddings ì‚¬ìš©(í…ìŠ¤íŠ¸ ìš”ì•½ì— ì í•©, ì´ë¯¸ì§€ ìš”ì•½ í…ìŠ¤íŠ¸ë„ ë™ì¼ ì²˜ë¦¬)

CHUNK_SIZE     = 2000
CHUNK_OVERLAP  = 200
COLLECTION_NAME = "mm_rag_finance"

# =========================
# PDF íŒŒì„œ
# =========================
def extract_pdf_elements(pdf_path: str) -> T.List:
    """
    PDF íŒŒì¼ì—ì„œ ì´ë¯¸ì§€, í…Œì´ë¸”, í…ìŠ¤íŠ¸ ë¸”ë¡ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    - ì´ë¯¸ì§€: ì§€ì •í•œ ë””ë ‰í„°ë¦¬ì— ì €ì¥
    - í…Œì´ë¸”: í…ìŠ¤íŠ¸í™”ëœ í…Œì´ë¸” ìš”ì†Œ
    - í…ìŠ¤íŠ¸: ì œëª© ë‹¨ìœ„ë¡œ í° ë¸”ë¡ ìƒì„±(ê¸¸ë©´ ìë™ ë¶„í• )
    """
    out_dir = os.path.join(os.path.dirname(pdf_path), "figures")
    os.makedirs(out_dir, exist_ok=True)

    elements = partition_pdf(
        filename=pdf_path,
        extract_images_in_pdf=True,
        infer_table_structure=True,
        chunking_strategy="by_title",
        max_characters=4000,
        new_after_n_chars=3800,
        combine_text_under_n_chars=2000,
        image_output_dir_path=out_dir,
    )
    return elements


def categorize_elements(raw_pdf_elements):
    """
    Unstructured elements â†’ í…ìŠ¤íŠ¸/í…Œì´ë¸”ë¡œ ë¶„ë¥˜ (ì´ë¯¸ì§€ëŠ” ë””ìŠ¤í¬ì— ì €ì¥ë¨)
    """
    tables, texts = [], []
    for el in raw_pdf_elements:
        t = str(type(el))
        if "unstructured.documents.elements.Table" in t:
            tables.append(str(el))
        elif "unstructured.documents.elements.CompositeElement" in t:
            texts.append(str(el))
    return texts, tables


# =========================
# í…ìŠ¤íŠ¸/í‘œ ìš”ì•½ (DeepSeek-R1)
# =========================
SUMMARY_PROMPT_KO = """ë‹¹ì‹ ì€ í‘œì™€ í…ìŠ¤íŠ¸ë¥¼ ê²€ìƒ‰ ìµœì í™” í˜•íƒœë¡œ ìš”ì•½í•˜ëŠ” ë„ìš°ë¯¸ì…ë‹ˆë‹¤.
- í•µì‹¬ í‚¤ì›Œë“œ, ìˆ˜ì¹˜, ê³ ìœ ëª…ì‚¬ë¥¼ í¬í•¨í•´ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”.
- ë‚˜ì¤‘ì— ì›ë¬¸ì„ ì°¾ê¸° ì‰½ê²Œ ì œëª©/ì£¼ì œ/ì§€í‘œëª…ì„ ê°•ì¡°í•˜ì„¸ìš”.

ìš”ì•½ ëŒ€ìƒ:
{element}
"""

def summarize_batch_text(elements: T.List[str], model_name: str = DEEPSEEK_MODEL, max_concurrency: int = 4) -> T.List[str]:
    """
    DeepSeek-R1ë¡œ í…ìŠ¤íŠ¸/í…Œì´ë¸”ì„ ë°°ì¹˜ ìš”ì•½ (LangChain Runnable batch)
    """
    if not elements:
        return []
    prompt = ChatPromptTemplate.from_template(SUMMARY_PROMPT_KO)
    llm = ChatOllama(model=model_name, temperature=0.2)
    chain = {"element": lambda x: x} | prompt | llm | StrOutputParser()
    # ê°„ë‹¨í•œ ë³‘ë ¬ ì²˜ë¦¬: chunk ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ ìˆœì°¨ ë°°ì¹˜
    out = []
    for i in range(0, len(elements), max_concurrency):
        out.extend(chain.batch(elements[i:i+max_concurrency]))
    return out


# =========================
# ì´ë¯¸ì§€ ìœ í‹¸
# =========================
def encode_image(image_path: str) -> str:
    with open(image_path, "rb") as f:
        return base64.b64encode(f.read()).decode("utf-8")


def list_extracted_images(figures_dir: str) -> T.List[str]:
    if not os.path.isdir(figures_dir):
        return []
    imgs = []
    for fn in sorted(os.listdir(figures_dir)):
        if fn.lower().endswith((".jpg", ".jpeg", ".png")):
            imgs.append(os.path.join(figures_dir, fn))
    return imgs


# =========================
# LLaVA í˜¸ì¶œ (Ollama REST) â€” ì´ë¯¸ì§€ ìš”ì•½/ë©€í‹°ëª¨ë‹¬ ì‘ë‹µ
# =========================
def ollama_generate_with_images(prompt: str, images_b64: T.List[str], model: str = LLAVA_MODEL, stream: bool = True, url: str = "http://localhost:11434/api/generate") -> str:
    """
    Ollama /api/generate ë¥¼ ì‚¬ìš©í•´ ì´ë¯¸ì§€ í¬í•¨ í”„ë¡¬í”„íŠ¸ë¡œ LLaVA í˜¸ì¶œ
    """
    payload = {
        "model": model,
        "prompt": prompt,
        "images": images_b64,
        "stream": stream,
    }
    r = requests.post(url, json=payload, stream=stream, timeout=600)
    r.raise_for_status()
    if stream:
        full = ""
        for line in r.iter_lines():
            if not line:
                continue
            obj = json.loads(line)
            full += obj.get("response", "")
        return full
    else:
        return r.json().get("response", "")


# =========================
# ì´ë¯¸ì§€ ìš”ì•½ (LLaVA)
# =========================
IMAGE_SUMMARY_PROMPT_KO = """ë‹¹ì‹ ì€ íˆ¬ì ë¦¬ì„œì¹˜ ë³´ì¡°ì›ì…ë‹ˆë‹¤.
ë‹¤ìŒ ì´ë¯¸ì§€ë¥¼ ê²€ìƒ‰ ìµœì í™” ìš”ì•½ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì •ë¦¬í•˜ì„¸ìš”.
- ì°¨íŠ¸/ì¶•/ê¸°ê°„/í•µì‹¬ì§€í‘œ/ì¶”ì„¸/ì´ìƒì¹˜/ê²°ë¡ ì„ í¬í•¨
- í•œêµ­ì–´ë¡œ 3~5ë¬¸ì¥

ì´ë¯¸ì§€ ì„¤ëª…:
"""

def summarize_images_base64(images_b64: T.List[str]) -> T.List[str]:
    if not images_b64:
        return []
    summaries = []
    for b64 in images_b64:
        s = ollama_generate_with_images(IMAGE_SUMMARY_PROMPT_KO, [b64], model=LLAVA_MODEL, stream=True)
        summaries.append(s.strip())
    return summaries


# =========================
# ë©€í‹° ë²¡í„° ë¦¬íŠ¸ë¦¬ë²„ (ìš”ì•½ë¬¸ ì¸ë±ì‹± â†’ ì›ë¬¸/ì›ì´ë¯¸ì§€ ë°˜í™˜)
# =========================
def create_multi_vector_retriever(
    text_summaries: T.List[str],
    texts: T.List[str],
    table_summaries: T.List[str],
    tables: T.List[str],
    image_summaries: T.List[str],
    images_base64: T.List[str],
) -> MultiVectorRetriever:
    """
    - vectorstore(ìš”ì•½ë¬¸ ì„ë² ë”©) + docstore(ì›ë¬¸/ì›ì´ë¯¸ì§€ Base64) êµ¬ì„±
    - ìš”ì•½ë¬¸ìœ¼ë¡œ ê²€ìƒ‰ â†’ idë¡œ ì›ë¬¸ payload ë°˜í™˜
    """
    embedding = OpenCLIPEmbeddings()  # í…ìŠ¤íŠ¸ ì„ë² ë”©
    vectorstore = Chroma(collection_name=COLLECTION_NAME, embedding_function=embedding)
    store = InMemoryStore()
    id_key = "doc_id"

    retriever = MultiVectorRetriever(
        vectorstore=vectorstore,
        docstore=store,
        id_key=id_key,
    )

    def add_documents(summaries: T.List[str], originals: T.List[str]):
        doc_ids = [str(uuid.uuid4()) for _ in originals]
        # ìš”ì•½ë¬¸ì„ ë²¡í„°DBì— ì €ì¥
        summary_docs = [Document(page_content=s, metadata={id_key: doc_ids[i]}) for i, s in enumerate(summaries)]
        retriever.vectorstore.add_documents(summary_docs)
        # ì›ë¬¸ì„ ë³„ë„ ì €ì¥(ê²€ìƒ‰ í›„ ë°˜í™˜í•˜ê¸° ìœ„í•¨)
        retriever.docstore.mset(list(zip(doc_ids, originals)))

    if text_summaries:
        add_documents(text_summaries, texts)
    if table_summaries:
        add_documents(table_summaries, tables)
    if image_summaries:
        add_documents(image_summaries, images_base64)

    return retriever


# =========================
# ê²€ìƒ‰ ê²°ê³¼ â†’ ì´ë¯¸ì§€/í…ìŠ¤íŠ¸ ë¶„ë¦¬ & ë¦¬ì‚¬ì´ì¦ˆ
# =========================
def looks_like_base64(sb: str) -> bool:
    return re.match(r"^[A-Za-z0-9+/]+={0,2}$", sb or "") is not None

def is_image_data(b64data: str) -> bool:
    try:
        header = base64.b64decode(b64data)[:8]
    except Exception:
        return False
    signatures = {
        b"\xff\xd8\xff": "jpg",
        b"\x89PNG\r\n\x1a\n": "png",
        b"GIF8": "gif",
        b"RIFF": "webp",
    }
    return any(header.startswith(sig) for sig in signatures)

def resize_base64_image(b64: str, size=(1024, 576)) -> str:
    img_data = base64.b64decode(b64)
    img = Image.open(io.BytesIO(img_data))
    img = img.convert("RGB")
    img = img.resize(size, Image.LANCZOS)
    buf = io.BytesIO()
    img.save(buf, format="JPEG", quality=90)
    return base64.b64encode(buf.getvalue()).decode("utf-8")

def split_image_text_types(docs: T.List[T.Union[Document, str]]) -> T.Dict[str, T.List[str]]:
    images, texts = [], []
    for d in docs:
        content = d.page_content if isinstance(d, Document) else str(d)
        if looks_like_base64(content) and is_image_data(content):
            images.append(resize_base64_image(content))
        else:
            texts.append(content)
    return {"images": images, "texts": texts}


# =========================
# ë©€í‹°ëª¨ë‹¬ RAG ì‘ë‹µ
# =========================
FINAL_INSTRUCT_KO = """ë‹¹ì‹ ì€ ê¸ˆìœµ ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
ì•„ë˜ í…ìŠ¤íŠ¸/í‘œ/ì´ë¯¸ì§€(ì°¨íŠ¸)ë¥¼ ì¢…í•©í•´ íˆ¬ì ê´€ì ì˜ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ë¥¼ í•œêµ­ì–´ë¡œ ì œì‹œí•˜ì„¸ìš”.

ìš”êµ¬ì‚¬í•­:
- í•µì‹¬ ìš”ì•½(3~5ë¬¸ì¥)
- ê·¼ê±°: ì§€í‘œ/ìˆ˜ì¹˜/ê¸°ê°„ì„ ì¸ìš©
- ë¦¬ìŠ¤í¬ ìš”ì¸ê³¼ ê´€ì°° í¬ì¸íŠ¸
- ì¶”ê°€ë¡œ í™•ì¸í•´ì•¼ í•  ë°ì´í„° 2~3ê°œ

ì‚¬ìš©ì ì§ˆë¬¸: {question}

ì°¸ê³  í…ìŠ¤íŠ¸/í‘œ:
{context_texts}
"""

def answer_with_rag(question: str, retrieved_docs: T.List[T.Union[Document, str]]) -> str:
    data = split_image_text_types(retrieved_docs)
    context_texts = "\n\n".join(data["texts"]) if data["texts"] else "(í…ìŠ¤íŠ¸ ì—†ìŒ)"

    # ì´ë¯¸ì§€ê°€ ì¡´ì¬í•˜ë©´ LLaVA ë©€í‹°ëª¨ë‹¬ë¡œ, ì•„ë‹ˆë©´ DeepSeek-R1 ì‚¬ìš©
    if data["images"]:
        prompt = FINAL_INSTRUCT_KO.format(question=question, context_texts=context_texts)
        return ollama_generate_with_images(prompt, data["images"], model=LLAVA_MODEL, stream=True)
    else:
        prompt = ChatPromptTemplate.from_template(FINAL_INSTRUCT_KO)
        llm = ChatOllama(model=DEEPSEEK_MODEL, temperature=0.2)
        chain = {"question": RunnablePassthrough(), "context_texts": lambda _: context_texts} | prompt | llm | StrOutputParser()
        return chain.invoke(question)


# =========================
# Gradio ì•± ë¡œì§
# =========================
class SessionState:
    def __init__(self):
        self.texts: T.List[str] = []
        self.tables: T.List[str] = []
        self.images_b64: T.List[str] = []
        self.text_summaries: T.List[str] = []
        self.table_summaries: T.List[str] = []
        self.image_summaries: T.List[str] = []
        self.retriever: T.Optional[MultiVectorRetriever] = None
        self.figures_dir: T.Optional[str] = None
        self.pdf_paths: T.List[str] = []

STATE = SessionState()

def build_index(pdf_files: T.List[gr.File], summarize_texts: bool, progress=gr.Progress(track_tqdm=True)):
    if not pdf_files:
        return "ğŸ“„ PDFë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.", None, None, None

    # ì„ì‹œ ë””ë ‰í„°ë¦¬ì— ë³µì‚¬
    tmpdir = tempfile.TemporaryDirectory()
    pdf_paths = []
    for f in pdf_files:
        dst = os.path.join(tmpdir.name, os.path.basename(f.name))
        with open(dst, "wb") as out:
            out.write(open(f.name, "rb").read())
        pdf_paths.append(dst)

    # PDF íŒŒì‹±
    all_texts, all_tables, all_images_b64 = [], [], []
    figures_dir_last = None

    for p in pdf_paths:
        progress(0.1, desc=f"PDF íŒŒì‹± ì¤‘: {os.path.basename(p)}")
        elements = extract_pdf_elements(p)
        texts, tables = categorize_elements(elements)
        all_texts.extend(texts)
        all_tables.extend(tables)

        figures_dir = os.path.join(os.path.dirname(p), "figures")
        figures_dir_last = figures_dir
        img_paths = list_extracted_images(figures_dir)
        all_images_b64.extend([encode_image(ip) for ip in img_paths])

    # í…ìŠ¤íŠ¸ ë¶„í• 
    splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)
    texts_2k = splitter.split_text(" ".join(all_texts)) if all_texts else []

    # ìš”ì•½ ìƒì„±
    progress(0.5, desc="ìš”ì•½ ìƒì„± ì¤‘ (í…ìŠ¤íŠ¸/í‘œ/ì´ë¯¸ì§€)")
    text_summaries = summarize_batch_text(texts_2k, DEEPSEEK_MODEL) if (texts_2k and summarize_texts) else texts_2k
    table_summaries = summarize_batch_text(all_tables, DEEPSEEK_MODEL) if all_tables else []
    image_summaries = summarize_images_base64(all_images_b64) if all_images_b64 else []

    # ë¦¬íŠ¸ë¦¬ë²„ êµ¬ì¶•
    progress(0.8, desc="ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘ (Chroma)")
    retriever = create_multi_vector_retriever(
        text_summaries=text_summaries,
        texts=texts_2k,
        table_summaries=table_summaries,
        tables=all_tables,
        image_summaries=image_summaries,
        images_base64=all_images_b64,
    )

    # ìƒíƒœ ì €ì¥
    STATE.texts = texts_2k
    STATE.tables = all_tables
    STATE.images_b64 = all_images_b64
    STATE.text_summaries = text_summaries
    STATE.table_summaries = table_summaries
    STATE.image_summaries = image_summaries
    STATE.retriever = retriever
    STATE.figures_dir = figures_dir_last
    STATE.pdf_paths = pdf_paths

    # ê°¤ëŸ¬ë¦¬ í”„ë¦¬ë·°ìš© ì´ë¯¸ì§€ (ì• 6ê°œ)
    preview_imgs = []
    for b64 in all_images_b64[:6]:
        preview_imgs.append(Image.open(io.BytesIO(base64.b64decode(b64))).convert("RGB"))

    status = f"âœ… ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ\n- í…ìŠ¤íŠ¸ ì²­í¬: {len(texts_2k)}ê°œ\n- í…Œì´ë¸”: {len(all_tables)}ê°œ\n- ì´ë¯¸ì§€: {len(all_images_b64)}ê°œ"
    return status, preview_imgs, "\n\n".join(text_summaries[:3]) if text_summaries else "", "\n\n".join(image_summaries[:3]) if image_summaries else ""


def run_query(question: str, top_k: int):
    if not question.strip():
        return "ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.", None
    if not STATE.retriever:
        return "ë¨¼ì € PDFë¥¼ ì—…ë¡œë“œí•˜ê³  [ì¸ë±ìŠ¤ ë§Œë“¤ê¸°]ë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”.", None

    # ê²€ìƒ‰
    docs = STATE.retriever.vectorstore.similarity_search(question, k=top_k)
    # docstoreì—ì„œ ì›ë¬¸/ì´ë¯¸ì§€ ì¹˜í™˜
    resolved = []
    for d in docs:
        doc_id = d.metadata.get("doc_id")
        original = STATE.retriever.docstore.mget([doc_id])[0] if doc_id else None
        if original is not None:
            resolved.append(original if isinstance(original, Document) else Document(page_content=str(original)))
        else:
            resolved.append(d)

    # ë‹µë³€ ìƒì„±
    answer = answer_with_rag(question, resolved)

    # ì‹œê° í”„ë¦¬ë·° (ìƒìœ„ ì´ë¯¸ì§€ 3ì¥)
    split = split_image_text_types(resolved)
    img_preview = []
    for b64 in split["images"][:3]:
        img_preview.append(Image.open(io.BytesIO(base64.b64decode(b64))).convert("RGB"))

    return answer, img_preview if img_preview else None


# =========================
# Gradio UI
# =========================
with gr.Blocks(title="ë©€í‹°ëª¨ë‹¬ ê¸ˆìœµ RAG (Local Â· DeepSeek-R1 Â· LLaVA)") as demo:
    gr.Markdown("## ğŸ“Š ë©€í‹°ëª¨ë‹¬ ê¸ˆìœµ RAG â€” PDFì—ì„œ í…ìŠ¤íŠ¸/í‘œ/ì´ë¯¸ì§€ë¥¼ ì¶”ì¶œí•´ ë¡œì»¬ LLMìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.")
    gr.Markdown(
        "- **LLM**: DeepSeek-R1 (í…ìŠ¤íŠ¸), LLaVA (ì´ë¯¸ì§€/ë©€í‹°ëª¨ë‹¬) â€” *Ollama ë¡œì»¬ ì‹¤í–‰*\n"
        "- **íŒŒì„œ**: unstructured (ì´ë¯¸ì§€/í‘œ/í…ìŠ¤íŠ¸ ì¶”ì¶œ)\n"
        "- **ë²¡í„°DB**: Chroma + OpenCLIPEmbeddings (ìš”ì•½ë¬¸ ì¸ë±ì‹±)\n"
        "- **ê°œì¸ì •ë³´/ë¹„ìš©**: API Key ë¶ˆí•„ìš”, ëª¨ë“  ì²˜ë¦¬ ë¡œì»¬ ìˆ˜í–‰\n"
    )

    with gr.Row():
        pdf_in = gr.File(label="PDF ì—…ë¡œë“œ (ë³µìˆ˜ ì„ íƒ ê°€ëŠ¥)", file_types=[".pdf"], file_count="multiple")
        summarize_ck = gr.Checkbox(label="í…ìŠ¤íŠ¸/í‘œ ìš”ì•½ ì‚¬ìš© (ê¶Œì¥)", value=True)
        build_btn = gr.Button("ì¸ë±ìŠ¤ ë§Œë“¤ê¸°")

    status_out = gr.Markdown()
    with gr.Row():
        img_gallery = gr.Gallery(label="ì¶”ì¶œ ì´ë¯¸ì§€ ë¯¸ë¦¬ë³´ê¸°", columns=3, rows=2, height="auto")
    with gr.Row():
        textsum_out = gr.Markdown(label="í…ìŠ¤íŠ¸ ìš”ì•½ (ì¼ë¶€)")
        imgsum_out = gr.Markdown(label="ì´ë¯¸ì§€ ìš”ì•½ (ì¼ë¶€)")

    gr.Markdown("---")
    with gr.Row():
        query_in = gr.Textbox(label="ì§ˆë¬¸", placeholder="ì˜ˆ) ì½”ìŠ¤í”¼ ê´€ë ¨í•´ì„œ ì „ë§ì„ ì¢…í•©ì ìœ¼ë¡œ ì•Œë ¤ì¤˜")
        topk_in = gr.Slider(2, 10, value=4, step=1, label="ê²€ìƒ‰ ìƒìœ„ K")
        run_btn = gr.Button("ì§ˆë¬¸í•˜ê¸°")

    answer_out = gr.Markdown()
    with gr.Row():
        ans_gallery = gr.Gallery(label="ë‹µë³€ì— ì‚¬ìš©ëœ ì´ë¯¸ì§€ (ì¼ë¶€)", columns=3, rows=1, height="auto")

    build_btn.click(fn=build_index, inputs=[pdf_in, summarize_ck], outputs=[status_out, img_gallery, textsum_out, imgsum_out])
    run_btn.click(fn=run_query, inputs=[query_in, topk_in], outputs=[answer_out, ans_gallery])

if __name__ == "__main__":
    # ì™¸ë¶€ ì ‘ì† í•„ìš”ì‹œ server_name="0.0.0.0"
    demo.launch(server_name="0.0.0.0", server_port=7860)

