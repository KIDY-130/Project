#64
"""
ëª©í‘œ: OpenAI ì˜ì¡´ ì½”ë“œë¥¼ ì˜¤í”ˆì†ŒìŠ¤ LLM(DeepSeek-R1, Ollama) + ë¡œì»¬ ì„ë² ë”© + Gradio UIë¡œ ë³€í™˜

í•µì‹¬ ì „í™˜ì 
- ChatOpenAI â†’ ChatOllama(model="deepseek-r1")
- OpenAIEmbeddings â†’ OllamaEmbeddings(model="nomic-embed-text")
- API Key ì œê±°(ë¡œì»¬ LLM ì‚¬ìš©)
- PDF ë¡œë” ìœ ì§€(PyPDFLoader) + BM25 + Chroma(EnsembleRetriever)
- MMR ê²€ìƒ‰ + (ì˜µì…˜) MultiQueryRetriever êµ¬ì„±
- Gradio ê¸°ë°˜ ê·¸ë˜í”½ ì¸í„°í˜ì´ìŠ¤ ì œê³µ

ì‚¬ì „ ì¤€ë¹„(í„°ë¯¸ë„)
  ollama pull deepseek-r1
  ollama pull nomic-embed-text

í•„ìš” íŒ¨í‚¤ì§€(í„°ë¯¸ë„)
  pip install -U langchain langchain-community chromadb pypdf bs4 gradio

ì°¸ê³ 
- LangChain 0.2+ ê¶Œì¥ import ê²½ë¡œ ì‚¬ìš©(langchain_community.*)
- ë„¤íŠ¸ì›Œí¬/íŒŒì¼ ì ‘ê·¼ ê¶Œí•œ í•„ìš” ì‹œ OS ê¶Œí•œ í™•ì¸
"""
from __future__ import annotations
import os
import tempfile
from typing import List, Dict, Any

# ---------------------- LangChain: Loaders/Embeddings/Stores ----------------------
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import OllamaEmbeddings

# ---------------------- LangChain: LLM / Retrievers / Chains ----------------------
from langchain_community.chat_models import ChatOllama
from langchain.retrievers import EnsembleRetriever
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# ---------------------- UI ----------------------
import gradio as gr

# ===================== ì „ì—­ ìƒíƒœ =====================
PERSIST_DIR = os.path.join(tempfile.gettempdir(), "pdf_rag_chroma")
VECTORSTORE = None
CHROMA_RETRIEVER = None
BM25 = None
ENSEMBLE = None
DOCS: List[Any] = []

# ëª¨ë¸ ì´ë¦„ (Ollama)
LLM_MODEL = "deepseek-r1"
EMBED_MODEL = "nomic-embed-text"

# ===================== ìœ í‹¸/ì½”ì–´ ë¡œì§ =====================

def load_pdf_docs(pdf_path: str) -> List[Any]:
    """PDFë¥¼ í˜ì´ì§€ ë‹¨ìœ„ë¡œ ë¡œë“œí•˜ì—¬ LangChain ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜."""
    loader = PyPDFLoader(pdf_path)
    pages = loader.load_and_split()
    return pages


def split_docs(docs: List[Any], chunk_size: int = 300, chunk_overlap: int = 50) -> List[Any]:
    """ë¬¸ì„œ chunking. í† í° ì¸ì½”ë” ì˜ì¡´ ì—†ì´ ê²¬ê³ í•˜ê²Œ ë™ì‘."""
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separators=["\n\n", "\n", "ã€‚", "ï¼", "!", "?", ".", " "]
    )
    return splitter.split_documents(docs)


def build_vectorstore_from_texts(texts: List[Any]) -> Chroma:
    """Chroma ë²¡í„°ìŠ¤í† ì–´ ìƒì„±(ë¡œì»¬ ì˜ì†)."""
    embeddings = OllamaEmbeddings(model=EMBED_MODEL)
    vs = Chroma.from_documents(texts, embedding=embeddings, persist_directory=PERSIST_DIR)
    vs.persist()
    return vs


def build_retrievers(texts: List[Any], vs: Chroma, k: int = 1, fetch_k: int = 4):
    """MMR Chroma + BM25 + Ensemble retriever êµ¬ì„± ë° ë°˜í™˜."""
    chroma_ret = vs.as_retriever(search_type="mmr", search_kwargs={"k": k, "fetch_k": fetch_k})

    # BM25ëŠ” í† í°í™”/ì „ì²˜ë¦¬ë¥¼ ë‚´ë¶€ì ìœ¼ë¡œ ìˆ˜í–‰
    bm25 = BM25Retriever.from_documents(texts)
    bm25.k = max(2, k)  # ì†ŒëŸ‰ì´ë¼ë„ 2ê°œ ì´ìƒ ë°˜í™˜í•˜ì—¬ ì•™ìƒë¸” ê°€ì¹˜ë¥¼ í™•ë³´

    ensemble = EnsembleRetriever(
        retrievers=[bm25, chroma_ret],
        weights=[0.2, 0.8],  # ì˜ë¯¸ ì„ë² ë”© ê°€ì¤‘ì¹˜ë¥¼ ë” ë†’ê²Œ ë¶€ì—¬
    )
    return chroma_ret, bm25, ensemble


def get_multiquery(ensemble):
    """MultiQueryRetriever: ë‹¤ì–‘í•œ ê´€ì ì˜ ì§ˆì˜ ìƒì„± â†’ ì•™ìƒë¸” ê²€ìƒ‰.
    ChatOllama(DeepSeek-R1)ë¡œ ì§ˆì˜ ìƒì„±. (OpenAI ì—†ì´ ë™ì‘)
    """
    llm = ChatOllama(model=LLM_MODEL, temperature=0.2, num_ctx=4096)
    mq = MultiQueryRetriever.from_llm(retriever=ensemble, llm=llm,
                                      prompt=ChatPromptTemplate.from_template(
                                          """
                                          ì•„ë˜ ì›ë³¸ ì§ˆë¬¸ì„ ì˜ë¯¸ê°€ ë‹¤ë¥¸ 5ê°œ í•œêµ­ì–´ ê²€ìƒ‰ ì§ˆë¬¸ìœ¼ë¡œ ë³€í™˜í•˜ì„¸ìš”.
                                          - ê° ì¤„ì— í•˜ë‚˜ì˜ ê²€ìƒ‰ ì§ˆë¬¸ë§Œ ì¶œë ¥
                                          - ë¶ˆí•„ìš”í•œ ì„¤ëª… ê¸ˆì§€
                                          ì›ë³¸ ì§ˆë¬¸: {question}
                                          """))
    return mq


def build_rag_chain(retriever, with_multiquery: bool = False):
    """retriever(+ì˜µì…˜ MQ) + LLM + í¬ë§·í„°ë¡œ RAG ì²´ì¸ êµ¬ì„±."""
    llm = ChatOllama(model=LLM_MODEL, temperature=0.2, num_ctx=4096)

    prompt = ChatPromptTemplate.from_template(
        """
        ì•„ë˜ëŠ” ë¬¸ì„œì—ì„œ ì°¾ì€ ë§¥ë½ì…ë‹ˆë‹¤. ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ë‹µí•˜ì„¸ìš”.
        í•„ìš”í•˜ë©´ í•µì‹¬ ê·¼ê±°ë¥¼ ë¶ˆë¦¿ìœ¼ë¡œ ì •ë¦¬í•˜ê³ , ì¶œì²˜(í˜ì´ì§€/ë©”íƒ€ë°ì´í„°)ê°€ ë³´ì´ë©´ í•¨ê»˜ í‘œê¸°í•˜ì„¸ìš”.

        [ë§¥ë½]
        {context}

        [ì§ˆë¬¸]
        {question}
        """
    )

    def format_docs(docs: List[Any]) -> str:
        blocks = []
        for d in docs:
            meta = d.metadata or {}
            src = meta.get("source", meta.get("file_path", ""))
            page = meta.get("page", "")
            snippet = d.page_content.strip().replace("\n", " ")[:500]
            blocks.append(f"- ì¶œì²˜: {src} (p.{page})\n  ë‚´ìš©: {snippet}")
        return "\n\n".join(blocks)

    base = {"context": retriever | format_docs, "question": RunnablePassthrough()} | prompt | llm | StrOutputParser()

    if with_multiquery:
        # MultiQueryRetriever ë¥¼ retriever ë¡œ ëŒ€ì²´í•˜ì—¬ ì²´ì¸ êµ¬ì„±
        mq = get_multiquery(retriever)
        return {"context": mq | format_docs, "question": RunnablePassthrough()} | prompt | llm | StrOutputParser()

    return base


# ===================== Gradio UI ì½œë°± =====================

def ui_build_index(pdf_file, chunk_size, chunk_overlap, k, fetch_k):
    """PDF ì—…ë¡œë“œ â†’ ë¶„í•  â†’ Chroma/BM25/Ensemble êµ¬ì„±."""
    global VECTORSTORE, CHROMA_RETRIEVER, BM25, ENSEMBLE, DOCS

    if pdf_file is None:
        return "PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.", None

    # GradioëŠ” ì„ì‹œ íŒŒì¼ ê²½ë¡œë¥¼ ì œê³µ
    pdf_path = pdf_file.name

    DOCS = load_pdf_docs(pdf_path)
    chunks = split_docs(DOCS, int(chunk_size), int(chunk_overlap))
    vs = build_vectorstore_from_texts(chunks)
    chroma_ret, bm25, ensemble = build_retrievers(chunks, vs, int(k), int(fetch_k))

    VECTORSTORE, CHROMA_RETRIEVER, BM25, ENSEMBLE = vs, chroma_ret, bm25, ensemble

    return (
        f"ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ: ë¬¸ì„œ {len(DOCS)}ê°œ / ì²­í¬ {len(chunks)}ê°œ\n"
        f"MMR(k={k}, fetch_k={fetch_k}), BM25(k={bm25.k})"
    ), None


def ui_ask(question, use_multiquery):
    if ENSEMBLE is None:
        return "ë¨¼ì € PDFë¡œ ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•˜ì„¸ìš”.", []
    if not question or not question.strip():
        return "ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.", []

    chain = build_rag_chain(ENSEMBLE, with_multiquery=bool(use_multiquery))
    answer = chain.invoke(question.strip())

    # ë¯¸ë¦¬ë³´ê¸°: í˜„ì¬ retrieverë¡œ ìƒìœ„ ë¬¸ì„œ ì¼ë¶€ í‘œì‹œ
    docs_preview = ENSEMBLE.get_relevant_documents(question)[:3]
    rows = []
    for d in docs_preview:
        meta = d.metadata or {}
        rows.append({
            "page": meta.get("page", ""),
            "source": meta.get("source", meta.get("file_path", "")),
            "snippet": d.page_content.strip().replace("\n", " ")[:200],
        })

    return answer, rows


# ===================== Gradio App =====================
with gr.Blocks(theme=gr.themes.Soft(primary_hue=gr.themes.colors.indigo)) as demo:
    gr.Markdown("""
    # ğŸ“„ PDF RAG (Ollama + DeepSeek-R1)
    - ë¡œì»¬ LLM/ì„ë² ë”©ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤. (API Key ë¶ˆí•„ìš”)
    - PDF ì—…ë¡œë“œ â†’ **ì¸ë±ìŠ¤ ìƒì„±** â†’ **ì§ˆë¬¸** ìˆœì„œë¡œ ì‚¬ìš©í•˜ì„¸ìš”.
    """)

    with gr.Row():
        pdf = gr.File(label="PDF ì—…ë¡œë“œ", file_types=[".pdf"], type="filepath")

    with gr.Accordion("ì¸ë±ì‹± ì„¤ì •", open=False):
        with gr.Row():
            cs = gr.Slider(200, 1200, value=300, step=50, label="chunk_size")
            co = gr.Slider(0, 300, value=50, step=10, label="chunk_overlap")
        with gr.Row():
            k = gr.Slider(1, 5, value=1, step=1, label="MMR k (ë°˜í™˜ ë¬¸ì„œ ìˆ˜)")
            fk = gr.Slider(1, 12, value=4, step=1, label="MMR fetch_k (ê³ ë ¤ ë¬¸ì„œ ìˆ˜)")
        build_btn = gr.Button("ì¸ë±ìŠ¤ ìƒì„±", variant="primary")

    status = gr.Markdown("ì—…ë¡œë“œ í›„ ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•˜ì„¸ìš”.")

    with gr.Row():
        q = gr.Textbox(label="ì§ˆë¬¸", placeholder="ì˜ˆ) ì—ì½”í”„ë¡œì— ëŒ€í•´ì„œ ì•Œë ¤ì¤˜", lines=2)
    with gr.Row():
        use_mq = gr.Checkbox(value=False, label="Multi-Query ì‚¬ìš©(ì§ˆë¬¸ í™•ì¥)")
        ask_btn = gr.Button("ì§ˆì˜", variant="primary")

    ans = gr.Markdown(label="ë‹µë³€")
    table = gr.Dataframe(headers=["page", "source", "snippet"], datatype=["str", "str", "str"], wrap=True, label="ì°¸ê³  ë¬¸ì„œ (ìƒìœ„ 3ê°œ)")

    build_btn.click(fn=ui_build_index, inputs=[pdf, cs, co, k, fk], outputs=[status, table])
    ask_btn.click(fn=ui_ask, inputs=[q, use_mq], outputs=[ans, table])


if __name__ == "__main__":
    print("[ì•ˆë‚´] Ollama ëª¨ë¸ ì¤€ë¹„ ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”:")
    print("  - LLM: deepseek-r1 (ollama pull deepseek-r1)")
    print("  - Embedding: nomic-embed-text (ollama pull nomic-embed-text)\n")

    demo.launch(server_name="0.0.0.0", server_port=7860)







